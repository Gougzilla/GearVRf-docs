{
    "docs": [
        {
            "location": "/", 
            "text": "GearVR Framework Project\n\n\nThe Gear VR Framework (GearVRf) Project is a lightweight, powerful, open source rendering engine with a Java interface for developing mobile VR games and applications for Gear VR and Google Daydream View.\n\n\nGearVRf is:\n\n\n\n\n\n\nSimple\n - Java interface, Android Studio build environment and a simple SDK allow you to prototype rapidly. In-depth OpenGL and Oculus/Daydream rendering knowledge is not required. \n\n\n\n\n\n\nPowerful\n - VR-specific rendering optimizations and optional access to low-level graphics pipeline allow you to create and optimize high performance graphics.   \n\n\n\n\n\n\nOptimized for mobile\n - Built with mobile performance in mind, GearVRf provides easy and performance-oriented access to Android OS system level calls.  \n\n\n\n\n\n\nOpen source\n - that means no licensing fees or royalties \never\n, and active developer community contributions \n\n\n\n\n\n\nEfficient\n - GearVRf's interface layer is abstracted from the target mobile VR platform SDK. You can write code once and build for both Gear VR and Daydream/Cardboard. Default build options create a single apk that works for both, with run-time flow checks for Oculus service that revert to Daydream if Oculus not available. Google Cardboard is even supported, as Daydream's backend reverts to the Google Cardboard service if the mobile device does not support Daydream!\n\n\n\n\n\n\nGet Started", 
            "title": "Home"
        }, 
        {
            "location": "/#gearvr-framework-project", 
            "text": "The Gear VR Framework (GearVRf) Project is a lightweight, powerful, open source rendering engine with a Java interface for developing mobile VR games and applications for Gear VR and Google Daydream View.  GearVRf is:    Simple  - Java interface, Android Studio build environment and a simple SDK allow you to prototype rapidly. In-depth OpenGL and Oculus/Daydream rendering knowledge is not required.     Powerful  - VR-specific rendering optimizations and optional access to low-level graphics pipeline allow you to create and optimize high performance graphics.       Optimized for mobile  - Built with mobile performance in mind, GearVRf provides easy and performance-oriented access to Android OS system level calls.      Open source  - that means no licensing fees or royalties  ever , and active developer community contributions     Efficient  - GearVRf's interface layer is abstracted from the target mobile VR platform SDK. You can write code once and build for both Gear VR and Daydream/Cardboard. Default build options create a single apk that works for both, with run-time flow checks for Oculus service that revert to Daydream if Oculus not available. Google Cardboard is even supported, as Daydream's backend reverts to the Google Cardboard service if the mobile device does not support Daydream!    Get Started", 
            "title": "GearVR Framework Project"
        }, 
        {
            "location": "/getting_started/", 
            "text": "Software Requirements\n\n\nBefore start using GearVR Framework, make sure you download the following SDKs\n\n\n\n\nAndroid Studio\n\n\nJDK 1.7 or above\n\n\nOculus Mobile SDK\n\n\nGoogle VR SDK\n\n\n\n\nHardware Requirements\n\n\nGearVR Framework supports following devices\n\n\n\n\nGear VR compatible Samsung phone - Galaxy S8, Galaxy S8+, Galaxy S7, Galaxy S7 Edge, Note 5, Galaxy S6, Galaxy S6 Edge, Galaxy Edge+, Note 4\n\n\nSamsung Gear VR headset\n\n\nDaydream-ready phone\n\n\nGoogle Daydream View VR headset\n\n\n\n\nGetting Started\n\n\nGetting started with GearVR Framework in few simple steps\n\n\n\n\nDownload the \ntemplate project\n\n\nMake sure to download your \nOculus signature file\n and copy it under \napp\\src\\main\\assets\n folder\n\n\nOpen the project with Android Studio\n\n\nClick Run button and put on your VR device\n\n\n\n\n\n\nNote\n\n\nYou can test VR apps without headset, by enable Samsung VR service developer mode.\nSettings \n Applications \n manage applications \n Gear VR Service \n Manage Storage - press the VR Service Version 6 times. After that a 'You are a developer' message will appear.\n\n\n\n\n\n\nNote\n\n\nMake sure install your VR app with a valid oculus signature on the device first. Otherwise you'll see a 'You are not a developer' message.\n\n\n\n\n\n\nWarning\n\n\nScreen will keep blinking after you turn on the developer mode", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting_started/#software-requirements", 
            "text": "Before start using GearVR Framework, make sure you download the following SDKs   Android Studio  JDK 1.7 or above  Oculus Mobile SDK  Google VR SDK", 
            "title": "Software Requirements"
        }, 
        {
            "location": "/getting_started/#hardware-requirements", 
            "text": "GearVR Framework supports following devices   Gear VR compatible Samsung phone - Galaxy S8, Galaxy S8+, Galaxy S7, Galaxy S7 Edge, Note 5, Galaxy S6, Galaxy S6 Edge, Galaxy Edge+, Note 4  Samsung Gear VR headset  Daydream-ready phone  Google Daydream View VR headset", 
            "title": "Hardware Requirements"
        }, 
        {
            "location": "/getting_started/#getting-started", 
            "text": "Getting started with GearVR Framework in few simple steps   Download the  template project  Make sure to download your  Oculus signature file  and copy it under  app\\src\\main\\assets  folder  Open the project with Android Studio  Click Run button and put on your VR device    Note  You can test VR apps without headset, by enable Samsung VR service developer mode.\nSettings   Applications   manage applications   Gear VR Service   Manage Storage - press the VR Service Version 6 times. After that a 'You are a developer' message will appear.    Note  Make sure install your VR app with a valid oculus signature on the device first. Otherwise you'll see a 'You are not a developer' message.    Warning  Screen will keep blinking after you turn on the developer mode", 
            "title": "Getting Started"
        }, 
        {
            "location": "/tutorials/simple_vr_app/", 
            "text": "Overview\n\n\nAfter setting up GearVR Framework, let's create our first VR app and learn a few very important concept in the process.\n\n\nCreate Project\n\n\nThe easiest way to create a GearVR Framework project is by copying the \ntemplate project\n \n\n\nProject Structure\n\n\nBefore we start, let's take a look at some essential parts of fo a GearVR Framework app\n\n\n\n\nThe template project contains two classes, \nMainActivity\n and \nMainScene\n\n\n\n\n\n\nMainActivity\n is the entry point of the app, like \nandroid.app.Activity\n it handles the initialization and life cycle of a VR app.\n\n\n\n\n\n\nMainScene\n is the container of a scene, just like a scene in the movie, it contains things like camera, characters, visual effects etc, it is the place for all your VR content.\n\n\n\n\n\n\nIn the assets folder there are two files: \ngvr.xml\n and \noculussig\n file\n\n\n\n\n\n\ngvr.xml\n is where you config various behavior of GearVR framework, which we'll get into detail in future tutorials\n\n\n\n\n\n\noculussig\n is the oculus signing file which allows you to deploy debug apps to VR devices, so always make sure this you have a signing file in your project\n\n\n\n\n\n\nScene\n\n\nUsually a VR app/game consists of one or more scenes. The templeate project already created one Scene called \nMainScene\n and it should be the starting point for your VR project. \n\n\n\n\nNote\n\n\nMainScene\n extends from \nGVRMain\n, if you're creating your own entry point class, make sure to extend \nGVRMain\n\n\n\n\nThere are two functions in \nMainScene\n both are important for the scene to work\n\n\n\n\nonInit() is called when the scene is being loaded, can be used to perform actions like object creation, assets loading.\n\n\nonStep() called once per frame, can be used to perform things like animation, AI or user interactions\n\n\n\n\nAdd object\n\n\nAdding a object to the scene is simple, just create the object and specify the material and add it to the scene\n\n\nFirst let's add a new member varible for the Cube to the \nMainScene\n\n\n    \nGVRCubeSceneObject\n \nmCube\n\n\n\n\n\n\nThen add the cube to our scene with following code in \nonInit()\n function\n\n\n    \n//Create a cube\n\n    \nmCube\n \n=\n \nnew\n \nGVRCubeSceneObject\n(\ngvrContext\n);\n\n\n    \n//Set shader for the cube\n\n    \nmCube\n.\ngetRenderData\n().\nsetShaderTemplate\n(\nGVRPhongShader\n.\nclass\n);\n\n\n    \n//Set position of the cube at (0, -2, -3)\n\n    \nmCube\n.\ngetTransform\n().\nsetPosition\n(\n0\n,\n \n-\n2\n,\n \n-\n3\n);\n\n\n    \n//Add cube to the scene\n\n    \ngvrContext\n.\ngetMainScene\n().\naddSceneObject\n(\nmCube\n);\n\n\n\n\n\n\nBuild and run the app, you should be able to see a white cube on the screen\n\n\n\n\nNote\n\n\nIf you're using \"VR developer mode\" without headset the orentation might be different, you might need to turn around to see the cube\n\n\n\n\nMake it move\n\n\nNow let's make the cube rotate. Because we want to see the cube rotate continuously, we need to update it's rotation every frame. So instead of the \nonInit()\n function we need to add the rotation logic into \nonStep()\n function\n\n\nAdd the following code to the \nonStep()\n function\n\n\n    \n//Rotate the cube along the Y axis\n\n    \nmCube\n.\ngetTransform\n().\nrotateByAxis\n(\n1\n,\n \n0\n,\n \n1\n,\n \n0\n);\n\n\n\n\n\n\nBuild and run the app, you should be able to see a rotating cube.\n\n\nNow that you have a rotating cube in VR, feel free to try different things, how about, change it's color, make it scalue up and down or move it around.\n\n\nSource Code", 
            "title": "Simple VR app"
        }, 
        {
            "location": "/tutorials/simple_vr_app/#overview", 
            "text": "After setting up GearVR Framework, let's create our first VR app and learn a few very important concept in the process.", 
            "title": "Overview"
        }, 
        {
            "location": "/tutorials/simple_vr_app/#create-project", 
            "text": "The easiest way to create a GearVR Framework project is by copying the  template project", 
            "title": "Create Project"
        }, 
        {
            "location": "/tutorials/simple_vr_app/#project-structure", 
            "text": "Before we start, let's take a look at some essential parts of fo a GearVR Framework app   The template project contains two classes,  MainActivity  and  MainScene    MainActivity  is the entry point of the app, like  android.app.Activity  it handles the initialization and life cycle of a VR app.    MainScene  is the container of a scene, just like a scene in the movie, it contains things like camera, characters, visual effects etc, it is the place for all your VR content.    In the assets folder there are two files:  gvr.xml  and  oculussig  file    gvr.xml  is where you config various behavior of GearVR framework, which we'll get into detail in future tutorials    oculussig  is the oculus signing file which allows you to deploy debug apps to VR devices, so always make sure this you have a signing file in your project", 
            "title": "Project Structure"
        }, 
        {
            "location": "/tutorials/simple_vr_app/#scene", 
            "text": "Usually a VR app/game consists of one or more scenes. The templeate project already created one Scene called  MainScene  and it should be the starting point for your VR project.    Note  MainScene  extends from  GVRMain , if you're creating your own entry point class, make sure to extend  GVRMain   There are two functions in  MainScene  both are important for the scene to work   onInit() is called when the scene is being loaded, can be used to perform actions like object creation, assets loading.  onStep() called once per frame, can be used to perform things like animation, AI or user interactions", 
            "title": "Scene"
        }, 
        {
            "location": "/tutorials/simple_vr_app/#add-object", 
            "text": "Adding a object to the scene is simple, just create the object and specify the material and add it to the scene  First let's add a new member varible for the Cube to the  MainScene       GVRCubeSceneObject   mCube   Then add the cube to our scene with following code in  onInit()  function       //Create a cube \n     mCube   =   new   GVRCubeSceneObject ( gvrContext ); \n\n     //Set shader for the cube \n     mCube . getRenderData (). setShaderTemplate ( GVRPhongShader . class ); \n\n     //Set position of the cube at (0, -2, -3) \n     mCube . getTransform (). setPosition ( 0 ,   - 2 ,   - 3 ); \n\n     //Add cube to the scene \n     gvrContext . getMainScene (). addSceneObject ( mCube );   Build and run the app, you should be able to see a white cube on the screen   Note  If you're using \"VR developer mode\" without headset the orentation might be different, you might need to turn around to see the cube", 
            "title": "Add object"
        }, 
        {
            "location": "/tutorials/simple_vr_app/#make-it-move", 
            "text": "Now let's make the cube rotate. Because we want to see the cube rotate continuously, we need to update it's rotation every frame. So instead of the  onInit()  function we need to add the rotation logic into  onStep()  function  Add the following code to the  onStep()  function       //Rotate the cube along the Y axis \n     mCube . getTransform (). rotateByAxis ( 1 ,   0 ,   1 ,   0 );   Build and run the app, you should be able to see a rotating cube.  Now that you have a rotating cube in VR, feel free to try different things, how about, change it's color, make it scalue up and down or move it around.  Source Code", 
            "title": "Make it move"
        }, 
        {
            "location": "/tutorials/360_photo_app/", 
            "text": "Overview\n\n\nLast time we showed how easy it is to create a simple game scene, This time we're going to create an app for viewing 360 photos.\n\n\nTo display a photo in VR you first need to have a 360 photo. Then display the photo inside a sphere. When user look at the image from inside the sphere, it will create a immersive experience for the them. \n\n\nCreate Project\n\n\nMake a copy of \ntemplate project\n and copy your oculus signing files to the assets folder\n\n\nLoad Image\n\n\nIn order to display the image we need to load the image into the memory first. And here is how to do it.\n\n\n\n\n\n\ndownload a 360 photo from \nhere\n \n\n\n\n\n\n\nplace it under \n\\app\\src\\main\\res\\raw\n folder\n\n\n\n\n\n\nload image with the following code\n\n\n\n\n\n\nFuture\nGVRTexture\n \ntexture\n \n=\n \n    \ngvrContext\n.\ngetAssetLoader\n().\nloadFutureTexture\n(\n\n        \nnew\n \nGVRAndroidResource\n(\ngvrContext\n,\n \nR\n.\nraw\n.\nphotosphere\n)\n\n    \n);\n\n\n\n\n\n\n\n\nNote\n\n\nWe use loadFutureTexture because we want to load the texture asynchronously.\n\n\n\n\nCreate Sphere\n\n\nAdd the following code to create a sphere and apply the texture we previously loaded\n\n\n    \nGVRSphereSceneObject\n \nsphere\n \n=\n \n        \nnew\n \nGVRSphereSceneObject\n(\ngvrContext\n,\n \nfalse\n,\n \ntexture\n);\n\n\n    \n//Add Sphere to the scene\n\n    \ngvrContext\n.\ngetMainScene\n().\naddSceneObject\n(\nsphere\n);\n\n\n\n\n\n\n\n\nNote\n\n\nWe specify faceingOut parameter as false, because the player is inside the sphere looking out.\n\n\nYou can also specify the stack and slice parameter to make the sphere more smooth.", 
            "title": "360 Photo app"
        }, 
        {
            "location": "/tutorials/360_photo_app/#overview", 
            "text": "Last time we showed how easy it is to create a simple game scene, This time we're going to create an app for viewing 360 photos.  To display a photo in VR you first need to have a 360 photo. Then display the photo inside a sphere. When user look at the image from inside the sphere, it will create a immersive experience for the them.", 
            "title": "Overview"
        }, 
        {
            "location": "/tutorials/360_photo_app/#create-project", 
            "text": "Make a copy of  template project  and copy your oculus signing files to the assets folder", 
            "title": "Create Project"
        }, 
        {
            "location": "/tutorials/360_photo_app/#load-image", 
            "text": "In order to display the image we need to load the image into the memory first. And here is how to do it.    download a 360 photo from  here      place it under  \\app\\src\\main\\res\\raw  folder    load image with the following code    Future GVRTexture   texture   =  \n     gvrContext . getAssetLoader (). loadFutureTexture ( \n         new   GVRAndroidResource ( gvrContext ,   R . raw . photosphere ) \n     );    Note  We use loadFutureTexture because we want to load the texture asynchronously.", 
            "title": "Load Image"
        }, 
        {
            "location": "/tutorials/360_photo_app/#create-sphere", 
            "text": "Add the following code to create a sphere and apply the texture we previously loaded       GVRSphereSceneObject   sphere   =  \n         new   GVRSphereSceneObject ( gvrContext ,   false ,   texture ); \n\n     //Add Sphere to the scene \n     gvrContext . getMainScene (). addSceneObject ( sphere );    Note  We specify faceingOut parameter as false, because the player is inside the sphere looking out.  You can also specify the stack and slice parameter to make the sphere more smooth.", 
            "title": "Create Sphere"
        }, 
        {
            "location": "/programming_guide/overview/", 
            "text": "GearVR Framework Development Overview\n\n\nIntroduction to GearVRf integration and VR app development\n\n\nGearVRf provides tools to speed up development of advanced features in high quality VR applications. Available EGL extensions (including dual scan, front buffer, MSAA, and tile rendering) allow the best render quality.\n\n\nGearVRf is a native code 3D rendering engine with an Android library interface. You can build non-trivial content using only built-in objects. You can add new objects (such as scene objects with or without GL shaders) derived from classes or by overriding some methods - GearVRf takes care of all hardware handholding. You can do just about everything in Java - all source code is published, so you can easily add to or tweak native code.\n\n\nAnatomy of GearVRf Applications\n\n\nGearVRf is a framework which controls how and when your code is executed. Subclassing GearVRf objects allows you to add your own code. You can also listen to GearVRf events and provide callbacks that respond to them.\n\n\nA 3D scene is represented as a hierarchy of GearVRf scene objects. Each visible object has a triangle mesh describing its shape, a material describing its color properties and a transformation matrix controlling its position in the 3D world. You do not explicitly call OpenGL when using GearVRf. Instead, the GearVRF framework manages all rendering, providing a higher level abstraction for graphics.\n\n\nWhen constructing an Android application, you subclass the Activity class. Similarly, when constructing a GearVRF application you subclass GVRActivity, providing initialization code to create a GVRMain to set up the initial 3D scene and handle input events.\n\n\nDuring initialization, GVRActivity creates a GVRViewManager which does all the heavy lifting. This class is responsible for task scheduling, 3D rendering, animation and asset loading.\n\n\n\n\nThread Management\n\n\nOne key constraint of embedded GPU programming is that there is only one GL context. That is, all GPU commands must come from the same thread - the GL thread. The GPU should always be busy; therefore, the GL thread cannot be the main GUI thread.\n\n\nWhen starting GearVRf, your Android app creates the GL thread, puts the phone into stereoscopic mode, and supplies a pair of callback methods that run the app's startup and per-frame code on the GL thread. GearVRf provides methods for any thread to schedule runnable callbacks to the GL thread. All these callbacks mean that GearVRf programming is event-oriented on the GL thread in just the same way that Android programming is event-oriented on the GUI thread. Running two independent event systems on two independent threads does mean that you have to think about IPC whenever your Android Activity code on the GUI thread interacts with the GearVRf code on the GL thread. However, dual-thread operation also creates another huge section of your application that can take advantage of event atomicity. That is, callback events are method calls from a main loop - neither the GUI thread nor the GL thread ever runs more than one callback at one time, and each callback has to run to completion before any other callback can start on that thread. Your GL callbacks do not have to write code to keep other GL callbacks from seeing data structures in a partially updated state.\n\n\nScene Graph and Scene Objects\n\n\nYour startup code builds a scene graph made up of scene objects, and your per-frame code then manipulates the scene graph in real time. Each scene object has a 4x4 matrix that describes its position, orientation, and zoom relative to its parent. Each scene object can parent other scene objects, so complex objects can be composed of multiple small objects, each with its own shape and appearance, with all changing in synchrony. Each scene object provides methods to change its components using a lazy update pattern, which means that multiple updates per event cost very little more than a single update.\n\n\nYou make a scene object visible by adding a surface geometry and a skin. The geometry is a mesh of 3D triangles. GearVRf provides methods to build simple rectangular quads, and to load more complex meshes from files built by 3D model editors.\n\n\nEach material class contains the shader type, the GL identifier of a shader, values for all shader parameters, texture, and other uniform mappings. Each shader has two parts: a vertex shader and a fragment shader. The vertex shader is called for each vertex of each visible triangle and can compute triangle-specific values that are passed to the fragment shader, which draws each pixel of each visible triangle. GearVRf contains standard shaders that provide methods, such as simply sampling a texture (a bitmap image in GPU memory), without applying any lighting effects. You can create custom shaders by supplying vertex and fragment shaders and by declaring names to bind Java values to. The GL shader language is very simple and C-like; you can learn a lot by reading a few of the shaders in the sample applications.\n\n\nScene Graph\n\n\nThe scene graph describes the spatial relationship between objects in the scene. Each scene object has a 4x4 transformation matrix to position and orient it locally. The scene objects may be nested so that the transformations of the parent nodes are inherited by the children. This allows objects to be easily positioned and animated relative to one another.\n\n\nHere we see a scene graph for a butterfly with a body and two wings. Each scene object has a position and an orientation. The left and right wings can share the same mesh but it is positioned and oriented differently for each wing. The initial translation on the body is inherited by the wings. \n\n\n\n\nThe form of your scene graph can have implications for the performance of your application. Typically, having lots of small objects performs poorly compared to several large objects with a similar total vertex count. This is because there is a considerable amount of overhead in rendering a single object. GearVRf attempts to batch objects that do not move together to improve performance.\n\n\nPicking will work better on a spatially sorted scene graph. Grouping objects that are physically close together under a common ancestor will improve picking performance.\n\n\nTypes of Scene Objects\n\n\nYou can have invisible scene objects. These have a location and a set of child objects. This can be useful to move a set of scene objects as a unit preserving their relative geometry.\n\n\nVisible scene objects have a render data component attached which contains the geometry defining the shape of the object and a \nmaterial\n describing its appearance. The material contains the data that will be passed to the shader used by the GPU to draw the mesh.\n\n\nIn addition to displaying geometry, a scene object can display text, 360 photos, 360 video, normal photos and video, Android application view and internet browser views.\n\n\n\n\n\n\n\n\nScene Object Class\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGVRSphereSceneObject\n\n\nconstructs sphere geometry\n\n\n\n\n\n\nGVRConeSceneObject\n\n\nconstructs cone geometry\n\n\n\n\n\n\nGVRCylinderSceneObject\n\n\nconstructs cylinder geometry\n\n\n\n\n\n\nGVRTextViewSceneObject\n\n\ndisplays text\n\n\n\n\n\n\nGVRVideoSceneObject\n\n\ndisplays a video\n\n\n\n\n\n\nGVRWebViewSceneObject\n\n\ndisplays an internet browser window\n\n\n\n\n\n\nGVRCameraSceneObject\n\n\ndisplays video from the phone camera\n\n\n\n\n\n\n\n\nScene Construction Example\n\n\nConstructing the initial GearVRF scene usually involves importing a set of assets and placing them relative to one another. In this example we make a simple butterfly with an ellipsoid for a body and textured planes for wings.\n\n\n    \nGVRContext\n \ncontext\n;\n\n    \nGVRTexture\n \nwingtex\n \n=\n \ncontext\n.\nloadTexture\n(\nnew\n \nGVRAndroidResource\n(\ncontext\n,\n \nR\n.\ndrawable\n.\nwingtex\n));\n\n    \nGVRSceneObject\n \nbody\n \n=\n \nnew\n \nGVRSphereObject\n(\ncontext\n);\n\n    \nGVRSceneObject\n \nleftwing\n \n=\n \nnew\n \nGVRSceneObject\n(\ncontext\n,\n \nwingtex\n);\n\n    \nGVRSceneObject\n \nrightwing\n \n=\n \nnew\n \nGVRSceneObject\n(\ncontext\n,\n \nwingtex\n);\n\n    \nleftwing\n.\ngetTransform\n().\nsetPosition\n(-\n1\n,\n \n0\n,\n \n0\n);\n\n    \nrightwing\n.\ngetTransform\n().\nsetPosition\n(\n1\n,\n \n0\n,\n \n0\n);\n\n    \nrightwing\n.\ngetTransform\n().\nsetRotationY\n(\n180\n);\n\n\n\n\n\n\nScene Object Components\n\n\nA scene object can have one or more components attached which provide additional capabilities. All scene objects have a GVRTransform component which supplies the 4x4 matrix used to position and orient the object in the scene. Attaching a GVRRenderData component referencing geometry and material properties will cause the geometry to be displayed in the scene.\n\n\nThe following components can be attached to a GVRSceneObject:\n\n\n\n\nGVRTransform - 4x4 transformation matrix\n\n\nGVRRenderData - geometry with material properties\n\n\nGVRLightBase - illumination source\n\n\nGVRCamera - camera\n\n\nGVRCameraRig - stereoscopic camera rig\n\n\nGVRCollider - collision geometry\n\n\nGVRBehavior - user defined component\n\n\n\n\nEach scene object can only have one component of a particular type. For example, you cannot attach two lights or two cameras to a single scene object. Components are retrieved and removed based on their type. When a component is attached to a scene object, it derives its position and orientation from the GVRTransform attached to that scene object.\n\n\n\n\n\n\n\n\nGVRSceneObject  function\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGVRComponent getComponent(int type)\n\n\nGet the component of the specified class attached to the owner scene object.\n\n\n\n\n\n\nvoid attachComponent(GVRComponent)\n\n\nAttach the given component to the scene object.\n\n\n\n\n\n\nvoid detachComponent(GVRComponent)\n\n\nDetach the given component from the scene object.\n\n\n\n\n\n\nvoid detachComponenet(int type)\n\n\nDetach the component of the specified type from the scene object.\n\n\n\n\n\n\nList getAllComponents(int type)\n\n\nGet all components of the given type from the scene object and its children.", 
            "title": "Overview"
        }, 
        {
            "location": "/programming_guide/overview/#gearvr-framework-development-overview", 
            "text": "Introduction to GearVRf integration and VR app development  GearVRf provides tools to speed up development of advanced features in high quality VR applications. Available EGL extensions (including dual scan, front buffer, MSAA, and tile rendering) allow the best render quality.  GearVRf is a native code 3D rendering engine with an Android library interface. You can build non-trivial content using only built-in objects. You can add new objects (such as scene objects with or without GL shaders) derived from classes or by overriding some methods - GearVRf takes care of all hardware handholding. You can do just about everything in Java - all source code is published, so you can easily add to or tweak native code.", 
            "title": "GearVR Framework Development Overview"
        }, 
        {
            "location": "/programming_guide/overview/#anatomy-of-gearvrf-applications", 
            "text": "GearVRf is a framework which controls how and when your code is executed. Subclassing GearVRf objects allows you to add your own code. You can also listen to GearVRf events and provide callbacks that respond to them.  A 3D scene is represented as a hierarchy of GearVRf scene objects. Each visible object has a triangle mesh describing its shape, a material describing its color properties and a transformation matrix controlling its position in the 3D world. You do not explicitly call OpenGL when using GearVRf. Instead, the GearVRF framework manages all rendering, providing a higher level abstraction for graphics.  When constructing an Android application, you subclass the Activity class. Similarly, when constructing a GearVRF application you subclass GVRActivity, providing initialization code to create a GVRMain to set up the initial 3D scene and handle input events.  During initialization, GVRActivity creates a GVRViewManager which does all the heavy lifting. This class is responsible for task scheduling, 3D rendering, animation and asset loading.", 
            "title": "Anatomy of GearVRf Applications"
        }, 
        {
            "location": "/programming_guide/overview/#thread-management", 
            "text": "One key constraint of embedded GPU programming is that there is only one GL context. That is, all GPU commands must come from the same thread - the GL thread. The GPU should always be busy; therefore, the GL thread cannot be the main GUI thread.  When starting GearVRf, your Android app creates the GL thread, puts the phone into stereoscopic mode, and supplies a pair of callback methods that run the app's startup and per-frame code on the GL thread. GearVRf provides methods for any thread to schedule runnable callbacks to the GL thread. All these callbacks mean that GearVRf programming is event-oriented on the GL thread in just the same way that Android programming is event-oriented on the GUI thread. Running two independent event systems on two independent threads does mean that you have to think about IPC whenever your Android Activity code on the GUI thread interacts with the GearVRf code on the GL thread. However, dual-thread operation also creates another huge section of your application that can take advantage of event atomicity. That is, callback events are method calls from a main loop - neither the GUI thread nor the GL thread ever runs more than one callback at one time, and each callback has to run to completion before any other callback can start on that thread. Your GL callbacks do not have to write code to keep other GL callbacks from seeing data structures in a partially updated state.", 
            "title": "Thread Management"
        }, 
        {
            "location": "/programming_guide/overview/#scene-graph-and-scene-objects", 
            "text": "Your startup code builds a scene graph made up of scene objects, and your per-frame code then manipulates the scene graph in real time. Each scene object has a 4x4 matrix that describes its position, orientation, and zoom relative to its parent. Each scene object can parent other scene objects, so complex objects can be composed of multiple small objects, each with its own shape and appearance, with all changing in synchrony. Each scene object provides methods to change its components using a lazy update pattern, which means that multiple updates per event cost very little more than a single update.  You make a scene object visible by adding a surface geometry and a skin. The geometry is a mesh of 3D triangles. GearVRf provides methods to build simple rectangular quads, and to load more complex meshes from files built by 3D model editors.  Each material class contains the shader type, the GL identifier of a shader, values for all shader parameters, texture, and other uniform mappings. Each shader has two parts: a vertex shader and a fragment shader. The vertex shader is called for each vertex of each visible triangle and can compute triangle-specific values that are passed to the fragment shader, which draws each pixel of each visible triangle. GearVRf contains standard shaders that provide methods, such as simply sampling a texture (a bitmap image in GPU memory), without applying any lighting effects. You can create custom shaders by supplying vertex and fragment shaders and by declaring names to bind Java values to. The GL shader language is very simple and C-like; you can learn a lot by reading a few of the shaders in the sample applications.", 
            "title": "Scene Graph and Scene Objects"
        }, 
        {
            "location": "/programming_guide/overview/#scene-graph", 
            "text": "The scene graph describes the spatial relationship between objects in the scene. Each scene object has a 4x4 transformation matrix to position and orient it locally. The scene objects may be nested so that the transformations of the parent nodes are inherited by the children. This allows objects to be easily positioned and animated relative to one another.  Here we see a scene graph for a butterfly with a body and two wings. Each scene object has a position and an orientation. The left and right wings can share the same mesh but it is positioned and oriented differently for each wing. The initial translation on the body is inherited by the wings.    The form of your scene graph can have implications for the performance of your application. Typically, having lots of small objects performs poorly compared to several large objects with a similar total vertex count. This is because there is a considerable amount of overhead in rendering a single object. GearVRf attempts to batch objects that do not move together to improve performance.  Picking will work better on a spatially sorted scene graph. Grouping objects that are physically close together under a common ancestor will improve picking performance.", 
            "title": "Scene Graph"
        }, 
        {
            "location": "/programming_guide/overview/#types-of-scene-objects", 
            "text": "You can have invisible scene objects. These have a location and a set of child objects. This can be useful to move a set of scene objects as a unit preserving their relative geometry.  Visible scene objects have a render data component attached which contains the geometry defining the shape of the object and a  material  describing its appearance. The material contains the data that will be passed to the shader used by the GPU to draw the mesh.  In addition to displaying geometry, a scene object can display text, 360 photos, 360 video, normal photos and video, Android application view and internet browser views.     Scene Object Class  Description      GVRSphereSceneObject  constructs sphere geometry    GVRConeSceneObject  constructs cone geometry    GVRCylinderSceneObject  constructs cylinder geometry    GVRTextViewSceneObject  displays text    GVRVideoSceneObject  displays a video    GVRWebViewSceneObject  displays an internet browser window    GVRCameraSceneObject  displays video from the phone camera", 
            "title": "Types of Scene Objects"
        }, 
        {
            "location": "/programming_guide/overview/#scene-construction-example", 
            "text": "Constructing the initial GearVRF scene usually involves importing a set of assets and placing them relative to one another. In this example we make a simple butterfly with an ellipsoid for a body and textured planes for wings.       GVRContext   context ; \n     GVRTexture   wingtex   =   context . loadTexture ( new   GVRAndroidResource ( context ,   R . drawable . wingtex )); \n     GVRSceneObject   body   =   new   GVRSphereObject ( context ); \n     GVRSceneObject   leftwing   =   new   GVRSceneObject ( context ,   wingtex ); \n     GVRSceneObject   rightwing   =   new   GVRSceneObject ( context ,   wingtex ); \n     leftwing . getTransform (). setPosition (- 1 ,   0 ,   0 ); \n     rightwing . getTransform (). setPosition ( 1 ,   0 ,   0 ); \n     rightwing . getTransform (). setRotationY ( 180 );", 
            "title": "Scene Construction Example"
        }, 
        {
            "location": "/programming_guide/overview/#scene-object-components", 
            "text": "A scene object can have one or more components attached which provide additional capabilities. All scene objects have a GVRTransform component which supplies the 4x4 matrix used to position and orient the object in the scene. Attaching a GVRRenderData component referencing geometry and material properties will cause the geometry to be displayed in the scene.  The following components can be attached to a GVRSceneObject:   GVRTransform - 4x4 transformation matrix  GVRRenderData - geometry with material properties  GVRLightBase - illumination source  GVRCamera - camera  GVRCameraRig - stereoscopic camera rig  GVRCollider - collision geometry  GVRBehavior - user defined component   Each scene object can only have one component of a particular type. For example, you cannot attach two lights or two cameras to a single scene object. Components are retrieved and removed based on their type. When a component is attached to a scene object, it derives its position and orientation from the GVRTransform attached to that scene object.     GVRSceneObject  function  Description      GVRComponent getComponent(int type)  Get the component of the specified class attached to the owner scene object.    void attachComponent(GVRComponent)  Attach the given component to the scene object.    void detachComponent(GVRComponent)  Detach the given component from the scene object.    void detachComponenet(int type)  Detach the component of the specified type from the scene object.    List getAllComponents(int type)  Get all components of the given type from the scene object and its children.", 
            "title": "Scene Object Components"
        }, 
        {
            "location": "/programming_guide/features/loading_assets/", 
            "text": "GearVRf supports loading of 3D content files both synchronously and asynchronously. Your application may issue a blocking load and wait for the asset or get a callback when the asset loading is finished. GearVRf can import .OBJ, .FBX, Collada  (.dae) and X3D file formats, as well as all file formats supported by \nAssimp\n. GearVRf can also read all commonly used bitmap file formats.\n\n\nLoading a 3D Model\n\n\nLoading models is handled by the GVRAssetLoader class which is accessible from the context by calling GVRContext.getAssetLoader(). The asset loader can load models from a variety of places. You use the volume type to indicate where you are reading the model from.\n\n\n\n\nGVRResourceLoader.VolumeType.ANDROID_ASSETS designates the model is in the \"assets\" directory.\n\n\nGVRResourceLoader.VolumeType.ANDROID_SDCARD designates the model on the phone SD card.\n\n\nGVRResourceLoader.VolumeType.NETWORK designates the model is on the internet.\n\n\n\n\nThe \nGVRAssetLoader.loadModel\n function loads a model from a device and returns as soon as the model geometry has been parsed and accumulated. This model may not have been added to the scene yet. If you pass the current GVRScene as an argument, the asset loader will wait until all of the textures in the model have been loaded and then add it to the scene. If you omit the argument, the model is not added to the scene and you will need to add it in your own code.\n\n\nModel Loading Example\n\n\nThis example shows how to load a model which lives in the assets directory of your application. The path name for the asset is relative to that directory. The asset is loaded in the background in another thread and added to the scene when all of its textures have completed loading. The model returned may not be completely loaded but all of the geometry will be accessible. Usually assets are loaded in the onInit function of your main script.\n\n\npublic\n \nvoid\n \nonInit\n(\nGVRContext\n \ncontext\n)\n\n\n{\n\n    \nGVRScene\n \nscene\n \n=\n \ncontext\n.\ngetNextMainScene\n();\n\n    \ntry\n\n    \n{\n\n        \nString\n \nurl\n \n=\n \nhttps://raw.githubusercontent.com/gearvrf/GearVRf-Demos/master/gvrjassimpmodelloader/assets/trees/trees9.3ds\n;\n\n        \nGVRSceneObject\n \nmodel\n \n=\n \ncontext\n.\ngetAssetLoader\n().\nloadModel\n(\nurl\n,\n \nGVRResourceVolume\n.\nVolumeType\n.\nNET\n,\n \nscene\n);\n\n        \nmodel\n.\ngetTransform\n().\nsetPosition\n(\n0.0f\n,\n \n-\n4.0f\n,\n \n-\n20.0f\n);\n\n    \n}\n\n    \ncatch\n \n(\nIOException\n \ne\n)\n\n    \n{\n\n        \nLog\n.\ne\n(\nERROR\n,\n \nFailed to load model: %s\n,\n \ne\n);\n\n    \n}\n\n\n}", 
            "title": "Loading Assets"
        }, 
        {
            "location": "/programming_guide/features/loading_assets/#loading-a-3d-model", 
            "text": "Loading models is handled by the GVRAssetLoader class which is accessible from the context by calling GVRContext.getAssetLoader(). The asset loader can load models from a variety of places. You use the volume type to indicate where you are reading the model from.   GVRResourceLoader.VolumeType.ANDROID_ASSETS designates the model is in the \"assets\" directory.  GVRResourceLoader.VolumeType.ANDROID_SDCARD designates the model on the phone SD card.  GVRResourceLoader.VolumeType.NETWORK designates the model is on the internet.   The  GVRAssetLoader.loadModel  function loads a model from a device and returns as soon as the model geometry has been parsed and accumulated. This model may not have been added to the scene yet. If you pass the current GVRScene as an argument, the asset loader will wait until all of the textures in the model have been loaded and then add it to the scene. If you omit the argument, the model is not added to the scene and you will need to add it in your own code.", 
            "title": "Loading a 3D Model"
        }, 
        {
            "location": "/programming_guide/features/loading_assets/#model-loading-example", 
            "text": "This example shows how to load a model which lives in the assets directory of your application. The path name for the asset is relative to that directory. The asset is loaded in the background in another thread and added to the scene when all of its textures have completed loading. The model returned may not be completely loaded but all of the geometry will be accessible. Usually assets are loaded in the onInit function of your main script.  public   void   onInit ( GVRContext   context )  { \n     GVRScene   scene   =   context . getNextMainScene (); \n     try \n     { \n         String   url   =   https://raw.githubusercontent.com/gearvrf/GearVRf-Demos/master/gvrjassimpmodelloader/assets/trees/trees9.3ds ; \n         GVRSceneObject   model   =   context . getAssetLoader (). loadModel ( url ,   GVRResourceVolume . VolumeType . NET ,   scene ); \n         model . getTransform (). setPosition ( 0.0f ,   - 4.0f ,   - 20.0f ); \n     } \n     catch   ( IOException   e ) \n     { \n         Log . e ( ERROR ,   Failed to load model: %s ,   e ); \n     }  }", 
            "title": "Model Loading Example"
        }, 
        {
            "location": "/programming_guide/features/render_data/", 
            "text": "The render data component is what makes a scene object visible. It provides both geometry and appearance properties. The geometry is a single GVRMesh object which contains a set of indexed vertices. The appearance is a GVRMaterial object which contains a set of key/value pairs defining the variables to be sent to the shader. The shader is a program that executes on the GPU. During rendering, GearVRf manages data flow between your application and the GPU, sending the meshes and materials to the GPU as they are needed. This may require GearVRf to compile and load a shader into the GPU while your application is running. This may happen when you add something to the scene using GVRScene.addSceneObject. The render data component also controls how your mesh is rendered. You can enable or disable lighting, display an object only on one eye and control the order of rendering using its functions.\n\n\n\n\n\n\n\n\nGVRRenderData Function\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nenableLighting\n\n\nEnable light sources in the shader\n\n\n\n\n\n\ndisableLighting\n\n\nDisable light sources in the shader\n\n\n\n\n\n\nsetAlphaBlend\n\n\nEnable / disable alpha blending\n\n\n\n\n\n\nsetAlphaToCoverage\n\n\nEnable / disable alpha to coverage\n\n\n\n\n\n\nsetCullTest\n\n\nEnable / disable backface culling\n\n\n\n\n\n\nsetCullFace\n\n\nDesignate back or front faces for culling\n\n\n\n\n\n\nsetDepthTest\n\n\nEnable / disable depth testing (Z buffer)\n\n\n\n\n\n\nsetDrawMode\n\n\nDesignate triangles, lines or points\n\n\n\n\n\n\nsetRenderMask\n\n\nDesignate rendering left, right or both eyes\n\n\n\n\n\n\nsetRenderingOrder\n\n\nEstablish rendering order\n\n\n\n\n\n\nsetSampleCoverage\n\n\nSpecifies coverage of modification mask\n\n\n\n\n\n\nsetInvertCoverageMask\n\n\nDesignates whether modification mask is inverted\n\n\n\n\n\n\nsetMesh\n\n\nDesignate the mesh to render\n\n\n\n\n\n\nsetMaterial\n\n\nSpecify material properties for shader\n\n\n\n\n\n\nsetShaderTemplate\n\n\nEstablish shading model to use\n\n\n\n\n\n\n\n\nRender Passes\n\n\nA render pass lets you render the same scene object multiple times with different settings. This is useful to achieve effects like cartoon rendering or adding glow around an object. The benefit of using a render pass as opposed to duplicating the object is that culling, transformation and skinning are only performed once. A render pass encapsulates the material and rendering properties (but not the mesh).\n\n\nThis example shows how to implement a multi-sided material using render passes. It uses a red material for the front faces and a blue material for the back faces. \n\n\nGVRSceneObject\n \ncube\n \n=\n \nnew\n \nGVRCubeSceneObject\n(\ngvrContext\n);\n\n\n\nGVRRenderData\n \nrdata\n \n=\n \ncube\n.\ngetRenderData\n();\n\n\n\nGVRMaterial\n \nred\n \n=\n \nrdata\n.\ngetMaterial\n();\n\n\n\nGVRMaterial\n \nblue\n \n=\n \nnew\n \nGVRMaterial\n(\ngvrContext\n);\n\n\n\nGVRRenderPass\n \npass\n \n=\n \nnew\n \nGVRRenderPass\n(\ngvrContext\n);\n\n\n\n\n\nred\n.\nsetDiffuseColor\n(\n1\n,\n \n0\n,\n \n0\n,\n \n1\n);\n\n\n\nblue\n.\nsetDiffuseColor\n(\n0\n,\n \n0\n,\n \n1\n,\n \n0\n);\n\n\n\nrdata\n.\nsetCullFace\n(\nGVRCullFaceEnum\n.\nFront\n);\n\n\n\npass\n.\nsetMaterial\n(\nblue\n);\n\n\n\npass\n.\nsetCullFace\n(\nGVRCullFaceEnum\n.\nBack\n);\n\n\n\nrdata\n.\naddPass\n(\npass\n);", 
            "title": "Render Data"
        }, 
        {
            "location": "/programming_guide/features/render_data/#render-passes", 
            "text": "A render pass lets you render the same scene object multiple times with different settings. This is useful to achieve effects like cartoon rendering or adding glow around an object. The benefit of using a render pass as opposed to duplicating the object is that culling, transformation and skinning are only performed once. A render pass encapsulates the material and rendering properties (but not the mesh).  This example shows how to implement a multi-sided material using render passes. It uses a red material for the front faces and a blue material for the back faces.   GVRSceneObject   cube   =   new   GVRCubeSceneObject ( gvrContext );  GVRRenderData   rdata   =   cube . getRenderData ();  GVRMaterial   red   =   rdata . getMaterial ();  GVRMaterial   blue   =   new   GVRMaterial ( gvrContext );  GVRRenderPass   pass   =   new   GVRRenderPass ( gvrContext );  red . setDiffuseColor ( 1 ,   0 ,   0 ,   1 );  blue . setDiffuseColor ( 0 ,   0 ,   1 ,   0 );  rdata . setCullFace ( GVRCullFaceEnum . Front );  pass . setMaterial ( blue );  pass . setCullFace ( GVRCullFaceEnum . Back );  rdata . addPass ( pass );", 
            "title": "Render Passes"
        }, 
        {
            "location": "/programming_guide/features/meshes/", 
            "text": "VR mesh types, access, and examples\n\n\nIndexed triangle meshes are the only shape definition currently supported by GearVRF. Each mesh contains a set of vertices with the 3D locations of the triangle coordinates. Typically these are unique locations to maximize vertex sharing but this is not a requirement. A triangle has three indices designating which vertices are used by that triangle.\n\n\nIn addition to positions, a mesh may have normals and texture coordinates as well. These arrays, if present, must follow the same ordering as the vertices. There is only one set of triangle indices to reference the position, normal and texture coordinate. This is unlike some systems which permit multiple index tables. \n\n\n\n\nSkinned Meshes\n\n\nSkinned meshes have vertex bone data to indicate which bones affect which vertices in the mesh. A bone is a transform matrix which affects a subset of vertices in the mesh. Each vertex can be influenced by up to four bones.\n\n\nA mesh also contains a list of the bone transforms (GVRBone objects) that influence its vertices. The bone indices in the vertex array reference the bones in this list.\n\n\nGearVRf executes skinning on the GPU but it calculates the bone matrices on the CPU.\n\n\n\n\nAccessing Mesh Components\n\n\nThe vertex shader used to render the mesh determines which vertex components are required. The GearVRf build-in shaders rely on positions, normals, texture coordinates and bone information. You can write your own shaders which use other vertex components.\n\n\nEach vertex component has a unique name and type. GearVRf vertex components are vectors containing between one and four floats. Each component has a function wh|ich can get or set that component for the entire vertex array. GVRMesh provides convenience functions for the built-in types. Reading or writing the vertex array is a high overhead operation and should not be done every frame.\n\n\nThe index array describes an indexed triangle list. Each triangle has three consecutive 32-bit indices in the array designating the vertices from the vertex array that represent that triangle. The setIndices andgetIndices function provide access to the GVRMesh triangle data.\n\n\n\n\n\n\n\n\nShader Name\n\n\nGVRMesh Setter\n\n\nGVRMesh Getter\n\n\n\n\n\n\n\n\n\n\na_position\n\n\nsetVertices(float)\n\n\nfloat[] getVertices()\n\n\n\n\n\n\na_normal\n\n\nsetNormals(float[])\n\n\nfloat[] getNormals()\n\n\n\n\n\n\na_texcoord\n\n\nsetTexCoords(float[])\n\n\nfloat[] getTexCoords()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShader Type\n\n\nGVRMesh Setter\n\n\nGVRMesh Getter\n\n\n\n\n\n\n\n\n\n\nfloat3\n\n\nsetVec3Vector(String name, float[])\n\n\nfloat[] getVec3Vector(String name)\n\n\n\n\n\n\nfloat2\n\n\nsetVec2Vector(String name, float[])\n\n\nfloat[] getVec2Vector(String name)\n\n\n\n\n\n\nfloat4\n\n\nsetVec4Vector(String name, float[])\n\n\nfloat[] getVec4Vector(String name)\n\n\n\n\n\n\nfloat\n\n\nsetFloatVector(String name, float[])\n\n\nfloat[] getFloatVectorString name()\n\n\n\n\n\n\n\n\nMesh Construction Example\n\n\nMost of the time your code will obtain meshes by loading asset files. You can also construct or modify meshes programmatically. A mesh may contain positions, normal and texture coordinates. Depending on the shader used to display the mesh, some of these vertex components may not be used. For example, a shader which does not do lighting will typically not need normals. You can omit the normals and texture coordinate arrays if your shader doesn't need them.\n\n\nThis function constructs a mesh of two triangles with only positions and normals. (If you try to use a textured shader with this mesh, you will get an error.)\n\n\nGVRMesh\n \ncreateMesh\n(\nGVRContext\n \ngvrContext\n)\n\n\n{\n\n    \nGVRMesh\n \nmesh\n \n=\n \nnew\n \nGVRMesh\n(\ngvrContext\n);\n\n    \nfloat\n[]\n \nvertices\n \n=\n\n    \n{\n\n        \n-\n1.0f\n,\n \n0.0f\n,\n \n0.0f\n,\n\n        \n0.0f\n,\n \n1.0f\n,\n \n0.0f\n,\n\n        \n1.0f\n,\n \n0.0f\n,\n \n0.0f\n,\n\n        \n0.0f\n,\n \n-\n1.0f\n,\n \n0.0f\n\n    \n};\n\n    \nfloat\n[]\n \nnormals\n \n=\n\n    \n{\n\n        \n0\n,\n \n0\n,\n \n1\n,\n\n        \n0\n,\n \n0\n,\n \n1\n,\n\n        \n0\n,\n \n0\n,\n \n1\n,\n\n        \n0\n,\n \n0\n,\n \n1\n\n    \n};\n\n    \nchar\n[]\n \ntriangles\n \n=\n \n{\n \n0\n,\n \n1\n,\n \n2\n,\n \n2\n,\n \n3\n,\n \n0\n \n};\n\n    \nmesh\n.\nsetVertices\n(\nvertices\n);\n\n    \nmesh\n.\nsetNormals\n(\nnormals\n);\n\n    \nmesh\n.\nsetTriangles\n(\ntriangles\n);\n\n    \nreturn\n \nmesh\n;\n\n\n}\n\n\n\n\n\n\nYou need to attach your mesh to a GVRSceneObject before it can be displayed. The GVRRenderData object holds both a mesh and a material. Each visible scene object must have render data. This code adds the newly constructed mesh to the scene. Here we assume the GVRMaterial has already been constructed.\n\n\nGVRMesh\n \nmesh\n \n=\n \ncreateMesh\n(\ngvrContext\n);\n\n\nGVRSceneObject\n \nobj\n \n=\n \nnew\n \nGVRSceneObject\n(\ngvrContext\n,\n \nmesh\n);\n\n\nGVRRenderData\n \nrdata\n \n=\n \nobj\n.\ngetRenderData\n();\n\n\nrdata\n.\nsetMaterial\n(\nmaterial\n);", 
            "title": "Meshes"
        }, 
        {
            "location": "/programming_guide/features/meshes/#skinned-meshes", 
            "text": "Skinned meshes have vertex bone data to indicate which bones affect which vertices in the mesh. A bone is a transform matrix which affects a subset of vertices in the mesh. Each vertex can be influenced by up to four bones.  A mesh also contains a list of the bone transforms (GVRBone objects) that influence its vertices. The bone indices in the vertex array reference the bones in this list.  GearVRf executes skinning on the GPU but it calculates the bone matrices on the CPU.", 
            "title": "Skinned Meshes"
        }, 
        {
            "location": "/programming_guide/features/meshes/#accessing-mesh-components", 
            "text": "The vertex shader used to render the mesh determines which vertex components are required. The GearVRf build-in shaders rely on positions, normals, texture coordinates and bone information. You can write your own shaders which use other vertex components.  Each vertex component has a unique name and type. GearVRf vertex components are vectors containing between one and four floats. Each component has a function wh|ich can get or set that component for the entire vertex array. GVRMesh provides convenience functions for the built-in types. Reading or writing the vertex array is a high overhead operation and should not be done every frame.  The index array describes an indexed triangle list. Each triangle has three consecutive 32-bit indices in the array designating the vertices from the vertex array that represent that triangle. The setIndices andgetIndices function provide access to the GVRMesh triangle data.     Shader Name  GVRMesh Setter  GVRMesh Getter      a_position  setVertices(float)  float[] getVertices()    a_normal  setNormals(float[])  float[] getNormals()    a_texcoord  setTexCoords(float[])  float[] getTexCoords()        Shader Type  GVRMesh Setter  GVRMesh Getter      float3  setVec3Vector(String name, float[])  float[] getVec3Vector(String name)    float2  setVec2Vector(String name, float[])  float[] getVec2Vector(String name)    float4  setVec4Vector(String name, float[])  float[] getVec4Vector(String name)    float  setFloatVector(String name, float[])  float[] getFloatVectorString name()", 
            "title": "Accessing Mesh Components"
        }, 
        {
            "location": "/programming_guide/features/meshes/#mesh-construction-example", 
            "text": "Most of the time your code will obtain meshes by loading asset files. You can also construct or modify meshes programmatically. A mesh may contain positions, normal and texture coordinates. Depending on the shader used to display the mesh, some of these vertex components may not be used. For example, a shader which does not do lighting will typically not need normals. You can omit the normals and texture coordinate arrays if your shader doesn't need them.  This function constructs a mesh of two triangles with only positions and normals. (If you try to use a textured shader with this mesh, you will get an error.)  GVRMesh   createMesh ( GVRContext   gvrContext )  { \n     GVRMesh   mesh   =   new   GVRMesh ( gvrContext ); \n     float []   vertices   = \n     { \n         - 1.0f ,   0.0f ,   0.0f , \n         0.0f ,   1.0f ,   0.0f , \n         1.0f ,   0.0f ,   0.0f , \n         0.0f ,   - 1.0f ,   0.0f \n     }; \n     float []   normals   = \n     { \n         0 ,   0 ,   1 , \n         0 ,   0 ,   1 , \n         0 ,   0 ,   1 , \n         0 ,   0 ,   1 \n     }; \n     char []   triangles   =   {   0 ,   1 ,   2 ,   2 ,   3 ,   0   }; \n     mesh . setVertices ( vertices ); \n     mesh . setNormals ( normals ); \n     mesh . setTriangles ( triangles ); \n     return   mesh ;  }   You need to attach your mesh to a GVRSceneObject before it can be displayed. The GVRRenderData object holds both a mesh and a material. Each visible scene object must have render data. This code adds the newly constructed mesh to the scene. Here we assume the GVRMaterial has already been constructed.  GVRMesh   mesh   =   createMesh ( gvrContext );  GVRSceneObject   obj   =   new   GVRSceneObject ( gvrContext ,   mesh );  GVRRenderData   rdata   =   obj . getRenderData ();  rdata . setMaterial ( material );", 
            "title": "Mesh Construction Example"
        }, 
        {
            "location": "/programming_guide/features/materials/", 
            "text": "VR materials, access, and examples\n\n\nA material dictates how a scene object will be colored and textured. The visual appearance of a mesh in the scene is controlled by a fragment shader program in the GPU. GearVRf will automatically construct a shader for common use cases but you can also provide your own shader code.\n\n\nThe GVRMaterial object encapsulates the fragment shader and all of its associated data. This usually includes one or more texture maps and lighting properties that affect how the surface reacts to lights in the scene. Because you can write custom shaders, you can attach your own custom data to a material that your shader can use in computations. This data can be scalar numbers (float or int), arrays or textures. Each custom data element has a string key used to look up or modify its value. When the object is rendered, these values are passed to the shader program.\n\n\nThis picture shows a material with two custom data parameters - a diffuse color with an alpha (4 floats) and an integer enabling transparency. \n\n\n\n\nAccessing Material Parameters\n\n\nThe GVRMaterial provides access to the fragment shader uniforms by name. Setting the value of a material parameter will set the correspondingly named uniform variable in the fragment shader associated with that material. GVRMaterial has a set of functions that read and write the material parameters based on type.\n\n\n\n\n\n\n\n\nShader Type\n\n\nGVRMaterial Setter\n\n\nGVRMaterial Getter\n\n\n\n\n\n\n\n\n\n\nfloat\n\n\nsetFloat(String name, float v)\n\n\nfloat getFloat(String name)\n\n\n\n\n\n\nfloat2\n\n\nsetVec2(String name, float[] v)\n\n\nfloat[] getVec2(String name)\n\n\n\n\n\n\nfloat3\n\n\nsetVec3(String name, float[] v)\n\n\nfloat[] getVec3(String name)\n\n\n\n\n\n\nfloat4\n\n\nsetVec4(String name, float[] v)\n\n\nfloat[] getVec4(String name)\n\n\n\n\n\n\n\n\nMaterial Construction Example\n\n\nA material contains all the data required for the specific shader you are going to use. You will set up different parameters for different shaders. GearVRF has several GearVRfDeveloperGuide.LegacyShaders which can render meshes in a variety of ways. All of them use the same GVRMaterial object but with different properties.\n\n\nIn this example we construct a GVRMaterial to be used with the phong shader. This shader requires us to provide a diffuse texture as well as material and lighting properties.\n\n\nGVRMaterial\n \ncreateMaterial\n(\nGVRContext\n \ngvrContext\n)\n\n\n{\n\n   \nGVRMaterial\n \nmaterial\n \n=\n \ncreateMaterial\n(\ngvrContext\n);\n\n   \nGVRTexture\n \ntex\n \n=\n \ngvrContext\n.\nloadTexture\n(\nnew\n \nGVRAndroidResource\n(\ngvrContext\n,\nR\n.\ndrawable\n.\ngearvr_logo\n));\n\n\n   \nmaterial\n.\nsetDiffuseColor\n(\n0.5f\n,\n \n0.5f\n,\n \n0.5f\n);\n\n   \nmaterial\n.\nsetAmbientColor\n(\n1.0f\n,\n \n1.0f\n,\n \n1.0f\n,\n \n1.0f\n);\n\n   \nmaterial\n.\nsetSpecularColor\n(\n1.0f\n,\n \n1.0f\n,\n \n1.0f\n,\n \n1.0f\n);\n\n   \nmaterial\n.\nsetSpecularExponent\n(\n128.0f\n);\n\n   \nreturn\n \nmaterial\n;\n\n\n}", 
            "title": "Materials"
        }, 
        {
            "location": "/programming_guide/features/materials/#accessing-material-parameters", 
            "text": "The GVRMaterial provides access to the fragment shader uniforms by name. Setting the value of a material parameter will set the correspondingly named uniform variable in the fragment shader associated with that material. GVRMaterial has a set of functions that read and write the material parameters based on type.     Shader Type  GVRMaterial Setter  GVRMaterial Getter      float  setFloat(String name, float v)  float getFloat(String name)    float2  setVec2(String name, float[] v)  float[] getVec2(String name)    float3  setVec3(String name, float[] v)  float[] getVec3(String name)    float4  setVec4(String name, float[] v)  float[] getVec4(String name)", 
            "title": "Accessing Material Parameters"
        }, 
        {
            "location": "/programming_guide/features/materials/#material-construction-example", 
            "text": "A material contains all the data required for the specific shader you are going to use. You will set up different parameters for different shaders. GearVRF has several GearVRfDeveloperGuide.LegacyShaders which can render meshes in a variety of ways. All of them use the same GVRMaterial object but with different properties.  In this example we construct a GVRMaterial to be used with the phong shader. This shader requires us to provide a diffuse texture as well as material and lighting properties.  GVRMaterial   createMaterial ( GVRContext   gvrContext )  { \n    GVRMaterial   material   =   createMaterial ( gvrContext ); \n    GVRTexture   tex   =   gvrContext . loadTexture ( new   GVRAndroidResource ( gvrContext , R . drawable . gearvr_logo )); \n\n    material . setDiffuseColor ( 0.5f ,   0.5f ,   0.5f ); \n    material . setAmbientColor ( 1.0f ,   1.0f ,   1.0f ,   1.0f ); \n    material . setSpecularColor ( 1.0f ,   1.0f ,   1.0f ,   1.0f ); \n    material . setSpecularExponent ( 128.0f ); \n    return   material ;  }", 
            "title": "Material Construction Example"
        }, 
        {
            "location": "/programming_guide/features/lighting/", 
            "text": "VR lights, classes, shadows, uniforms, and examples\n\n\nLights control the illumination of visible scene objects. Depending on the color, intensity and position of the lights, objects may appear lighter, darker or shadowed. The final color of an object depends on both Materials and Lighting. Together they provide everything the fragment shader needs to compute the final color. The fragment shader combines the contributions of all the lights in the scene to compute illumination per pixel.\n\n\nAll lights in the scene are global - they illuminate all the scene objects that have the lighting effect enabled. A light must be attached to a scene object before it can illuminate anything. The scene object determines the position and direction of the light. By default, a light with no transformation points down the positive Z axis towards the viewer.\n\n\nBuilt-in Light Classes\n\n\nThe light's class determines what lighting algorithm is used by the GPU. Three built-in light classes are provided by GearVRF which implement the Phong shading model per pixel. These work together with the Phong surface shader class GVRPhongShader.\n\n\n\n\nGVRDirectLight: A directional light is infinitely far away and illuminates only from a specific direction. This light has color and direction properties.\n\n\nGVRPointLight: A point light illuminates in all directions from a specific position. This light has color and position properties.\n\n\nGVRSpotLight: A spot light illuminates in a cone radiating from a point. It has color, position and direction.\n\n\n\n\nAll lights have an enabled uniform which enables or disables the light. The point and spot lights support attenuation factors which control how the light falls off with distance according to this equation:\n\n\nAttenuation = 1 / (attenuation_constant + attenuation_linear * distance + attenuation_quadratic * distance * distance)\n\n\nThe light object contains the data used by the fragment shader. It is accessed in terms of key / value pairs where the key is a string containing the name of the uniform and the value is a scalar or vector. GearVRF automatically loads these values into the fragment shader uniforms for you.\n\n\nShadows\n\n\nGearVRf can calculate shadow maps for directional and spot lights. You can enable shadow mapping get calling GVRLightBase.setCastShadow with a true parameter. Shadow mapping involves considerable overhead per frame because it renders the scene from the viewpoint of the light to calculate the shadow map. It also uses up more uniform variables, reducing the number of available bones for skinning from 60 to 50. If you disable shadow mapping on all lights, GearVRf will free up all resources used for shadow mapping and return to normal performance.\n\n\nBuilt-in Light Uniforms\n\n\nThis table describes the uniforms used by the built-in Phong lighting implementation.\n\n\n\n\n\n\n\n\nuniform name\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nint\n\n\n1 = light is enabled, 0 = disabled\n\n\n\n\n\n\nworld_position\n\n\nvec3\n\n\nposition of light in world space\n\n\n\n\n\n\nworld_direction\n\n\nvec3\n\n\norientation of light in world space\n\n\n\n\n\n\nspecular_exponent\n\n\nvec4\n\n\ncolor reflected by specular light\n\n\n\n\n\n\nambient_intensity\n\n\nvec4\n\n\nintensity of ambient light\n\n\n\n\n\n\ndiffuse_intensity\n\n\nvec4\n\n\nintensity of diffuse light\n\n\n\n\n\n\nspecular_intensity\n\n\nvec4\n\n\nintensity of specular light\n\n\n\n\n\n\nattenuation_constant\n\n\nfloat\n\n\nconstant attenuation factor\n\n\n\n\n\n\nattenuation_linear\n\n\nvec4\n\n\nlinear attenuation factor\n\n\n\n\n\n\nattenuation_quadratic\n\n\nfloat\n\n\nquadratic attenuation\n\n\n\n\n\n\n\n\nLight Construction Example\n\n\nA light is a component that is attached to a scene object which gives it both a position and a direction. An individual light can be enabled and disabled programmatically without causing shader compilation. All other light attributes are implementation specific. This example uses the lights built-into GearVRF which implement the Phong lighting model. Here we constructs a red spot light.\n\n\nGVRPhongSpotLight\n \ncreateSpotLight\n(\nGVRContext\n \ngvrContext\n)\n\n\n{\n\n    \nGVRPhongSpotLight\n \nlight\n \n=\n \nnew\n \nGVRPhongSpotLight\n(\ngvrContext\n);\n\n\n    \nlight\n.\nsetDiffuseIntensity\n(\n1\n,\n \n0\n,\n \n0\n);\n\n    \nlight\n.\nsetSpecularIntensity\n(\n1\n,\n \n0\n,\n \n0\n);\n\n    \nlight\n.\nsetInnerCone\n(\n10\n);\n\n    \nlight\n.\nsetOuterCone\n(\n15\n);\n\n    \nreturn\n \nlight\n;\n\n\n}\n\n\n\n\n\n\nYou need to attach your light to a GVRSceneObject before it can illuminate anything. To enable multiple lighting support the GVRPhongShader template must be selected. You can also turn the lighting effect on and off for a particular mesh. In this example we light a sphere with the spot light created above.\n\n\nGVRLightTemplate\n \nlight\n \n=\n \ncreateSpotLight\n(\ngvrContext\n);\n\n\nGVRSceneObject\n \nlightNode\n \n=\n \nnew\n \nGVRSceneObject\n(\ngvrContext\n);\n\n\nGVRSceneObject\n \nsphereNode\n \n=\n \nnew\n \nGVRSphereSceneObject\n(\ngvrContext\n);\n\n\n\nGVRRenderData\n \nrdata\n \n=\n \nsphereNode\n.\ngetRenderData\n();\n\n\nrdata\n.\nenableLight\n();\n\n\nrdata\n.\nsetShaderTemplate\n(\nGVRPhongShader\n.\nclass\n);", 
            "title": "Lighting"
        }, 
        {
            "location": "/programming_guide/features/lighting/#built-in-light-classes", 
            "text": "The light's class determines what lighting algorithm is used by the GPU. Three built-in light classes are provided by GearVRF which implement the Phong shading model per pixel. These work together with the Phong surface shader class GVRPhongShader.   GVRDirectLight: A directional light is infinitely far away and illuminates only from a specific direction. This light has color and direction properties.  GVRPointLight: A point light illuminates in all directions from a specific position. This light has color and position properties.  GVRSpotLight: A spot light illuminates in a cone radiating from a point. It has color, position and direction.   All lights have an enabled uniform which enables or disables the light. The point and spot lights support attenuation factors which control how the light falls off with distance according to this equation:  Attenuation = 1 / (attenuation_constant + attenuation_linear * distance + attenuation_quadratic * distance * distance)  The light object contains the data used by the fragment shader. It is accessed in terms of key / value pairs where the key is a string containing the name of the uniform and the value is a scalar or vector. GearVRF automatically loads these values into the fragment shader uniforms for you.", 
            "title": "Built-in Light Classes"
        }, 
        {
            "location": "/programming_guide/features/lighting/#shadows", 
            "text": "GearVRf can calculate shadow maps for directional and spot lights. You can enable shadow mapping get calling GVRLightBase.setCastShadow with a true parameter. Shadow mapping involves considerable overhead per frame because it renders the scene from the viewpoint of the light to calculate the shadow map. It also uses up more uniform variables, reducing the number of available bones for skinning from 60 to 50. If you disable shadow mapping on all lights, GearVRf will free up all resources used for shadow mapping and return to normal performance.", 
            "title": "Shadows"
        }, 
        {
            "location": "/programming_guide/features/lighting/#built-in-light-uniforms", 
            "text": "This table describes the uniforms used by the built-in Phong lighting implementation.     uniform name  type  description      enabled  int  1 = light is enabled, 0 = disabled    world_position  vec3  position of light in world space    world_direction  vec3  orientation of light in world space    specular_exponent  vec4  color reflected by specular light    ambient_intensity  vec4  intensity of ambient light    diffuse_intensity  vec4  intensity of diffuse light    specular_intensity  vec4  intensity of specular light    attenuation_constant  float  constant attenuation factor    attenuation_linear  vec4  linear attenuation factor    attenuation_quadratic  float  quadratic attenuation", 
            "title": "Built-in Light Uniforms"
        }, 
        {
            "location": "/programming_guide/features/lighting/#light-construction-example", 
            "text": "A light is a component that is attached to a scene object which gives it both a position and a direction. An individual light can be enabled and disabled programmatically without causing shader compilation. All other light attributes are implementation specific. This example uses the lights built-into GearVRF which implement the Phong lighting model. Here we constructs a red spot light.  GVRPhongSpotLight   createSpotLight ( GVRContext   gvrContext )  { \n     GVRPhongSpotLight   light   =   new   GVRPhongSpotLight ( gvrContext ); \n\n     light . setDiffuseIntensity ( 1 ,   0 ,   0 ); \n     light . setSpecularIntensity ( 1 ,   0 ,   0 ); \n     light . setInnerCone ( 10 ); \n     light . setOuterCone ( 15 ); \n     return   light ;  }   You need to attach your light to a GVRSceneObject before it can illuminate anything. To enable multiple lighting support the GVRPhongShader template must be selected. You can also turn the lighting effect on and off for a particular mesh. In this example we light a sphere with the spot light created above.  GVRLightTemplate   light   =   createSpotLight ( gvrContext );  GVRSceneObject   lightNode   =   new   GVRSceneObject ( gvrContext );  GVRSceneObject   sphereNode   =   new   GVRSphereSceneObject ( gvrContext );  GVRRenderData   rdata   =   sphereNode . getRenderData ();  rdata . enableLight ();  rdata . setShaderTemplate ( GVRPhongShader . class );", 
            "title": "Light Construction Example"
        }, 
        {
            "location": "/programming_guide/features/phong_shader_template/", 
            "text": "Phong shader tempates, vertex and fragment shaders, and examples\n\n\nThe phong reflectance model is used to calculate how objects reflect light. This model assumes that reflected light is most intense at an angle perpendicular to the light source and falls off in a lobe based on angle from the viewer. The base surface material is assumed to reflect evenly but texture maps can be used to modify the normal per pixel to provide bumps or control reflection per pixel. \n\n\n\n\nInstead of implementing an extremely complex single shader to handle all of the many combinations of texture maps, lighting and materials, GearVRF supports the concept of shader templates. A shader template is a complex shader with a lot of #ifdef statements which allow it to be compiled in different ways depending on what features are required to render an object. GearVRF will examine your meshes, materials and lights and set the #ifdefs to generate a custom shader for each case.\n\n\nThe asset loader uses the phong shader template for all imported assets. This means that, if you import an asset containing new lights, other objects in your scene may be affected and will use different custom shaders that support the newly added lights. Similarly, importing an object that uses lightmapping if it has not been used before in the scene might cause a new shader to be generated.\n\n\nTo use the phong shader template programmatically, call GVRRenderData.setShaderTemplate(GVRPhongShader.class). This tells GearVRF to defer selecting a specific shader until the scene has been composed. After GVRActivity.onInit completes, the shader templates are used to generate and compile the needed shaders. If you import assets while the application is running, the asset loader will take care of binding the shaders. But if you program creatively and add objects to the scene in other ways, you may have to call GVRScene.bindShaders to make sure the proper shaders are generated.\n\n\nThe phong model separates light into several different types and allows different colors for each. The components are combined with corresponding material uniforms to independently control illumination for each type.\n\n\n\n\nAmbient light reflects uniformly everywhere in the scene and is added to all objects\n\n\nDiffuse light reflects at many angles and is stronger in the direction of the surface normal\n\n\nSpecular light reflects towards the viewer\n\n\n\n\nEach light type has a corresponding color uniform to define the overall object color and a texture sampler to provide a color at each pixel.\n\n\nPhong Shader Example\n\n\n    \nGVRTexture\n \ntex\n \n=\n \ncontext\n.\nloadTexture\n(\nnew\n \nGVRAndroidResource\n(\nmGVRContext\n,\n \nR\n.\ndrawable\n.\ngearvrflogo\n));\n\n    \nGVRSceneObject\n \nplane\n \n=\n \nnew\n \nGVRSceneObject\n(\ncontext\n,\n \n10.0f\n,\n \n4.0f\n,\n \ntex\n);\n\n    \nGVRRenderData\n \nrdata\n \n=\n \nbackdrop\n.\ngetRenderData\n();\n\n    \nGVRMaterial\n \nmaterial\n \n=\n \nnew\n \nGVRMaterial\n(\ncontext\n);\n\n\n    \nmaterial\n.\nsetVec4\n(\ndiffuse_color\n,\n \n0.8f\n,\n \n0.8f\n,\n \n0.8f\n,\n \n1.0f\n);\n\n    \nmaterial\n.\nsetVec4\n(\nambient_color\n,\n \n0.3f\n,\n \n0.3f\n,\n \n0.3f\n,\n \n1.0f\n);\n\n    \nmaterial\n.\nsetVec4\n(\nspecular_color\n,\n \n1.0f\n,\n \n1.0f\n,\n \n1.0f\n,\n \n1.0f\n);\n\n    \nmaterial\n.\nsetVec4\n(\nemissive_color\n,\n \n0.0f\n,\n \n0.0f\n,\n \n0.0f\n,\n \n0.0f\n);\n\n    \nmaterial\n.\nsetFloat\n(\nspecular_exponent\n,\n \n10.0f\n);\n\n    \nmaterial\n.\nsetTexture\n(\ndiffuseTexture\n,\n \ntex\n);\n\n    \nrdata\n.\nsetMaterial\n(\nmaterial\n);\n\n    \nrdata\n.\nsetShaderTemplate\n(\nGVRPhongShader\n.\nclass\n);\n\n\n\n\n\n\nVertex Shader\n\n\nThe phong vertex shader supports lighting with multiple light sources, normal mapping and skinning with up to 60 bones. Lighting calculations are done per pixel.\n\n\nPhong Shader Vertex Attributes\n\n\n\n\n\n\n\n\nAttribute\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\na_position\n\n\nvec3\n\n\nX, Y, Z position in model space\n\n\n\n\n\n\na_normal\n\n\nvec3\n\n\nnormal vector in model space\n\n\n\n\n\n\na_texcoord\n\n\nvec2\n\n\nU, V texture coordinate for diffuse texture\n\n\n\n\n\n\na_tangent\n\n\nvec3\n\n\ntangent for normal mapping\n\n\n\n\n\n\na_bitangent\n\n\nvec3\n\n\nbitangent for normal mapping\n\n\n\n\n\n\na_bone_weights\n\n\nvec4\n\n\nweights for 4 bones for skinning\n\n\n\n\n\n\na_bone_indices\n\n\nivec4\n\n\nbone matrix indices for 4 bones\n\n\n\n\n\n\n\n\nThe vertex shader uses one or more matrices calculated each frame by GearVRF. These matrices are supplied to all shaders so they are available for you to use in your own vertex and fragment shader code.\n\n\nPhong Shader Matrix Uniforms\n\n\n\n\n\n\n\n\nUniform\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nu_model\n\n\nmat4\n\n\nmodel matrix (model -\n world)\n\n\n\n\n\n\nu_view\n\n\nmat4\n\n\nview matrix (world -\n camera)\n\n\n\n\n\n\nu_mvp\n\n\nmat4\n\n\nmodel, view, projection (model -\n screen)\n\n\n\n\n\n\nu_mv\n\n\nmat4\n\n\nmodel, view (model -\n camera)\n\n\n\n\n\n\nu_mv_it\n\n\nmat4\n\n\ninverse transpose of model, view (for lighting)\n\n\n\n\n\n\n\n\nFragment Shader\n\n\nThe fragment shader performs the lighting calculations at each pixel. Shadow mapping is supported for multiple light sources but should be used sparingly because it is computationally expensive. It renders the scene from the viewpoint of each light that casts shadows.\n\n\nMany different types of texture maps are supporting by the phong fragment shader template but usually a scene object only uses one or two. Each texture map contributes differently to the overall color and reacts differently to the lighting in the scene.\n\n\nPhong Texture Maps\n\n\n\n\n\n\n\n\nSampler\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndiffuseTexture\n\n\nSupplies diffuse color per pixel\n\n\n\n\n\n\nambientTexture\n\n\nSupplies ambient color per pixel\n\n\n\n\n\n\nspecularTexture\n\n\nSupplies specular color per pixel\n\n\n\n\n\n\nemissiveTexture\n\n\nSupplies emissive color per pixel\n\n\n\n\n\n\nnormalTexture\n\n\nSupplies normal per pixel\n\n\n\n\n\n\n\n\nMaterials used with the phong shader template support these uniforms. Each different type of light has its own set of uniforms used to define the light properties.\n\n\nPhong Material Uniforms\n\n\n\n\n\n\n\n\nUniform\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndiffuse_color\n\n\nvec4\n\n\ncolor reflected by diffuse light, alpha component has overall opacity\n\n\n\n\n\n\nambient_color\n\n\nvec4\n\n\ncolor reflected by ambient light\n\n\n\n\n\n\nspecular_color\n\n\nvec4\n\n\ncolor reflected by specular light (towards viewer)\n\n\n\n\n\n\nemissive_color\n\n\nvec4\n\n\nlight color emitted by object\n\n\n\n\n\n\nspecular_exponent\n\n\nfloat\n\n\nspecular exponent (shininess)\n\n\n\n\n\n\nu_lightmap_offset\n\n\nvec2\n\n\ntexture coordinate offset for lightmap texture\n\n\n\n\n\n\nu_lightmap_scale\n\n\nvec2\n\n\ntexture coordinate scale for lightmap texture", 
            "title": "Phong Shader Template"
        }, 
        {
            "location": "/programming_guide/features/phong_shader_template/#phong-shader-example", 
            "text": "GVRTexture   tex   =   context . loadTexture ( new   GVRAndroidResource ( mGVRContext ,   R . drawable . gearvrflogo )); \n     GVRSceneObject   plane   =   new   GVRSceneObject ( context ,   10.0f ,   4.0f ,   tex ); \n     GVRRenderData   rdata   =   backdrop . getRenderData (); \n     GVRMaterial   material   =   new   GVRMaterial ( context ); \n\n     material . setVec4 ( diffuse_color ,   0.8f ,   0.8f ,   0.8f ,   1.0f ); \n     material . setVec4 ( ambient_color ,   0.3f ,   0.3f ,   0.3f ,   1.0f ); \n     material . setVec4 ( specular_color ,   1.0f ,   1.0f ,   1.0f ,   1.0f ); \n     material . setVec4 ( emissive_color ,   0.0f ,   0.0f ,   0.0f ,   0.0f ); \n     material . setFloat ( specular_exponent ,   10.0f ); \n     material . setTexture ( diffuseTexture ,   tex ); \n     rdata . setMaterial ( material ); \n     rdata . setShaderTemplate ( GVRPhongShader . class );", 
            "title": "Phong Shader Example"
        }, 
        {
            "location": "/programming_guide/features/phong_shader_template/#vertex-shader", 
            "text": "The phong vertex shader supports lighting with multiple light sources, normal mapping and skinning with up to 60 bones. Lighting calculations are done per pixel.  Phong Shader Vertex Attributes     Attribute  Type  Description      a_position  vec3  X, Y, Z position in model space    a_normal  vec3  normal vector in model space    a_texcoord  vec2  U, V texture coordinate for diffuse texture    a_tangent  vec3  tangent for normal mapping    a_bitangent  vec3  bitangent for normal mapping    a_bone_weights  vec4  weights for 4 bones for skinning    a_bone_indices  ivec4  bone matrix indices for 4 bones     The vertex shader uses one or more matrices calculated each frame by GearVRF. These matrices are supplied to all shaders so they are available for you to use in your own vertex and fragment shader code.  Phong Shader Matrix Uniforms     Uniform  Type  Description      u_model  mat4  model matrix (model -  world)    u_view  mat4  view matrix (world -  camera)    u_mvp  mat4  model, view, projection (model -  screen)    u_mv  mat4  model, view (model -  camera)    u_mv_it  mat4  inverse transpose of model, view (for lighting)", 
            "title": "Vertex Shader"
        }, 
        {
            "location": "/programming_guide/features/phong_shader_template/#fragment-shader", 
            "text": "The fragment shader performs the lighting calculations at each pixel. Shadow mapping is supported for multiple light sources but should be used sparingly because it is computationally expensive. It renders the scene from the viewpoint of each light that casts shadows.  Many different types of texture maps are supporting by the phong fragment shader template but usually a scene object only uses one or two. Each texture map contributes differently to the overall color and reacts differently to the lighting in the scene.  Phong Texture Maps     Sampler  Description      diffuseTexture  Supplies diffuse color per pixel    ambientTexture  Supplies ambient color per pixel    specularTexture  Supplies specular color per pixel    emissiveTexture  Supplies emissive color per pixel    normalTexture  Supplies normal per pixel     Materials used with the phong shader template support these uniforms. Each different type of light has its own set of uniforms used to define the light properties.  Phong Material Uniforms     Uniform  Type  Description      diffuse_color  vec4  color reflected by diffuse light, alpha component has overall opacity    ambient_color  vec4  color reflected by ambient light    specular_color  vec4  color reflected by specular light (towards viewer)    emissive_color  vec4  light color emitted by object    specular_exponent  float  specular exponent (shininess)    u_lightmap_offset  vec2  texture coordinate offset for lightmap texture    u_lightmap_scale  vec2  texture coordinate scale for lightmap texture", 
            "title": "Fragment Shader"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/", 
            "text": "Legacy shaders are included for compatibility with older GearVRF versions and will be deprecated in the future. Lighting support is limited to a single point light or no lights at all.\n\n\nUNLIT_HORIZONTAL_STEREO_SHADER\n\n\nThe computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The u_right parameter controls whether it displays on the left half or the right half of the output display.\n\n\n\n\n\n\n\n\nuniform\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nu_texture\n\n\nsampler2D\n\n\ndiffuse texture\n\n\n\n\n\n\nu_color\n\n\nvec3\n\n\nRGB diffuse color\n\n\n\n\n\n\nu_opacity\n\n\nfloat\n\n\nalpha for transparency\n\n\n\n\n\n\nu_right\n\n\nint\n\n\n1 = right eye, 0 = left\n\n\n\n\n\n\n\n\nUNLIT_VERTICAL_STEREO_SHADER\n\n\nThe computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The u_right parameter controls whether it displays on the top half or the bottom half of the output display.\n\n\n\n\n\n\n\n\nuniform\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nu_texture\n\n\nsampler2D\n\n\ndiffuse texture\n\n\n\n\n\n\nu_color\n\n\nvec3\n\n\nRGB diffuse color\n\n\n\n\n\n\nu_opacity\n\n\nfloat\n\n\nalpha for transparency\n\n\n\n\n\n\nu_right\n\n\nint\n\n\n1 = right eye, 0 = left\n\n\n\n\n\n\n\n\nOES_SHADER\n\n\nThe computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The texture supplied as u_ texture must be and OES external texture (type GL_TEXTURE_EXTERNAL_OES, not GL_TEXTURE_2D) as this shader uses samplerExternalOES as opposed to sampler2D.\n\n\n\n\n\n\n\n\nuniform\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nu_texture\n\n\nsampler2D\n\n\ndiffuse texture\n\n\n\n\n\n\nu_color\n\n\nvec3\n\n\nRGB diffuse color\n\n\n\n\n\n\nu_opacity\n\n\nfloat\n\n\nalpha for transparency\n\n\n\n\n\n\nu_right\n\n\nint\n\n\ndescription\n\n\n\n\n\n\n\n\nOES_HORIZONTAL_STEREO_SHADER\n\n\nThe computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The texture supplied as u_ texture must be and OES external texture (type GL_TEXTURE_EXTERNAL_OES, not GL_TEXTURE_2D) as this shader uses samplerExternalOES as opposed to sampler2D.The u_right parameter controls whether it displays on the left half or the right half of the output display.\n\n\n\n\n\n\n\n\nuniform\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nu_texture\n\n\nsampler2D\n\n\ndiffuse texture\n\n\n\n\n\n\nu_color\n\n\nvec3\n\n\nRGB diffuse color\n\n\n\n\n\n\nu_opacity\n\n\nfloat\n\n\nalpha for transparency\n\n\n\n\n\n\nu_right\n\n\nint\n\n\n1 = right eye, 0 = left\n\n\n\n\n\n\n\n\nOES_VERTICAL_STEREO_SHADER\n\n\nThe computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The texture supplied as u_ texture must be and OES external texture (type GL_TEXTURE_EXTERNAL_OES, not GL_TEXTURE_2D) as this shader uses samplerExternalOES as opposed to sampler2D. The u_right parameter controls whether it displays on the top half or the bottom half of the output display.\n\n\n\n\n\n\n\n\nuniform\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nu_texture\n\n\nsampler2D\n\n\ndiffuse texture\n\n\n\n\n\n\nu_color\n\n\nvec3\n\n\nRGB diffuse color\n\n\n\n\n\n\nu_opacity\n\n\nfloat\n\n\nalpha for transparency\n\n\n\n\n\n\nu_right\n\n\nint\n\n\n1 = right eye, 0 = left\n\n\n\n\n\n\n\n\nCUBEMAP_SHADER\n\n\nThe computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The diffuse texture must be a cube map texture (six different textures for each face of the cube).\n\n\n\n\n\n\n\n\nuniform\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nu_texture\n\n\nsampler2D\n\n\ndiffuse texture\n\n\n\n\n\n\nu_color\n\n\nvec3\n\n\nRGB diffuse color\n\n\n\n\n\n\nu_opacity\n\n\nfloat\n\n\nalpha for transparency\n\n\n\n\n\n\nu_right\n\n\nint\n\n\ndescription\n\n\n\n\n\n\n\n\nCUBEMAP_REFLECTION_SHADER\n\n\nThe computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The diffuse texture must be a cube map texture (six different textures for each face of the cube).\n\n\n\n\n\n\n\n\nuniform\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nu_texture\n\n\nsampler2D\n\n\ndiffuse texture\n\n\n\n\n\n\nu_color\n\n\nvec3\n\n\nRGB diffuse color\n\n\n\n\n\n\nu_opacity\n\n\nfloat\n\n\nalpha for transparency\n\n\n\n\n\n\nu_view_i\n\n\nmat4atrix\n\n\nview matrix\n\n\n\n\n\n\nv_viewspace_position\n\n\nvec3\n\n\nview space position\n\n\n\n\n\n\nv_viewspace_normal\n\n\nvec3\n\n\nview space normal\n\n\n\n\n\n\n\n\nTEXTURE_SHADER\n\n\nThe computed fragment color is the product of the diffuse texture, diffuse color and opacity as illuminated by a single point light. It requires the vertex to have positions, normals and texture coordinates. If the scene is not lit, the material and light intensity properties are ignored. GVRPhongShader provides the same functionality and supports multiple lights.\n\n\n\n\n\n\n\n\nuniform\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nu_texture\n\n\nsampler2D\n\n\ndiffuse texture\n\n\n\n\n\n\nu_color\n\n\nvec3\n\n\nRGB diffuse color\n\n\n\n\n\n\nu_opacity\n\n\nfloat\n\n\nalpha for transparency\n\n\n\n\n\n\nambient_color\n\n\nvec4\n\n\ncolor reflected by ambient light\n\n\n\n\n\n\ndiffuse_color\n\n\nvec4\n\n\ncolor reflected by diffuse light\n\n\n\n\n\n\nspecular_color\n\n\nfloat\n\n\nexponent for specular reflection\n\n\n\n\n\n\nspecular_exponent\n\n\nvec4\n\n\ncolor reflected by specular light\n\n\n\n\n\n\nambient_intensity\n\n\nvec4\n\n\nintensity of ambient light\n\n\n\n\n\n\ndiffuse_intensity\n\n\nvec4\n\n\nintensity of diffuse light\n\n\n\n\n\n\nspecular_intensity\n\n\nvec4\n\n\nintensity of specular light\n\n\n\n\n\n\nv_view_space_normal\n\n\nvec4\n\n\nview space normal\n\n\n\n\n\n\nv_view_space_light_directionl\n\n\nvec4\n\n\nview space light direction\n\n\n\n\n\n\n\n\nLIGHTMAP_SHADER\n\n\nThe computed fragment color is the product of the diffuse texture, diffuse color and opacity as illuminated by a light map. It requires the vertex to have positions, normals and texture coordinates. GVRPhongShader supports light mapping integrated with other surface shading capabilities.\n\n\n\n\n\n\n\n\nuniform\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nu_main_texture\n\n\nsampler2D\n\n\ndiffuse texture\n\n\n\n\n\n\nu_lightmap_texture\n\n\nsampler2D\n\n\nlight map texture\n\n\n\n\n\n\nu_lightmap_offset\n\n\nvec2\n\n\nlight map offset\n\n\n\n\n\n\nu_lightmap_scale\n\n\nvec2\n\n\nlight map scal\n\n\n\n\n\n\n\n\nASSIMP_SHADER\n\n\nThe computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The GearVRF asset importer no longer uses this shader, it uses GVRPhongShader instead.\n\n\n\n\n\n\n\n\nuniform\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nu_texture\n\n\nsampler2D\n\n\ndiffuse texture\n\n\n\n\n\n\nu_color\n\n\nvec3\n\n\nRGB diffuse color\n\n\n\n\n\n\nu_opacity\n\n\nfloat\n\n\nalpha for transparency\n\n\n\n\n\n\nu_ambient_color\n\n\nvec4\n\n\ncolor reflected by ambient light\n\n\n\n\n\n\nu_diffuse_color\n\n\nvec4\n\n\ncolor reflected by diffuse light", 
            "title": "Built-In Legacy Shader"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/#unlit_horizontal_stereo_shader", 
            "text": "The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The u_right parameter controls whether it displays on the left half or the right half of the output display.     uniform  type  description      u_texture  sampler2D  diffuse texture    u_color  vec3  RGB diffuse color    u_opacity  float  alpha for transparency    u_right  int  1 = right eye, 0 = left", 
            "title": "UNLIT_HORIZONTAL_STEREO_SHADER"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/#unlit_vertical_stereo_shader", 
            "text": "The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The u_right parameter controls whether it displays on the top half or the bottom half of the output display.     uniform  type  description      u_texture  sampler2D  diffuse texture    u_color  vec3  RGB diffuse color    u_opacity  float  alpha for transparency    u_right  int  1 = right eye, 0 = left", 
            "title": "UNLIT_VERTICAL_STEREO_SHADER"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/#oes_shader", 
            "text": "The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The texture supplied as u_ texture must be and OES external texture (type GL_TEXTURE_EXTERNAL_OES, not GL_TEXTURE_2D) as this shader uses samplerExternalOES as opposed to sampler2D.     uniform  type  description      u_texture  sampler2D  diffuse texture    u_color  vec3  RGB diffuse color    u_opacity  float  alpha for transparency    u_right  int  description", 
            "title": "OES_SHADER"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/#oes_horizontal_stereo_shader", 
            "text": "The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The texture supplied as u_ texture must be and OES external texture (type GL_TEXTURE_EXTERNAL_OES, not GL_TEXTURE_2D) as this shader uses samplerExternalOES as opposed to sampler2D.The u_right parameter controls whether it displays on the left half or the right half of the output display.     uniform  type  description      u_texture  sampler2D  diffuse texture    u_color  vec3  RGB diffuse color    u_opacity  float  alpha for transparency    u_right  int  1 = right eye, 0 = left", 
            "title": "OES_HORIZONTAL_STEREO_SHADER"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/#oes_vertical_stereo_shader", 
            "text": "The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The texture supplied as u_ texture must be and OES external texture (type GL_TEXTURE_EXTERNAL_OES, not GL_TEXTURE_2D) as this shader uses samplerExternalOES as opposed to sampler2D. The u_right parameter controls whether it displays on the top half or the bottom half of the output display.     uniform  type  description      u_texture  sampler2D  diffuse texture    u_color  vec3  RGB diffuse color    u_opacity  float  alpha for transparency    u_right  int  1 = right eye, 0 = left", 
            "title": "OES_VERTICAL_STEREO_SHADER"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/#cubemap_shader", 
            "text": "The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The diffuse texture must be a cube map texture (six different textures for each face of the cube).     uniform  type  description      u_texture  sampler2D  diffuse texture    u_color  vec3  RGB diffuse color    u_opacity  float  alpha for transparency    u_right  int  description", 
            "title": "CUBEMAP_SHADER"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/#cubemap_reflection_shader", 
            "text": "The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The diffuse texture must be a cube map texture (six different textures for each face of the cube).     uniform  type  description      u_texture  sampler2D  diffuse texture    u_color  vec3  RGB diffuse color    u_opacity  float  alpha for transparency    u_view_i  mat4atrix  view matrix    v_viewspace_position  vec3  view space position    v_viewspace_normal  vec3  view space normal", 
            "title": "CUBEMAP_REFLECTION_SHADER"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/#texture_shader", 
            "text": "The computed fragment color is the product of the diffuse texture, diffuse color and opacity as illuminated by a single point light. It requires the vertex to have positions, normals and texture coordinates. If the scene is not lit, the material and light intensity properties are ignored. GVRPhongShader provides the same functionality and supports multiple lights.     uniform  type  description      u_texture  sampler2D  diffuse texture    u_color  vec3  RGB diffuse color    u_opacity  float  alpha for transparency    ambient_color  vec4  color reflected by ambient light    diffuse_color  vec4  color reflected by diffuse light    specular_color  float  exponent for specular reflection    specular_exponent  vec4  color reflected by specular light    ambient_intensity  vec4  intensity of ambient light    diffuse_intensity  vec4  intensity of diffuse light    specular_intensity  vec4  intensity of specular light    v_view_space_normal  vec4  view space normal    v_view_space_light_directionl  vec4  view space light direction", 
            "title": "TEXTURE_SHADER"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/#lightmap_shader", 
            "text": "The computed fragment color is the product of the diffuse texture, diffuse color and opacity as illuminated by a light map. It requires the vertex to have positions, normals and texture coordinates. GVRPhongShader supports light mapping integrated with other surface shading capabilities.     uniform  type  description      u_main_texture  sampler2D  diffuse texture    u_lightmap_texture  sampler2D  light map texture    u_lightmap_offset  vec2  light map offset    u_lightmap_scale  vec2  light map scal", 
            "title": "LIGHTMAP_SHADER"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/#assimp_shader", 
            "text": "The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The GearVRF asset importer no longer uses this shader, it uses GVRPhongShader instead.     uniform  type  description      u_texture  sampler2D  diffuse texture    u_color  vec3  RGB diffuse color    u_opacity  float  alpha for transparency    u_ambient_color  vec4  color reflected by ambient light    u_diffuse_color  vec4  color reflected by diffuse light", 
            "title": "ASSIMP_SHADER"
        }, 
        {
            "location": "/programming_guide/features/picking/", 
            "text": "3D object picking, colliders, and examples\n\n\nFor a scene object to be pickable, it must have a collider component attached. The collider typically references collision geometry that is simpler than the scene object's mesh. For example, the collider might be a sphere or an axially aligned bounding box.\n\n\nTo pick a 3D object GearVRf casts a ray from the camera viewpoint in the direction the viewer is looking through the entire scene looking only at the geometry in the colliders. When the ray penetrates the collider geometry, the scene object that owns it is \"picked\". The list of picked objects is sorted based on distance from the camera so it is easy to choose the closest object to the viewer.\n\n\n\n\nTypes of Colliders\n\n\nGearVRf provides several types of colliders to use depending on how accurate you want picking to be.\n\n\n\n\nGVRSphereCollider\n is the fastest collision to compute but is the least accurate because it approximates the shape of the scene object as spherical. For meshes that are larger in one dimension than another, the picker might register false positives.\n\n\nGVRMeshCollider\n can be used in several ways. You can direct it to use the mesh of the scene object that owns it or you can provide your own collision mesh. You can also request the mesh collider to use the bounding box of the scene object's mesh. This is usually a lot faster and sufficient for a lot of picking needs. It accommodates irregularly shaped objects better than the sphere collider.\n\n\n\n\nPicking\n\n\nThe picking operation is performed by the GVRPicker class. The picker can operate in two modes. You can call the pickObjects function directly to get back the list of objects that were picked and information about the collision. You can also attach the picker to a scene object and it will automatically cast a ray from that scene object and generate events indicating what was picked.\n\n\nProcedural Picking\n\n\nTo use the picker procedurally you must provide the origin and direction of the pick ray in world coordinates and the GVRScene you want to pick against. The picker returns an array of GVRPickedObject instances that indicate what was picked and where it was hit. The hit position returned will be in the coordinate system of the collider geometry - not in world coordinates. To transform it to world coordinates you must multiply it by the model matrix of the scene object hit.\n\n\n\n\n\n\n\n\nGVRPicker.GVRPickedObject\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHitObject\n\n\nGVRSceneObject\n\n\nthe scene object that was hit\n\n\n\n\n\n\nHitCollider\n\n\nGVRCollider\n\n\nthe collider that was hit\n\n\n\n\n\n\nHitPosition\n\n\nfloat[3]\n\n\nX, Y, Z coordinates of where collider was hit\n\n\n\n\n\n\nHitDistance\n\n\nfloat\n\n\ndistance from camera in world coordinates\n\n\n\n\n\n\n\n\nPicking Events\n\n\nThe most convenient way to use the picker is to attach it to a scene object, typically the owner of the current camera, and respond to the pick events generated when objects are hit. Events are raised each time the picking ray enters or exits an object. You can also observe changes to the list of picked objects as a whole.\n\n\nTo handle pick events in your application you provide a class which implements the IPickEvents interface and attach it as a listener to the scene's event receiver. (The picker routes all pick events through the scene.)\n\n\nPicking Example\n\n\nThis example shows how to use picking events to do selection highlighting. When the pick ray enters an object, its material color is changed to red. When the pick ray exists, the color is changed back to white again.\n\n\npublic\n \nclass\n \nPickHandler\n \nimplements\n \nIPickEvents\n\n\n{\n\n    \npublic\n \nvoid\n \nonEnter\n(\nGVRSceneObject\n \nsceneObj\n,\n \nGVRPicker\n.\nGVRPickedObject\n \npickInfo\n)\n\n    \n{\n\n         \nsceneObj\n.\ngetRenderData\n().\ngetMaterial\n().\nsetDiffuseColor\n(\n1\n,\n \n0\n,\n \n0\n,\n \n1\n);\n\n    \n}\n\n    \npublic\n \nvoid\n \nonExit\n(\nGVRSceneObject\n \nsceneObj\n,\n \nGVRPicker\n.\nGVRPickedObject\n \npickInfo\n)\n\n    \n{\n\n        \nsceneObj\n.\ngetRenderData\n().\ngetMaterial\n().\nsetDiffuseColor\n(\n1\n,\n \n1\n,\n \n1\n,\n \n1\n);\n\n    \n}\n\n    \npublic\n \nvoid\n \nonInside\n(\nGVRSceneObject\n \nsceneObj\n,\n \nGVRPicker\n.\nGVRPickedObject\n \npickInfo\n)\n \n{\n \n}\n\n    \npublic\n \nvoid\n \nonPick\n(\nGVRPicker\n)\n \n{\n \n}\n\n    \npublic\n \nvoid\n \nonNoPick\n(\nGVRPicker\n)\n \n{\n \n}\n\n\n}\n\n\n\npublic\n \nvoid\n \nonInit\n(\nGVRContext\n \ncontext\n)\n\n\n{\n\n    \nGVRScene\n \nscene\n \n=\n \ncontext\n.\ngetNextMainScene\n();\n\n    \n{\n\n         \nGVRSceneObject\n \nsphere\n \n=\n \nnew\n \nGVRSphereSceneObject\n(\ncontext\n);\n\n         \nscene\n.\ngetEventReceiver\n().\naddListener\n(\nnew\n \nPickHandler\n());\n\n         \nscene\n.\ngetMainCameraRig\n().\ngetOwnerObject\n().\nattachComponent\n(\nnew\n \nGVRPicker\n(\ncontext\n,\n \nscene\n));\n\n         \nsphere\n.\ngetTransform\n().\nsetPositionZ\n(-\n2.0f\n);\n\n         \nsphere\n.\nattachComponent\n(\nnew\n \nGVRSphereCollider\n(\ncontext\n));\n\n         \nscene\n.\naddSceneObject\n(\nsphere\n);\n\n    \n}\n\n\n}", 
            "title": "Picking"
        }, 
        {
            "location": "/programming_guide/features/picking/#types-of-colliders", 
            "text": "GearVRf provides several types of colliders to use depending on how accurate you want picking to be.   GVRSphereCollider  is the fastest collision to compute but is the least accurate because it approximates the shape of the scene object as spherical. For meshes that are larger in one dimension than another, the picker might register false positives.  GVRMeshCollider  can be used in several ways. You can direct it to use the mesh of the scene object that owns it or you can provide your own collision mesh. You can also request the mesh collider to use the bounding box of the scene object's mesh. This is usually a lot faster and sufficient for a lot of picking needs. It accommodates irregularly shaped objects better than the sphere collider.", 
            "title": "Types of Colliders"
        }, 
        {
            "location": "/programming_guide/features/picking/#picking", 
            "text": "The picking operation is performed by the GVRPicker class. The picker can operate in two modes. You can call the pickObjects function directly to get back the list of objects that were picked and information about the collision. You can also attach the picker to a scene object and it will automatically cast a ray from that scene object and generate events indicating what was picked.", 
            "title": "Picking"
        }, 
        {
            "location": "/programming_guide/features/picking/#procedural-picking", 
            "text": "To use the picker procedurally you must provide the origin and direction of the pick ray in world coordinates and the GVRScene you want to pick against. The picker returns an array of GVRPickedObject instances that indicate what was picked and where it was hit. The hit position returned will be in the coordinate system of the collider geometry - not in world coordinates. To transform it to world coordinates you must multiply it by the model matrix of the scene object hit.     GVRPicker.GVRPickedObject        HitObject  GVRSceneObject  the scene object that was hit    HitCollider  GVRCollider  the collider that was hit    HitPosition  float[3]  X, Y, Z coordinates of where collider was hit    HitDistance  float  distance from camera in world coordinates", 
            "title": "Procedural Picking"
        }, 
        {
            "location": "/programming_guide/features/picking/#picking-events", 
            "text": "The most convenient way to use the picker is to attach it to a scene object, typically the owner of the current camera, and respond to the pick events generated when objects are hit. Events are raised each time the picking ray enters or exits an object. You can also observe changes to the list of picked objects as a whole.  To handle pick events in your application you provide a class which implements the IPickEvents interface and attach it as a listener to the scene's event receiver. (The picker routes all pick events through the scene.)", 
            "title": "Picking Events"
        }, 
        {
            "location": "/programming_guide/features/picking/#picking-example", 
            "text": "This example shows how to use picking events to do selection highlighting. When the pick ray enters an object, its material color is changed to red. When the pick ray exists, the color is changed back to white again.  public   class   PickHandler   implements   IPickEvents  { \n     public   void   onEnter ( GVRSceneObject   sceneObj ,   GVRPicker . GVRPickedObject   pickInfo ) \n     { \n          sceneObj . getRenderData (). getMaterial (). setDiffuseColor ( 1 ,   0 ,   0 ,   1 ); \n     } \n     public   void   onExit ( GVRSceneObject   sceneObj ,   GVRPicker . GVRPickedObject   pickInfo ) \n     { \n         sceneObj . getRenderData (). getMaterial (). setDiffuseColor ( 1 ,   1 ,   1 ,   1 ); \n     } \n     public   void   onInside ( GVRSceneObject   sceneObj ,   GVRPicker . GVRPickedObject   pickInfo )   {   } \n     public   void   onPick ( GVRPicker )   {   } \n     public   void   onNoPick ( GVRPicker )   {   }  }  public   void   onInit ( GVRContext   context )  { \n     GVRScene   scene   =   context . getNextMainScene (); \n     { \n          GVRSceneObject   sphere   =   new   GVRSphereSceneObject ( context ); \n          scene . getEventReceiver (). addListener ( new   PickHandler ()); \n          scene . getMainCameraRig (). getOwnerObject (). attachComponent ( new   GVRPicker ( context ,   scene )); \n          sphere . getTransform (). setPositionZ (- 2.0f ); \n          sphere . attachComponent ( new   GVRSphereCollider ( context )); \n          scene . addSceneObject ( sphere ); \n     }  }", 
            "title": "Picking Example"
        }, 
        {
            "location": "/programming_guide/api_reference/", 
            "text": "Online GearVRf API Reference (\nhttp://docs.gearvrf.org\n)", 
            "title": "API Reference"
        }, 
        {
            "location": "/programming_guide/build_instructions/", 
            "text": "GearVRf app requirements, build instructions, building and running, and sample apps\n\n\nIn order to build and and run GearVRf applications (your own or sample apps) in Android Studio, you have two options:\n\n\n\n\n(Preferred) Download the \nlatest release\n of previously built Gear VR Framework .aar files\n\n\nOptionally, locally build the Gear VR Framework from the latest \nsource code\n\n\n\n\nPrerequisites\n\n\nFor BOTH methods, locally building AND using a pre-built framework, the following prerequisites must be met.\n\n\n1. Install Required SDKs\n\n\nMake sure follow the \nGetting Started Guide\n and install all the required SDKs\n\n\n2. Building Gear VR Framework\n\n\nYou can optionally locally build GearVRf from source code using Android Studio.\n\n\nHere are the steps:\n\n\n\n\nAdd \nOVR_MOBILE_SDK\n to \ngradle.properties\n and set it to the path to the Oculus Mobile SDK; recommended to use the global gradle.properties \n$HOMEPATH/.gradle/gradle.properties\n or \n~/.gradle/gradle.properties)\n.\n\n\nAdd \nANDROID_NDK_HOM\n to gradle.properties and set it to the path to the Android NDK installation.\n\n\nNavigate to the GearVR Framework and select the folder, and click \nOK\n\n\nClick \nMake Project\n (from the \nBuild\n menu)\n\n\n\n\n3. Building and Running GearVRf Applications\n\n\nAfter you have the Gear VR Framework, by either locally building or using a pre-built framework, you can now import, build, and run GearVRf applications (your own or sample apps) in Android Studio. The specific procedure you use depends on whether you locally built the Gear VR Framework from source code files or used pre-built framework files.\n\n\n\n\nWhen building your own GearVRf apps from scratch, copy the appropriate device XML file from the GearVRf SDK to your application's assets folder. \nGearVRf provides an xml file for you to use: gvr.xml.\n\n\nImport the GearVRf application code.\n\n\nClick \nFile -\n Open ...\n\n\nNavigate to the project folder (for example, gvr-simplesample).\n\n\nClick \nOK\n\n\n\n\n\n\nClean and build the application.\n\n\nGo to the \nBuild\n menu and click \nClean\n...\n\n\nClick \nMake Project\n (from the \nBuild\n menu)\n\n\n\n\n\n\nRun the application.\n\n\nConnect an Android mobile device to your local machine.\n\n\nSelect your project in the project explorer\n\n\nClick \nRun\n on the toolbar\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nYou may need to apply to Oculus for a signature file for your device.\nFor application and use details, refer to the online signature generator (\nhttps://dashboard.oculus.com/tools/osig-generator/\n)\n\n\n\n\nGenerate Javadocs\n\n\nWhen locally building the Gear VR Framework, you can optionally generate Javadoc files with details about the GearVRf API.\n\n\nOptional: To get GearVRf API reference details by generating GearVRf Javadoc files in the specified directory:\n\n\n\n\nIn Android Studio, click \nProject \n Generate Javadoc...\n\n\nIn the Generate Javadoc window:\n\n\nJavadoc command\n: (Pathname to the Javadoc executable file) Typically, in the bin directory of the Java directory.\n\n\nClick on the plus-icon to \nexpand the Framework listing\n.\n\n\nCheckmark \nsrc\n\n\nSelect \nUse standard doclet\n\n\nDestination:\n (Convenient local directory to contain the Javadoc files)\n\n\nOptional Specify VM options:\n    NOTE: You may encounter errors if VM options is not specified.\n\n\nClick \nNext \n\n\nClick \nNext \n\n\nVM options: -bootclasspath \npath to your Android jar file\n\n\n\n\n\n\nClick \nFinish", 
            "title": "Build Instructions"
        }, 
        {
            "location": "/programming_guide/build_instructions/#prerequisites", 
            "text": "For BOTH methods, locally building AND using a pre-built framework, the following prerequisites must be met.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/programming_guide/build_instructions/#1-install-required-sdks", 
            "text": "Make sure follow the  Getting Started Guide  and install all the required SDKs", 
            "title": "1. Install Required SDKs"
        }, 
        {
            "location": "/programming_guide/build_instructions/#2-building-gear-vr-framework", 
            "text": "You can optionally locally build GearVRf from source code using Android Studio.  Here are the steps:   Add  OVR_MOBILE_SDK  to  gradle.properties  and set it to the path to the Oculus Mobile SDK; recommended to use the global gradle.properties  $HOMEPATH/.gradle/gradle.properties  or  ~/.gradle/gradle.properties) .  Add  ANDROID_NDK_HOM  to gradle.properties and set it to the path to the Android NDK installation.  Navigate to the GearVR Framework and select the folder, and click  OK  Click  Make Project  (from the  Build  menu)", 
            "title": "2. Building Gear VR Framework"
        }, 
        {
            "location": "/programming_guide/build_instructions/#3-building-and-running-gearvrf-applications", 
            "text": "After you have the Gear VR Framework, by either locally building or using a pre-built framework, you can now import, build, and run GearVRf applications (your own or sample apps) in Android Studio. The specific procedure you use depends on whether you locally built the Gear VR Framework from source code files or used pre-built framework files.   When building your own GearVRf apps from scratch, copy the appropriate device XML file from the GearVRf SDK to your application's assets folder. \nGearVRf provides an xml file for you to use: gvr.xml.  Import the GearVRf application code.  Click  File -  Open ...  Navigate to the project folder (for example, gvr-simplesample).  Click  OK    Clean and build the application.  Go to the  Build  menu and click  Clean ...  Click  Make Project  (from the  Build  menu)    Run the application.  Connect an Android mobile device to your local machine.  Select your project in the project explorer  Click  Run  on the toolbar      Note  You may need to apply to Oculus for a signature file for your device.\nFor application and use details, refer to the online signature generator ( https://dashboard.oculus.com/tools/osig-generator/ )", 
            "title": "3. Building and Running GearVRf Applications"
        }, 
        {
            "location": "/programming_guide/build_instructions/#generate-javadocs", 
            "text": "When locally building the Gear VR Framework, you can optionally generate Javadoc files with details about the GearVRf API.  Optional: To get GearVRf API reference details by generating GearVRf Javadoc files in the specified directory:   In Android Studio, click  Project   Generate Javadoc...  In the Generate Javadoc window:  Javadoc command : (Pathname to the Javadoc executable file) Typically, in the bin directory of the Java directory.  Click on the plus-icon to  expand the Framework listing .  Checkmark  src  Select  Use standard doclet  Destination:  (Convenient local directory to contain the Javadoc files)  Optional Specify VM options:\n    NOTE: You may encounter errors if VM options is not specified.  Click  Next   Click  Next   VM options: -bootclasspath  path to your Android jar file    Click  Finish", 
            "title": "Generate Javadocs"
        }, 
        {
            "location": "/programming_guide/gearvr_settings_files/", 
            "text": "Definitions of GearVRf XML settings file parameters\n\n\nBefore rendering can start, the framework needs to know about the characteristics of the display device. These are specified in the XML settings file passed to GVRActivity.setMain or GVRActivity.setScript when your application is being initialized.\n\n\n\n\n\n\n\n\nvr-app-settings\n\n\n\n\n\n\n\n\n\n\n\n\nframebufferPixelsHigh\n\n\nHeight of the framebuffer\n\n\n\n\n\n\nframebufferPixelsWide\n\n\nWidth of the framebuffer\n\n\n\n\n\n\nshowLoadingIcon\n\n\nEnable / disable loading icon\n\n\n\n\n\n\nuseProtectedFramebuffer\n\n\nEnable / disable use of protected framebuffer\n\n\n\n\n\n\nuseSrgbFramebuffer\n\n\nEnable / disable use of SRGB framebuffer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmono-mode-parms\n\n\n\n\n\n\n\n\n\n\n\n\nallowPowerSave\n\n\nIf enabled, the application will run at 30 fps when power is low. Otherwise, it will show an error message when power is low.\n\n\n\n\n\n\nresetWindowFullScreen\n\n\nIf enabled, the fullscreen flag of the activity window will be on when a VR activity returns from background to foreground. It will help performance since it won't draw a DecorView as background.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nperformance-parms\n\n\n\n\n\n\n\n\n\n\n\n\ncpuLevel\n\n\nCPU clock level\n\n\n\n\n\n\ngpuLevel\n\n\nGPU clock level\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neye-buffer-parms\n\n\n\n\n\n\n\n\n\n\n\n\ncolorFormat\n\n\nFormat of the color buffer (default COLOR_8888) \n COLOR_5551 5 bits R,G,B, 1 bit alpha \n COLOR_565 5 bits red, 6 bits green, 5 bits blue\n COLOR_4444 4 bits RGBA\n COLOR_888 8 bits RGBA \n COLOR_888_sRGB SRGB color format \n COLOR_RGBA16F 16 bits float RGBA\n\n\n\n\n\n\ndepthFormat\n\n\nFormat of the depth buffer (default DEPTH_24) \n DEPTH_0 no depth buffer \n DEPTH_16 16 bit depth buffer \n DEPTH_24 24 bit depth buffer \n DEPTH_24_STENCIL_8 32 bit depth buffer\n\n\n\n\n\n\nfov-y\n\n\nY field of view in degrees (default 90)\n\n\n\n\n\n\nresolutionWidth\n\n\nEye buffer resolution width in pixels (default 1024)\n\n\n\n\n\n\nresolutionHeight\n\n\nEye buffer resolution height in pixels (default 1024)\n\n\n\n\n\n\nresolveDepth\n\n\nTrue to resolve framebuffer to a texture (default false)\n\n\n\n\n\n\nmultiSamples\n\n\nNumber of framebuffer multisamples for anti-aliasing \n 1 = no multisampling (not recommended) \n 2 = 2xMSAA recommended setting \n 4 = 4xMSAA Higher visual quality but lower performance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhead-model-parms\n\n\n\n\n\n\n\n\n\n\n\n\neyeHeight\n\n\nDistance from ground to eye\n\n\n\n\n\n\nheadModelDepth\n\n\nOffset of head center ahead of eyes based on eye height\n\n\n\n\n\n\nheadModelHeight\n\n\nDistance from neck joint to eyes based on eye height\n\n\n\n\n\n\ninterpupillaryDistance\n\n\nDistance between left and right eye", 
            "title": "GearVRf Settings File"
        }, 
        {
            "location": "/programming_guide/faq/", 
            "text": "1. Is there any example of object following the head tracking, just like a reticle?\n\n\nSee \ngvr-tutorial-lesson2 sample\n. Examine the BalloonMain.java and the headTracker tracker object it sets up. The key part is adding the object to the main camera rig.\n\n\n2. I want to implement a scrollable list of item like ListView in Android. How to go about that?\n\n\n\n\nBackground objects: rendering order N, depth test on\n\n\nClip object: rendering order N+1, depth test on, alpha blend on, alpha = 0 (completely transparent)\n\n\nList view object: rendering order N+2, depth test on\n\n\n\n\nThe clip object should be a plane with a hole in it where you want to see the list view. It should be completely transparent. It will be rendered after the background so it will update the depth buffer but the background will show thru completely. This clip object should have a Z value putting it IN FRONT of the list view object even though it will be rendered before that object (because you set the rendering order to a smaller value).\n\n\nGearVRF will render objects in ascending rendering order so the background will be rendered first. The clip object will update the depth buffer so that anything drawn BEHIND it will show thru the hole but will be obscured by the transparent clip area (the depth buffer will do the clipping for us).\n\n\n3. Can i use an emulator during development for testing?\n\n\nShort answer: No.\n\n\nLong answer: It would likely be somewhat painful to do. Oculus only provides 32bit arm libraries. Which means you would need to set up an arm emulator (rather than an x86 one). In that emulator, we would detect the oculus service is not on the system and fall back to daydream. However, in our experience, running an arm emulator is horrifically slow, especially for anything GL related. It's best to stick with a physical phone for development.\n\n\n4. I am using Windows, trying to build the framework and getting weird errors. Like this one:\n\n\n...\\GearVRf\\GVRf\\Framework\\framework\\..\\backend_oculus/src/main/jni/util/configuration_helper.cpp:235:1: fatal error: opening dependency file ./obj/local/armeabi-v7a/objs/...\\GearVRf\\GVRf\\Framework\\framework\\..\\backend_oculus/src/main/jni/util/configuration_helper.o.d: No such file or directory\n\n\n\n\n\nYour paths might be too long. Try moving the framework to C:\\ and build again.\n\n\n5. I want to inflate and show an Android view. Can I do that?\n\n\nYes. The gvr-renderableview sample shows how to do that.\n\n\n6. I want to use ExoPlayer instead of MediaPlayer for video playback. Can I do this?\n\n\nYes. See the gvr-360video sample, which allows you to use either. Set the USE_EXO_PLAYER flag in Minimal360VideoActivity.java.\n\n\n7. How can I create a mixed VR android app and launching VR Mode later, by clicking a button for example? I need to create an activity visualized in normal mode for settings and later launch a VR mode, showing the \"you need gear vr\" screen if you have not attached it.\n\n\nUnfortunately, this is not supported. Apps get marked as \"vr\" not individual activities. Which means the prompt will show when you try to launch your \"normal\" activity. This is not a gvrf limitation.\n\n\n8. Trying to build a sample but I get the following error:\n\n\nWhat\n \nwent\n \nwrong\n:\n\n\nExecution\n \nfailed\n \nfor\n \ntask\n \n:app:transformClassesWithDexForDebug\n.\n\n\n \ncom\n.\nandroid\n.\nbuild\n.\napi\n.\ntransform\n.\nTransformException\n:\n \ncom\n.\nandroid\n.\nide\n.\ncommon\n.\nprocess\n.\nProcessException\n:\n \njava\n.\nutil\n.\nconcurrent\n.\nExecutionException\n:\n \ncom\n.\nandroid\n.\ndex\n.\nDexException\n:\n \nMultiple\n \ndex\n \nfiles\n \ndefine\n \nLcom\n/\noculus\n/\nsystemutils\n/\nBuildConfig\n;\n\n\n\n\n\n\nMost likely you still have VrApi.jar and SystemUtils.jar under the framework module (GearVRf/GVRf/Framework/framework/src/main/libs/). Please remove them from there, clean and build.\n\n\n9. I am using Linux and getting a strange aapt error during the build. Something like \njava.io.IOException: Cannot run program \"/aapt\": error=2, No such file or directory\n\n\nYou might be missing support for executing 32bit binaries and/or libraries aapt depends on. Please run the following:\n\n\nsudo dpkg --add-architecture i386\nsudo apt-get update\nsudo apt-get install libc6:i386 libncurses5:i386 libstdc++6:i386\nsudo apt-get install zlib1g:i386\n\n\n\n\n\n10. Is there support for the Oculus Platform SDK?\n\n\nYes, the entitlement check is supported. Go to GVRf/Extensions/. There is a platformsdk_support module. To build it run \n./gradlew -Pplatformsdk_support=true platformsdk_support:assembleDebug\n. Checkout the javadoc in PlatformEntitlementCheck.java. Have been verified to work with Platform SDK versions 1.6, 1.7 and 1.8. For further information see \nhttps://github.com/Samsung/GearVRf/wiki/Entitlement-Check-using-GVRF\n\n\n11. Is there any way to play youtube video from url?\n\n\nYes. See \nhttps://github.com/Samsung/GearVRf/issues/1033#issuecomment-278244683\n\n\n12. I am trying to use GVRF on a Google Pixel phone and I get this exception:\n\n\n02-15 19:53:15.697 23156-23156/? E/AndroidRuntime: FATAL EXCEPTION: main\nProcess: pl.lynx.daydream.test, PID: 23156\njava.lang.UnsatisfiedLinkError: dalvik.system.PathClassLoader[DexPathList[[zip file \n/data/app/pl.lynx.daydream.test-2/base.apk\n],nativeLibraryDirectories=[/data/app/pl.lynx.daydream.test-2/lib/arm64, /system/fake-libs64, /data/app/pl.lynx.daydream.test-2/base.apk!/lib/arm64-v8a, /system/lib64, /vendor/lib64]]] couldn\nt find \nlibgvrf.so\n\n\n\n\n\n\nDaydream has 64bit binaries but GVRf only supports 32bit binaries. In your app's gradle file you need to add this:\n\n\nandroid {\n\n    // ignore the x86 and arm-v8 files from the google vr libraries\n    packagingOptions {\n        exclude \nlib/x86/libgvr.so\n\n        exclude \nlib/arm64-v8a/libgvr.so\n\n    }\n}\n\n\n\n\n\n13. I used to build the demos from the GearVRf-Demos repo just fine. Suddenly I am getting errors. What happened?\n\n\nThe master branch is subject to frequent improvements. The GVRf team pushes updated framework snapshots to the maven repo, but due to the gradle's caching you are most likely using outdated snapshot. Please pass the --refresh-dependencies argument to gradlew if you are building from the command line. Or you can just delete the gradle cache via \n\n\nrm -rf ~/.gradle/caches/.\n\n\n\n\n\nAlternatively you could use the 3.1 branch which is stable. After cloning the demos repo, switch to the release_v3.1 branch.\n\n\n14. I am building an app from scratch. How do I add support for GVRF to my app? What are the minimum dependencies?\n\n\nPlease see this bare-bones project that can serve as a reference: \nhttps://github.com/gearvrf/GearVRf-Demos/tree/master/template/GVRFApplication\n.\n\n\n15. My app in the Oculus Store fails to install on Android N devices. I get an UNTRUSTED_APK_ERROR error.\n\n\nOculus doesn't support APK signature scheme v2 yet. It should be disabled if you plan to submit apps to the Oculus store. Android Studio seems to apply the scheme unconditionally. Build from the command line and include the following section in your gradle file:\n\n\nandroid {\n    signingConfigs {\n        release {\n            v2SigningEnabled false\n            storeFile file(\nfull-path-to-your-store-file\n)\n            storePassword \nyour_store_pwd\n\n            keyAlias \nalias\n\n            keyPassword \nkey_pwd\n\n        }\n    }\n    defaultConfig {\n        signingConfig signingConfigs.release\n    }\n}\n\n\n\n\n\n16. Can you run GearVR app on your phone without GearVR?\n\n\nYes, in your phone's Settings-\nApplications-\nApplication Manager-\nGear VR Service-\nManage Storage Tab on VR Service Version multiple times until the 'Developer Options' menu appears. Then flick on the 'Developer mode' switch. You may need to do this every time when the phone restarts\n\n\n17. How to reduce nausea?\n\n\nMaintain a high frame-rate, such as 90+ fps.\n\n\n18. How many triangles can I display max for a good VR experience with high frame rate?\n\n\nOn a mobile phone such as Galaxy S6/S7, please keep triangle count in the thousands to tens of thousands range if possible, depending on shader complexities.\n\n\n19. What are some graphics performance tips?\n\n\nKeep draw calls minimal and relatively cheap pixel shader. Keep in mind shadows from shadow map more or less doubles the triangle rendered. Use profiler to see if you are really GPU bound.\n\n\n20. Which phones are compatible with GearVR?\n\n\nCurrently, Samsung Galaxy S6, S6 Edge, S6 Edge+, S7, S7 Edge, S7 Edge+, S8, S8+, Note 5", 
            "title": "FAQ"
        }, 
        {
            "location": "/programming_guide/faq/#1-is-there-any-example-of-object-following-the-head-tracking-just-like-a-reticle", 
            "text": "See  gvr-tutorial-lesson2 sample . Examine the BalloonMain.java and the headTracker tracker object it sets up. The key part is adding the object to the main camera rig.", 
            "title": "1. Is there any example of object following the head tracking, just like a reticle?"
        }, 
        {
            "location": "/programming_guide/faq/#2-i-want-to-implement-a-scrollable-list-of-item-like-listview-in-android-how-to-go-about-that", 
            "text": "Background objects: rendering order N, depth test on  Clip object: rendering order N+1, depth test on, alpha blend on, alpha = 0 (completely transparent)  List view object: rendering order N+2, depth test on   The clip object should be a plane with a hole in it where you want to see the list view. It should be completely transparent. It will be rendered after the background so it will update the depth buffer but the background will show thru completely. This clip object should have a Z value putting it IN FRONT of the list view object even though it will be rendered before that object (because you set the rendering order to a smaller value).  GearVRF will render objects in ascending rendering order so the background will be rendered first. The clip object will update the depth buffer so that anything drawn BEHIND it will show thru the hole but will be obscured by the transparent clip area (the depth buffer will do the clipping for us).", 
            "title": "2. I want to implement a scrollable list of item like ListView in Android. How to go about that?"
        }, 
        {
            "location": "/programming_guide/faq/#3-can-i-use-an-emulator-during-development-for-testing", 
            "text": "Short answer: No.  Long answer: It would likely be somewhat painful to do. Oculus only provides 32bit arm libraries. Which means you would need to set up an arm emulator (rather than an x86 one). In that emulator, we would detect the oculus service is not on the system and fall back to daydream. However, in our experience, running an arm emulator is horrifically slow, especially for anything GL related. It's best to stick with a physical phone for development.", 
            "title": "3. Can i use an emulator during development for testing?"
        }, 
        {
            "location": "/programming_guide/faq/#4-i-am-using-windows-trying-to-build-the-framework-and-getting-weird-errors-like-this-one", 
            "text": "...\\GearVRf\\GVRf\\Framework\\framework\\..\\backend_oculus/src/main/jni/util/configuration_helper.cpp:235:1: fatal error: opening dependency file ./obj/local/armeabi-v7a/objs/...\\GearVRf\\GVRf\\Framework\\framework\\..\\backend_oculus/src/main/jni/util/configuration_helper.o.d: No such file or directory  Your paths might be too long. Try moving the framework to C:\\ and build again.", 
            "title": "4. I am using Windows, trying to build the framework and getting weird errors. Like this one:"
        }, 
        {
            "location": "/programming_guide/faq/#5-i-want-to-inflate-and-show-an-android-view-can-i-do-that", 
            "text": "Yes. The gvr-renderableview sample shows how to do that.", 
            "title": "5. I want to inflate and show an Android view. Can I do that?"
        }, 
        {
            "location": "/programming_guide/faq/#6-i-want-to-use-exoplayer-instead-of-mediaplayer-for-video-playback-can-i-do-this", 
            "text": "Yes. See the gvr-360video sample, which allows you to use either. Set the USE_EXO_PLAYER flag in Minimal360VideoActivity.java.", 
            "title": "6. I want to use ExoPlayer instead of MediaPlayer for video playback. Can I do this?"
        }, 
        {
            "location": "/programming_guide/faq/#7-how-can-i-create-a-mixed-vr-android-app-and-launching-vr-mode-later-by-clicking-a-button-for-example-i-need-to-create-an-activity-visualized-in-normal-mode-for-settings-and-later-launch-a-vr-mode-showing-the-you-need-gear-vr-screen-if-you-have-not-attached-it", 
            "text": "Unfortunately, this is not supported. Apps get marked as \"vr\" not individual activities. Which means the prompt will show when you try to launch your \"normal\" activity. This is not a gvrf limitation.", 
            "title": "7. How can I create a mixed VR android app and launching VR Mode later, by clicking a button for example? I need to create an activity visualized in normal mode for settings and later launch a VR mode, showing the \"you need gear vr\" screen if you have not attached it."
        }, 
        {
            "location": "/programming_guide/faq/#8-trying-to-build-a-sample-but-i-get-the-following-error", 
            "text": "What   went   wrong :  Execution   failed   for   task   :app:transformClassesWithDexForDebug .    com . android . build . api . transform . TransformException :   com . android . ide . common . process . ProcessException :   java . util . concurrent . ExecutionException :   com . android . dex . DexException :   Multiple   dex   files   define   Lcom / oculus / systemutils / BuildConfig ;   Most likely you still have VrApi.jar and SystemUtils.jar under the framework module (GearVRf/GVRf/Framework/framework/src/main/libs/). Please remove them from there, clean and build.", 
            "title": "8. Trying to build a sample but I get the following error:"
        }, 
        {
            "location": "/programming_guide/faq/#9-i-am-using-linux-and-getting-a-strange-aapt-error-during-the-build-something-like-javaioioexception-cannot-run-program-aapt-error2-no-such-file-or-directory", 
            "text": "You might be missing support for executing 32bit binaries and/or libraries aapt depends on. Please run the following:  sudo dpkg --add-architecture i386\nsudo apt-get update\nsudo apt-get install libc6:i386 libncurses5:i386 libstdc++6:i386\nsudo apt-get install zlib1g:i386", 
            "title": "9. I am using Linux and getting a strange aapt error during the build. Something like java.io.IOException: Cannot run program \"/aapt\": error=2, No such file or directory"
        }, 
        {
            "location": "/programming_guide/faq/#10-is-there-support-for-the-oculus-platform-sdk", 
            "text": "Yes, the entitlement check is supported. Go to GVRf/Extensions/. There is a platformsdk_support module. To build it run  ./gradlew -Pplatformsdk_support=true platformsdk_support:assembleDebug . Checkout the javadoc in PlatformEntitlementCheck.java. Have been verified to work with Platform SDK versions 1.6, 1.7 and 1.8. For further information see  https://github.com/Samsung/GearVRf/wiki/Entitlement-Check-using-GVRF", 
            "title": "10. Is there support for the Oculus Platform SDK?"
        }, 
        {
            "location": "/programming_guide/faq/#11-is-there-any-way-to-play-youtube-video-from-url", 
            "text": "Yes. See  https://github.com/Samsung/GearVRf/issues/1033#issuecomment-278244683", 
            "title": "11. Is there any way to play youtube video from url?"
        }, 
        {
            "location": "/programming_guide/faq/#12-i-am-trying-to-use-gvrf-on-a-google-pixel-phone-and-i-get-this-exception", 
            "text": "02-15 19:53:15.697 23156-23156/? E/AndroidRuntime: FATAL EXCEPTION: main\nProcess: pl.lynx.daydream.test, PID: 23156\njava.lang.UnsatisfiedLinkError: dalvik.system.PathClassLoader[DexPathList[[zip file  /data/app/pl.lynx.daydream.test-2/base.apk ],nativeLibraryDirectories=[/data/app/pl.lynx.daydream.test-2/lib/arm64, /system/fake-libs64, /data/app/pl.lynx.daydream.test-2/base.apk!/lib/arm64-v8a, /system/lib64, /vendor/lib64]]] couldn t find  libgvrf.so   Daydream has 64bit binaries but GVRf only supports 32bit binaries. In your app's gradle file you need to add this:  android {\n\n    // ignore the x86 and arm-v8 files from the google vr libraries\n    packagingOptions {\n        exclude  lib/x86/libgvr.so \n        exclude  lib/arm64-v8a/libgvr.so \n    }\n}", 
            "title": "12. I am trying to use GVRF on a Google Pixel phone and I get this exception:"
        }, 
        {
            "location": "/programming_guide/faq/#13-i-used-to-build-the-demos-from-the-gearvrf-demos-repo-just-fine-suddenly-i-am-getting-errors-what-happened", 
            "text": "The master branch is subject to frequent improvements. The GVRf team pushes updated framework snapshots to the maven repo, but due to the gradle's caching you are most likely using outdated snapshot. Please pass the --refresh-dependencies argument to gradlew if you are building from the command line. Or you can just delete the gradle cache via   rm -rf ~/.gradle/caches/.  Alternatively you could use the 3.1 branch which is stable. After cloning the demos repo, switch to the release_v3.1 branch.", 
            "title": "13. I used to build the demos from the GearVRf-Demos repo just fine. Suddenly I am getting errors. What happened?"
        }, 
        {
            "location": "/programming_guide/faq/#14-i-am-building-an-app-from-scratch-how-do-i-add-support-for-gvrf-to-my-app-what-are-the-minimum-dependencies", 
            "text": "Please see this bare-bones project that can serve as a reference:  https://github.com/gearvrf/GearVRf-Demos/tree/master/template/GVRFApplication .", 
            "title": "14. I am building an app from scratch. How do I add support for GVRF to my app? What are the minimum dependencies?"
        }, 
        {
            "location": "/programming_guide/faq/#15-my-app-in-the-oculus-store-fails-to-install-on-android-n-devices-i-get-an-untrusted_apk_error-error", 
            "text": "Oculus doesn't support APK signature scheme v2 yet. It should be disabled if you plan to submit apps to the Oculus store. Android Studio seems to apply the scheme unconditionally. Build from the command line and include the following section in your gradle file:  android {\n    signingConfigs {\n        release {\n            v2SigningEnabled false\n            storeFile file( full-path-to-your-store-file )\n            storePassword  your_store_pwd \n            keyAlias  alias \n            keyPassword  key_pwd \n        }\n    }\n    defaultConfig {\n        signingConfig signingConfigs.release\n    }\n}", 
            "title": "15. My app in the Oculus Store fails to install on Android N devices. I get an UNTRUSTED_APK_ERROR error."
        }, 
        {
            "location": "/programming_guide/faq/#16-can-you-run-gearvr-app-on-your-phone-without-gearvr", 
            "text": "Yes, in your phone's Settings- Applications- Application Manager- Gear VR Service- Manage Storage Tab on VR Service Version multiple times until the 'Developer Options' menu appears. Then flick on the 'Developer mode' switch. You may need to do this every time when the phone restarts", 
            "title": "16. Can you run GearVR app on your phone without GearVR?"
        }, 
        {
            "location": "/programming_guide/faq/#17-how-to-reduce-nausea", 
            "text": "Maintain a high frame-rate, such as 90+ fps.", 
            "title": "17. How to reduce nausea?"
        }, 
        {
            "location": "/programming_guide/faq/#18-how-many-triangles-can-i-display-max-for-a-good-vr-experience-with-high-frame-rate", 
            "text": "On a mobile phone such as Galaxy S6/S7, please keep triangle count in the thousands to tens of thousands range if possible, depending on shader complexities.", 
            "title": "18. How many triangles can I display max for a good VR experience with high frame rate?"
        }, 
        {
            "location": "/programming_guide/faq/#19-what-are-some-graphics-performance-tips", 
            "text": "Keep draw calls minimal and relatively cheap pixel shader. Keep in mind shadows from shadow map more or less doubles the triangle rendered. Use profiler to see if you are really GPU bound.", 
            "title": "19. What are some graphics performance tips?"
        }, 
        {
            "location": "/programming_guide/faq/#20-which-phones-are-compatible-with-gearvr", 
            "text": "Currently, Samsung Galaxy S6, S6 Edge, S6 Edge+, S7, S7 Edge, S7 Edge+, S8, S8+, Note 5", 
            "title": "20. Which phones are compatible with GearVR?"
        }, 
        {
            "location": "/programming_guide/video_tutorials/", 
            "text": "Below is a set of six lessons that show how to build a simple VR game using GearVRf. The sessions are 10 - 20 minutes long and show live demonstrations of GearVRf programming.\n\n\nThe sample code used in all the tutorials can be found here: \nhttps://github.com/gearvrf/GearVRf-Demos\n  \n\n\nLesson 1: Overview of Tutorials\n\n\n\n\n\nLesson 2: Application Structure and Scene Graph\n\n\n\n\n\nLesson 3: Events and Picking\n\n\n\n\nLesson 4: Components\n\n\n\n\nLesson 5: Sound and Text\n\n\n\n\nLesson 6: Working with Assets", 
            "title": "Video Tutorials"
        }, 
        {
            "location": "/programming_guide/video_tutorials/#lesson-1-overview-of-tutorials", 
            "text": "", 
            "title": "Lesson 1: Overview of Tutorials"
        }, 
        {
            "location": "/programming_guide/video_tutorials/#lesson-2-application-structure-and-scene-graph", 
            "text": "", 
            "title": "Lesson 2: Application Structure and Scene Graph"
        }, 
        {
            "location": "/programming_guide/video_tutorials/#lesson-3-events-and-picking", 
            "text": "", 
            "title": "Lesson 3: Events and Picking"
        }, 
        {
            "location": "/programming_guide/video_tutorials/#lesson-4-components", 
            "text": "", 
            "title": "Lesson 4: Components"
        }, 
        {
            "location": "/programming_guide/video_tutorials/#lesson-5-sound-and-text", 
            "text": "", 
            "title": "Lesson 5: Sound and Text"
        }, 
        {
            "location": "/programming_guide/video_tutorials/#lesson-6-working-with-assets", 
            "text": "", 
            "title": "Lesson 6: Working with Assets"
        }, 
        {
            "location": "/programming_guide/sample_code/", 
            "text": "GearVRf Samples and Demos\n\n\nTo get the GearVR Framework Samples and Demos, clone the following repository in the same directory as where you did the clone for the framework source code:\n\n\n$ git clone https://github.com/gearvrf/GearVRf-Demos.git -b release_v3.2\n\n\n\n\n\n\n\nNote\n\n\nYou should put both GearVRf/ and GearVRf-Demos/ in the same directory.\n\n\n\n\nSample GearVRf Applications\n\n\nSample GearVRf applications, available in the GearVRf SDK, can provide you with valuable insight into writing your own VR applications.\n\n\n\n\nNote\n\n\nThe flat images below represent GearVRf applications with actual stereographic displays.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolar System\n \n (gvrsolarsystem) \n \n\n\nSolar system with four inner rotating planets revolving around the rotating sun as viewed from the point on the moon closest to Earth\n \n Asynchronous loading of meshes and textures is used. \nLoading order is by priority; no mesh or texture is loaded twice. \nAnimation is used to create the illusion of rotation and revolution.\n\n\n\n\n\n\nDynamic Graphics Board\n(gvropacityanigalery)\n\n\nImages and video displayed on a board inside a 3D scene\nA video or image is displayed in a rectangular scene object (up the stairs).\nAnimation is used to switch images and video smoothly.\nOnStep decides and defines the animation to apply, based on HMD orientation.\nPost-effect converts view to sepia colors.\n\n\n\n\n\n\nEye Picking\n \n(gvreyepickingsample)\n\n\nColor of objects (bunnies and rectangles) changes to red when an object is at the center of the view\nOnStep updates scene object colors.\nImplements a color custom shader.\n\n\n\n\n\n\nPick and Move\n \n(gvr-pickandmove)\n\n\n3D mirror ball can be selected and repositioned in a 3D scene\nUses the HMD trackpad.\nUses Cubemap support.\nUses scene picking.\n\n\n\n\n\n\nUseful Scene Objects\n(scene-objects)\n\n\nContains fundamental scene objects (such as a cube, sphere, cone, and text) that you can use in your own apps", 
            "title": "Sample Code"
        }, 
        {
            "location": "/programming_guide/sample_code/#gearvrf-samples-and-demos", 
            "text": "To get the GearVR Framework Samples and Demos, clone the following repository in the same directory as where you did the clone for the framework source code:  $ git clone https://github.com/gearvrf/GearVRf-Demos.git -b release_v3.2   Note  You should put both GearVRf/ and GearVRf-Demos/ in the same directory.", 
            "title": "GearVRf Samples and Demos"
        }, 
        {
            "location": "/programming_guide/sample_code/#sample-gearvrf-applications", 
            "text": "Sample GearVRf applications, available in the GearVRf SDK, can provide you with valuable insight into writing your own VR applications.   Note  The flat images below represent GearVRf applications with actual stereographic displays.            Solar System    (gvrsolarsystem)     Solar system with four inner rotating planets revolving around the rotating sun as viewed from the point on the moon closest to Earth    Asynchronous loading of meshes and textures is used.  Loading order is by priority; no mesh or texture is loaded twice.  Animation is used to create the illusion of rotation and revolution.    Dynamic Graphics Board (gvropacityanigalery)  Images and video displayed on a board inside a 3D scene A video or image is displayed in a rectangular scene object (up the stairs). Animation is used to switch images and video smoothly. OnStep decides and defines the animation to apply, based on HMD orientation. Post-effect converts view to sepia colors.    Eye Picking   (gvreyepickingsample)  Color of objects (bunnies and rectangles) changes to red when an object is at the center of the view OnStep updates scene object colors. Implements a color custom shader.    Pick and Move   (gvr-pickandmove)  3D mirror ball can be selected and repositioned in a 3D scene Uses the HMD trackpad. Uses Cubemap support. Uses scene picking.    Useful Scene Objects (scene-objects)  Contains fundamental scene objects (such as a cube, sphere, cone, and text) that you can use in your own apps", 
            "title": "Sample GearVRf Applications"
        }, 
        {
            "location": "/about/contribution/", 
            "text": "Gear VR development process, guidelines, and tips; and getting answers, reporting a bug, and submitting a patch\n\n\nGetting involved with the GearVRf Project is easy.\n\n\nTo contribute to the GearVRf Project (such as reporting bugs and submitting patches):\n\n\n\n\nFollow the \nGitHub contributor guidelines\n\n\nAdd the \nGearVRf DCO\n signoff to each commit message during development.\n\n\n\n\nDevelopment Process\n\n\nIt is the responsibility of GearVRf Maintainers and Reviewers to decide whether submitted code should be integrated into the mainline code, returned for revision, or rejected.\n\n\nIndividual developers maintain a local copy of the GearVRf codebase using the git revision control system. Git ensures that all participants are working with a common and up-to-date code base at all times. Each developer works to develop, debug, build, and validate their own code against the current codebase, so that when the time comes to integrate into the mainline Project, their changes apply cleanly and with a minimum amount of merging effort.\n\n\nThe GearVRf Project development process is marked by the following highlights:\n\n\n\n\nThe feature development process starts with an author discussing a proposed feature with the Maintainers and/or or Reviewers.\n\n\nThe Maintainers and Reviewers evaluate the idea, give feedback, and finally approve or reject the proposal.\n\n\nThe author shares the proposal with the community via the mailing list.\n\n\nThe community provides feedback which can be used by the author to modify their proposal and share it with the community again.\n\n\nThe above steps are repeated until the community reaches a consensus according to the Community Guidelines.\n\n\nAfter a consensus is reached, the author proceeds with the implementation and testing of the feature.\n\n\nAfter the author is confident their code is ready for integration:\n\n\nThe author generates a patch and signs off on their code.\n\n\nThe author submits a patch by opening a \npull request\n.\n\n\n\n\n\n\nThe Maintainers and/or Reviewers watch the pull request for the patch, test the code, and accept or reject the patch accordingly.\n\n\nAfter the code passes code review, the Maintainers and/or Reviewers accept the code (integrated into the main branch), which completes the development process.\n\n\n\n\nAfter a patch has been accepted, it remains the authoring developer's responsibility to maintain the code throughout its lifecycle, and to provide security and feature updates as needed.\n\n\nFor more information about GitHub issues, refer to the \nGitHub issues guidelines\n.\n\n\nCoding Guidelines\n\n\nWhen generating you own GearVRf project code, please follow these guidelines.\n\n\n\n\n\n\nGeneral:\n\n\n\n\nDo not abbreviate variable names.\n\n\nAbbreviations may not be familiar to new and other project members. Code with abbreviations will not be merged.\n\n\nPublic classes must start with GVR (for example, when adding a Foo class, the class name should be GVRFoo).\n\n\nImplementation classes should not start with GVR.\n\n\n\n\n\n\n\n\nIn Java:\n\n\n\n\nUse camel case for names (for example, setBackgroundColor).\n\n\nSet up and use the auto-formatter for Java code in Eclipse (see below).\n\n\n\n\n\n\n\n\nIn C++:\n\n\n\n\nUse underscore case for names (for example, gvr_note4).\n\n\nPut all JNI interface calls in a separate file with the postfix _jni. \n\nFor example, put the JNI interfaces for GVRSceneObject in a separate file scene_object_jni.cpp\n\n\nFollow the actual logic in plain C++ .h and .cpp files.\n\n\nFor each new C++ file that has a correlative Java GVR class, do not add GVR as a prefix to the file name. \n\nFor example, for GVRSceneObject.java, the C++ file name would be scene_object.cpp/scene_object.h\n\n\nSet up and use the auto-formatter for C++ code in Eclipse (see below).\n\n\n\n\n\n\n\n\nTo set up and use the Java auto-formatter in Eclipse:\n\n\n\n\n\n\nIn Eclipse, set up auto-formatting by following methods:\n\n\n\n\nDownload the \nCode formatter profile: Java conventions (all spaces) XML\n file.\n\n\nClick \nWindow \n Preferences\n\n\nIn the Preferences dialog box:\n\n\nClick \nJava \n Code Style \n Formatter\n OR \nJava \n Code Style\n\n\nImport \nJava-conventions-all-spaces.xml\n file\n\n\n\n\n\n\n\n\n\n\n\n\nIn Eclipse, auto-format your code\n\n\n\n\n\n\nTo set up and use the C++ auto-formatter in Eclipse:\n\n\n\n\nIn Eclipse, set up auto-formatting by following methods:\n\n\nDownload the \nCode formatter profile: K\nR, spaces only\n\n\nClick \nWindow \n Preferences\n\n\nIn the Preferences dialog box:\n\n\nClick \nC/C++ \n Code Style \n Formatter\n OR \nC/C++ \n Code Style\n\n\nImport \nK-and-R-C++-spaces-only.xml\n file\n\n\n\n\n\n\n\n\n\n\nIn Eclipse, auto-format your code\n\n\n\n\nSubmit a Patch\n\n\nThe following guidelines on the submission process are provided to help you be more effective when submitting code to the GearVRf Project.\n\n\nWhen development is complete, a patch set should be submitted via Github pull requests. A review of the patch set will take place. When accepted, the patch set will be integrated into the next build, verified, and tested. It is then the responsibility of the authoring developer to maintain the code throughout its lifecycle.\n\n\nPlease submit all patches in public by opening a pull request. Patches sent privately to Maintainers or Reviewers will not be considered. Because the GearVRf Project is an open source Project, be prepared for feedback and criticism--it happens to everyone. If asked to rework your code, be persistent and resubmit after making changes.\n\n\n\n\n\n\nScope the patch\n\n\nSmaller patches are generally easier to understand and test, so please submit changes in the smallest increments possible, within reason. Smaller patches are less likely to have unintended consequences, and if they do, getting to root cause is much easier for you and the Maintainers and Reviewers. Additionally, smaller patches are much more likely to be accepted.\n\n\n\n\n\n\nSign your work with the\n  \nGearVRf DCO\n.\n\n\nThe sign-off is a simple line at the end of the explanation for the patch, which certifies that you wrote it or otherwise have the right to pass it on as an open-source patch. The rules are pretty simple, and the sign-off is required for a patch to be accepted.\n\n\n\n\n\n\nOpen a Github \npull request\n\n\n\n\n\n\nWhat if my patch is rejected? \n\n\nIt happens all the time, for many reasons, and not necessarily because the code is bad. Take the feedback, adapt your code, and try again. Remember, the ultimate goal is to preserve the quality of the code and maintain the focus of the Project through intensive review.\nMaintainers typically have to process a lot of submissions, and the time for any individual response is generally limited. If the reason for rejection is unclear, please ask for more information on the mailing list or on the IRC channel.\nIf you have a solid technical reason to disagree with feedback and you feel that reason has been overlooked, take the time to thoroughly explain it in your response.\n\n\n\n\n\n\nEscalation\n\n\nIf you submitted a patch and did not receive a response within 5 business days:\n\n\n\n\nPlease send an email to the GearVRf Project Developers \nMailing List\n.\n\n\nIn the first line of the email, include this phrase \"Patch escalation: no response for x days\". \nThis is one of those rare cases where you should top post, to make sure that Maintainers and Reviewers see the escalation text, which cues them to make sure someone responds.\n\n\n\n\n\n\n\n\nCode review\n\n\nCode review can be performed by all the members of the Project (not just Maintainers and Reviewers). Members can review code changes and share their opinion by comments with the following principles:\n    * Discuss code; never discuss the code's author.\n    * Respect and acknowledge contributions, suggestions, and comments.\n    * Listen and be open to all different opinions.\n    * Help each other.\n\n\nChanges are submitted via pull requests and only the Maintainer or Reviewers of the module affected by the code change should approve or reject the pull request.\nChanges should be reviewed in reasonable amount of time. Maintainers and Reviewers should leave changes open for some time (at least 1 full business day) so others can offer feedback. Review times increase with the complexity of the review.\n\n\nGitHub Development Tips\n\n\nTips for working on GitHub\n\n\n\n\n\n\nFork the \nGitHub repository\n and clone it locally.\n\n\nConnect your local repository to the original upstream repository by adding it as a remote.\n\n\nPull in upstream changes often to stay up-to-date so that when you submit your pull request, merge conflicts will be less likely.\n\n\nFor more details, see \nGitHub fork synching guidelines\n.\n\n\n\n\n\n\nCreate a \nbranch\n for your edits.\n\n\n\n\n\n\n\n\n\n\nOur usual github workflow:\n\n\n\n\nGoto: \nhttps://github.com/Samsung/GearVRf/\n\n\nFind the \u2018fork\u2019 button in the upper right. Fork GearVRf into your own repository\n\n\nIn your own fork of GearVRf, click on the \u2018branch\u2019 button and create a new branch\n\n\nClone your repo onto your local machine. (you\u2019ll notice a convenience \u2018HTTPS clone URL\u2019 thing to the right on the webpage, that\u2019ll give the full URL you need to clone. The URL will look something like: \nhttps://github.com/thomasflynn/GearVRf.git\n, but with your own github id in the middle there.\n\n\nYou\u2019ll need to get the Samsung GearVRf repo as the upstream remote repo for your fork. git remote add parent \nhttps://github.com/Samsung/GearVRf/\n\n\nSwitch to the branch you created (git checkout branchname) on your local machine.\n\n\nMake your changes.\n\n\nGit add, git commit.\n\n\nGit push origin branchname:branchname ; \n- this will push it up to your forked repo on github.\n\n\nOn the webpage for your repo, you\u2019ll see a \u2018pull request button\u2019. Click that. You\u2019ll see your commit message and you\u2019ll need to add your DCO (see submitting a patch on gearvrf.org. also: wiki.gearvrf.org/bin/view/GearVRF/GearVRfDCO\n\n\nClick the green \u2018create pull request\u2019 button at the bottom.\n\n\n\n\nIf you need to upload a second patchset due to comments on your pull-request\n\n\n\n\nMake changes in your branch\n\n\nGit add, git commit, git push origin branchname:branchname\n\n\nYour new changes are now a part of the commit.\n\n\n\n\nTo rebase:\n\n\n\n\nSwitch to your master branch: git checkout master\n\n\nPull the remote master: git pull parent master:master\n\n\nForce-push the update to your master branch: git push \u2013f origin master:master\n\n\nSwitch to your branch: git checkout branchname\n\n\nRebase: git rebase master\n\n\nGit add, git commit, git push -f origin branchname:branchname\n\n\n\n\nGet Answers and Report a Bug\n\n\nIf you have a question about GearVRf code, have trouble following documentation, or find a bug, review the current GearVRf issues in GitHub, and if necessary, create a new issue.\n\n\nTips on GitHub Issues\n\n\n\n\n\n\nCheck existing GearVRf issues for the answer to your \nissue\n.\n\n\nDuplicating an issue slows you and others. Search through open and closed issues to see if the problem you are running into has already been addressed.\n\n\n\n\n\n\nIf necessary, open a \nnew issue\n.\n\n\n\n\nClearly describe the issue. \n\n\nWhat did you expect to happen?\n\n\nWhat actually happened instead?\n\n\nHow can someone else recreate the problem?\n\n\n\n\n\n\nLink to demos that recreate the problem on things such as \nJSFiddle\n or \nCodePen\n.\n\n\nInclude system details (such as the hardware, library, and operating system you are using and their versions).\n\n\n\n\nPaste error output and logs in the issue or in a \nGist\n. \n\n\nWhen pasting in the issue, wrap code in three backticks: ``` so that it renders nicely.", 
            "title": "Contribution"
        }, 
        {
            "location": "/about/contribution/#development-process", 
            "text": "It is the responsibility of GearVRf Maintainers and Reviewers to decide whether submitted code should be integrated into the mainline code, returned for revision, or rejected.  Individual developers maintain a local copy of the GearVRf codebase using the git revision control system. Git ensures that all participants are working with a common and up-to-date code base at all times. Each developer works to develop, debug, build, and validate their own code against the current codebase, so that when the time comes to integrate into the mainline Project, their changes apply cleanly and with a minimum amount of merging effort.  The GearVRf Project development process is marked by the following highlights:   The feature development process starts with an author discussing a proposed feature with the Maintainers and/or or Reviewers.  The Maintainers and Reviewers evaluate the idea, give feedback, and finally approve or reject the proposal.  The author shares the proposal with the community via the mailing list.  The community provides feedback which can be used by the author to modify their proposal and share it with the community again.  The above steps are repeated until the community reaches a consensus according to the Community Guidelines.  After a consensus is reached, the author proceeds with the implementation and testing of the feature.  After the author is confident their code is ready for integration:  The author generates a patch and signs off on their code.  The author submits a patch by opening a  pull request .    The Maintainers and/or Reviewers watch the pull request for the patch, test the code, and accept or reject the patch accordingly.  After the code passes code review, the Maintainers and/or Reviewers accept the code (integrated into the main branch), which completes the development process.   After a patch has been accepted, it remains the authoring developer's responsibility to maintain the code throughout its lifecycle, and to provide security and feature updates as needed.  For more information about GitHub issues, refer to the  GitHub issues guidelines .", 
            "title": "Development Process"
        }, 
        {
            "location": "/about/contribution/#coding-guidelines", 
            "text": "When generating you own GearVRf project code, please follow these guidelines.    General:   Do not abbreviate variable names.  Abbreviations may not be familiar to new and other project members. Code with abbreviations will not be merged.  Public classes must start with GVR (for example, when adding a Foo class, the class name should be GVRFoo).  Implementation classes should not start with GVR.     In Java:   Use camel case for names (for example, setBackgroundColor).  Set up and use the auto-formatter for Java code in Eclipse (see below).     In C++:   Use underscore case for names (for example, gvr_note4).  Put all JNI interface calls in a separate file with the postfix _jni.  For example, put the JNI interfaces for GVRSceneObject in a separate file scene_object_jni.cpp  Follow the actual logic in plain C++ .h and .cpp files.  For each new C++ file that has a correlative Java GVR class, do not add GVR as a prefix to the file name.  For example, for GVRSceneObject.java, the C++ file name would be scene_object.cpp/scene_object.h  Set up and use the auto-formatter for C++ code in Eclipse (see below).     To set up and use the Java auto-formatter in Eclipse:    In Eclipse, set up auto-formatting by following methods:   Download the  Code formatter profile: Java conventions (all spaces) XML  file.  Click  Window   Preferences  In the Preferences dialog box:  Click  Java   Code Style   Formatter  OR  Java   Code Style  Import  Java-conventions-all-spaces.xml  file       In Eclipse, auto-format your code    To set up and use the C++ auto-formatter in Eclipse:   In Eclipse, set up auto-formatting by following methods:  Download the  Code formatter profile: K R, spaces only  Click  Window   Preferences  In the Preferences dialog box:  Click  C/C++   Code Style   Formatter  OR  C/C++   Code Style  Import  K-and-R-C++-spaces-only.xml  file      In Eclipse, auto-format your code", 
            "title": "Coding Guidelines"
        }, 
        {
            "location": "/about/contribution/#submit-a-patch", 
            "text": "The following guidelines on the submission process are provided to help you be more effective when submitting code to the GearVRf Project.  When development is complete, a patch set should be submitted via Github pull requests. A review of the patch set will take place. When accepted, the patch set will be integrated into the next build, verified, and tested. It is then the responsibility of the authoring developer to maintain the code throughout its lifecycle.  Please submit all patches in public by opening a pull request. Patches sent privately to Maintainers or Reviewers will not be considered. Because the GearVRf Project is an open source Project, be prepared for feedback and criticism--it happens to everyone. If asked to rework your code, be persistent and resubmit after making changes.    Scope the patch  Smaller patches are generally easier to understand and test, so please submit changes in the smallest increments possible, within reason. Smaller patches are less likely to have unintended consequences, and if they do, getting to root cause is much easier for you and the Maintainers and Reviewers. Additionally, smaller patches are much more likely to be accepted.    Sign your work with the    GearVRf DCO .  The sign-off is a simple line at the end of the explanation for the patch, which certifies that you wrote it or otherwise have the right to pass it on as an open-source patch. The rules are pretty simple, and the sign-off is required for a patch to be accepted.    Open a Github  pull request    What if my patch is rejected?   It happens all the time, for many reasons, and not necessarily because the code is bad. Take the feedback, adapt your code, and try again. Remember, the ultimate goal is to preserve the quality of the code and maintain the focus of the Project through intensive review.\nMaintainers typically have to process a lot of submissions, and the time for any individual response is generally limited. If the reason for rejection is unclear, please ask for more information on the mailing list or on the IRC channel.\nIf you have a solid technical reason to disagree with feedback and you feel that reason has been overlooked, take the time to thoroughly explain it in your response.    Escalation  If you submitted a patch and did not receive a response within 5 business days:   Please send an email to the GearVRf Project Developers  Mailing List .  In the first line of the email, include this phrase \"Patch escalation: no response for x days\". \nThis is one of those rare cases where you should top post, to make sure that Maintainers and Reviewers see the escalation text, which cues them to make sure someone responds.     Code review  Code review can be performed by all the members of the Project (not just Maintainers and Reviewers). Members can review code changes and share their opinion by comments with the following principles:\n    * Discuss code; never discuss the code's author.\n    * Respect and acknowledge contributions, suggestions, and comments.\n    * Listen and be open to all different opinions.\n    * Help each other.  Changes are submitted via pull requests and only the Maintainer or Reviewers of the module affected by the code change should approve or reject the pull request.\nChanges should be reviewed in reasonable amount of time. Maintainers and Reviewers should leave changes open for some time (at least 1 full business day) so others can offer feedback. Review times increase with the complexity of the review.", 
            "title": "Submit a Patch"
        }, 
        {
            "location": "/about/contribution/#github-development-tips", 
            "text": "Tips for working on GitHub    Fork the  GitHub repository  and clone it locally.  Connect your local repository to the original upstream repository by adding it as a remote.  Pull in upstream changes often to stay up-to-date so that when you submit your pull request, merge conflicts will be less likely.  For more details, see  GitHub fork synching guidelines .    Create a  branch  for your edits.      Our usual github workflow:   Goto:  https://github.com/Samsung/GearVRf/  Find the \u2018fork\u2019 button in the upper right. Fork GearVRf into your own repository  In your own fork of GearVRf, click on the \u2018branch\u2019 button and create a new branch  Clone your repo onto your local machine. (you\u2019ll notice a convenience \u2018HTTPS clone URL\u2019 thing to the right on the webpage, that\u2019ll give the full URL you need to clone. The URL will look something like:  https://github.com/thomasflynn/GearVRf.git , but with your own github id in the middle there.  You\u2019ll need to get the Samsung GearVRf repo as the upstream remote repo for your fork. git remote add parent  https://github.com/Samsung/GearVRf/  Switch to the branch you created (git checkout branchname) on your local machine.  Make your changes.  Git add, git commit.  Git push origin branchname:branchname ;  - this will push it up to your forked repo on github.  On the webpage for your repo, you\u2019ll see a \u2018pull request button\u2019. Click that. You\u2019ll see your commit message and you\u2019ll need to add your DCO (see submitting a patch on gearvrf.org. also: wiki.gearvrf.org/bin/view/GearVRF/GearVRfDCO  Click the green \u2018create pull request\u2019 button at the bottom.   If you need to upload a second patchset due to comments on your pull-request   Make changes in your branch  Git add, git commit, git push origin branchname:branchname  Your new changes are now a part of the commit.   To rebase:   Switch to your master branch: git checkout master  Pull the remote master: git pull parent master:master  Force-push the update to your master branch: git push \u2013f origin master:master  Switch to your branch: git checkout branchname  Rebase: git rebase master  Git add, git commit, git push -f origin branchname:branchname", 
            "title": "GitHub Development Tips"
        }, 
        {
            "location": "/about/contribution/#get-answers-and-report-a-bug", 
            "text": "If you have a question about GearVRf code, have trouble following documentation, or find a bug, review the current GearVRf issues in GitHub, and if necessary, create a new issue.  Tips on GitHub Issues    Check existing GearVRf issues for the answer to your  issue .  Duplicating an issue slows you and others. Search through open and closed issues to see if the problem you are running into has already been addressed.    If necessary, open a  new issue .   Clearly describe the issue.   What did you expect to happen?  What actually happened instead?  How can someone else recreate the problem?    Link to demos that recreate the problem on things such as  JSFiddle  or  CodePen .  Include system details (such as the hardware, library, and operating system you are using and their versions).   Paste error output and logs in the issue or in a  Gist .   When pasting in the issue, wrap code in three backticks: ``` so that it renders nicely.", 
            "title": "Get Answers and Report a Bug"
        }, 
        {
            "location": "/about/certificate/", 
            "text": "certificate.md", 
            "title": "Developer Certificate of Origin"
        }, 
        {
            "location": "/about/release_notes/", 
            "text": "release_notes.md", 
            "title": "Release Notes"
        }, 
        {
            "location": "/about/roadmap/", 
            "text": "roadmap.md", 
            "title": "Roadmap"
        }, 
        {
            "location": "/about/license/", 
            "text": "license.md", 
            "title": "License"
        }, 
        {
            "location": "/about/privacy_policy/", 
            "text": "privacy_policy.md", 
            "title": "Privacy Policy"
        }, 
        {
            "location": "/about/marketing_resource/", 
            "text": "marketing_resource.md", 
            "title": "Marketing Resources"
        }
    ]
}