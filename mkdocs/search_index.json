{
    "docs": [
        {
            "location": "/", 
            "text": "GearVR Framework Project\n\n\nThe Gear VR Framework (GearVRf) Project is a lightweight, powerful, open source rendering engine with a Java interface for developing mobile VR games and applications for Gear VR and Google Daydream View.\n\n\nGearVRf is:\n\n\n\n\n\n\nSimple\n - Java interface, Android Studio build environment and a simple SDK allow you to prototype rapidly. In-depth OpenGL and Oculus/Daydream rendering knowledge is not required. \n\n\n\n\n\n\nPowerful\n - VR-specific rendering optimizations and optional access to low-level graphics pipeline allow you to create and optimize high performance graphics.   \n\n\n\n\n\n\nOptimized for mobile\n - Built with mobile performance in mind, GearVRf provides easy and performance-oriented access to Android OS system level calls.  \n\n\n\n\n\n\nOpen source\n - that means no licensing fees or royalties \never\n, and active developer community contributions \n\n\n\n\n\n\nEfficient\n - GearVRf's interface layer is abstracted from the target mobile VR platform SDK. You can write code once and build for both Gear VR and Daydream/Cardboard. Default build options create a single apk that works for both, with run-time flow checks for Oculus service that revert to Daydream if Oculus not available. Google Cardboard is even supported, as Daydream's backend reverts to the Google Cardboard service if the mobile device does not support Daydream!\n\n\n\n\n\n\n\n\nGet Started", 
            "title": "Home"
        }, 
        {
            "location": "/#gearvr-framework-project", 
            "text": "The Gear VR Framework (GearVRf) Project is a lightweight, powerful, open source rendering engine with a Java interface for developing mobile VR games and applications for Gear VR and Google Daydream View.  GearVRf is:    Simple  - Java interface, Android Studio build environment and a simple SDK allow you to prototype rapidly. In-depth OpenGL and Oculus/Daydream rendering knowledge is not required.     Powerful  - VR-specific rendering optimizations and optional access to low-level graphics pipeline allow you to create and optimize high performance graphics.       Optimized for mobile  - Built with mobile performance in mind, GearVRf provides easy and performance-oriented access to Android OS system level calls.      Open source  - that means no licensing fees or royalties  ever , and active developer community contributions     Efficient  - GearVRf's interface layer is abstracted from the target mobile VR platform SDK. You can write code once and build for both Gear VR and Daydream/Cardboard. Default build options create a single apk that works for both, with run-time flow checks for Oculus service that revert to Daydream if Oculus not available. Google Cardboard is even supported, as Daydream's backend reverts to the Google Cardboard service if the mobile device does not support Daydream!     Get Started", 
            "title": "GearVR Framework Project"
        }, 
        {
            "location": "/getting_started/", 
            "text": "Software Requirements\n\n\nBefore start using GearVR Framework, make sure you download the following SDKs\n\n\n\n\nAndroid Studio\n\n\nJDK 1.7 or above\n\n\nOculus Mobile SDK\n (If developing for Samsung GearVR)\n\n\nGoogle VR SDK\n (If developing for Google DayDream)\n\n\n\n\nHardware Requirements\n\n\nGearVR Framework supports following devices\n\n\n\n\nGear VR compatible Samsung phone:\n\n\nNote 8\n\n\nGalaxy S8\n\n\nGalaxy S8+\n\n\nGalaxy S7\n\n\nGalaxy S7 Edge\n\n\nNote 5\n\n\nGalaxy S6\n\n\nGalaxy S6 Edge\n\n\nGalaxy S6+\n\n\n\n\n\n\nSamsung Gear VR headset\n\n\nDaydream-ready phone\n\n\nGoogle Daydream View VR headset\n\n\n\n\nGetting Started\n\n\nGetting started with GearVR Framework in few simple steps\n\n\n\n\nDownload the \ntemplate project\n\n\nRename your project by changine the folder name\n\n\nOpen the project with Android Studio\n\n\nRename your Android App by updating \napp_name\n field of \napp/src/main/res/values/strings.xml\n\n\n(For Gear VR only) Make sure to download your \nOculus signature file\n and copy it under \napp\\src\\main\\assets\n folder\n\n\n\n\n(For DayDream only) comment out or remove following code\n\n\n\n\n\n\nin \napp/build.gradle\n \n\n\ncompile \"org.gearvrf:backend_oculus:$gearvrfVersion\"\n\n\n\n\n\n\nin \nAndroidManifest.xml\n\n\nmeta-data android:name=\"com.samsung.android.vr.application.mode\" android:value=\"vr_only\"/\n\n\n\n\n\n\n\n\n\n\nUpdate the applicationID in \napp/build.gradle\n to avoid conflict between other GearVR Framework apps.\n\n\n\n\nClick Run button and put on your VR device\n\n\n\n\nDevice Setup\n\n\nGear VR\n\n\nAfter you build the application, click \nStart\n and your device will install Oculus automatically.\n\n\n\n\nNote\n\n\nYou can test VR apps without a VR headset, by enabling Samsung VR service developer mode.\nSettings \n Apps \n manage applications \n Gear VR Service \n Storage \n Manage Storage - press the \"VR Service Version\" 6 times. After that a 'You are a developer' message will appear.\n\n\n\n\n\n\nNote\n\n\nMake sure to install your VR app with a valid oculus signature on the device first. Otherwise you'll see a 'You are not a developer' message.\n\n\n\n\n\n\nWarning\n\n\nScreen will start blinking after you turn on the developer mode\n\n\n\n\nDayDream\n\n\nEnable Google VR Service from \"Settings\" =\n \"Apps\" =\n \"Google VR Service\" make sure it has the permission it required to run.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting_started/#software-requirements", 
            "text": "Before start using GearVR Framework, make sure you download the following SDKs   Android Studio  JDK 1.7 or above  Oculus Mobile SDK  (If developing for Samsung GearVR)  Google VR SDK  (If developing for Google DayDream)", 
            "title": "Software Requirements"
        }, 
        {
            "location": "/getting_started/#hardware-requirements", 
            "text": "GearVR Framework supports following devices   Gear VR compatible Samsung phone:  Note 8  Galaxy S8  Galaxy S8+  Galaxy S7  Galaxy S7 Edge  Note 5  Galaxy S6  Galaxy S6 Edge  Galaxy S6+    Samsung Gear VR headset  Daydream-ready phone  Google Daydream View VR headset", 
            "title": "Hardware Requirements"
        }, 
        {
            "location": "/getting_started/#getting-started", 
            "text": "Getting started with GearVR Framework in few simple steps   Download the  template project  Rename your project by changine the folder name  Open the project with Android Studio  Rename your Android App by updating  app_name  field of  app/src/main/res/values/strings.xml  (For Gear VR only) Make sure to download your  Oculus signature file  and copy it under  app\\src\\main\\assets  folder   (For DayDream only) comment out or remove following code    in  app/build.gradle    compile \"org.gearvrf:backend_oculus:$gearvrfVersion\"    in  AndroidManifest.xml  meta-data android:name=\"com.samsung.android.vr.application.mode\" android:value=\"vr_only\"/      Update the applicationID in  app/build.gradle  to avoid conflict between other GearVR Framework apps.   Click Run button and put on your VR device", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting_started/#device-setup", 
            "text": "", 
            "title": "Device Setup"
        }, 
        {
            "location": "/getting_started/#gear-vr", 
            "text": "After you build the application, click  Start  and your device will install Oculus automatically.   Note  You can test VR apps without a VR headset, by enabling Samsung VR service developer mode.\nSettings   Apps   manage applications   Gear VR Service   Storage   Manage Storage - press the \"VR Service Version\" 6 times. After that a 'You are a developer' message will appear.    Note  Make sure to install your VR app with a valid oculus signature on the device first. Otherwise you'll see a 'You are not a developer' message.    Warning  Screen will start blinking after you turn on the developer mode", 
            "title": "Gear VR"
        }, 
        {
            "location": "/getting_started/#daydream", 
            "text": "Enable Google VR Service from \"Settings\" =  \"Apps\" =  \"Google VR Service\" make sure it has the permission it required to run.", 
            "title": "DayDream"
        }, 
        {
            "location": "/tutorials/simple_vr_app/", 
            "text": "Overview\n\n\nAfter setting up GearVR Framework, let's create our first VR app and learn a few very important concepts in the process.\n\n\nCreate Project\n\n\nCreate a GearVR Framework project by copying the \ntemplate project\n \n\n\nPerform the following steps to make sure your project runs correctly\n\n\n\n\n(if developing for Gear VR) Copy your \nOculus signature file\n to \napp/src/main/assets\n folder.\n\n\nChange the \napplicationId\n in \nbuild.gradle\n to a unique name to avoid naming conflict when you test the app later\n\n\nChange the \napp_name\n in \nres/values/strings.xml\n to avoid confusion when you debug the app.\n\n\n\n\nProject Structure\n\n\nBefore we start, let's take a look at some essential parts of a GearVR Framework app\n\n\n\n\nThe template project contains two classes, \nMainActivity\n and \nMainScene\n\n\n\n\n\n\nMainActivity\n is the entry point of the app, like \nandroid.app.Activity\n it handles the initialization and life cycle of a VR app.\n\n\n\n\n\n\nMainScene\n is the container of a scene, just like a scene in the movie, it contains things like camera, characters, visual effects etc, it is the place for all your VR content.\n\n\n\n\n\n\nIn the assets folder, there are two files: \ngvr.xml\n and \noculussig\n file\n\n\n\n\n\n\ngvr.xml\n is where you config various behavior of GearVR framework, which we'll get into detail in future tutorials\n\n\n\n\n\n\noculussig\n is the Oculus signing file which allows you to deploy debug apps to VR devices, so always make sure this you have a signing file in your project\n\n\n\n\n\n\nScene\n\n\nUsually a VR app/game consists of one or more scenes. The template project already created one Scene called \nMainScene\n and it should be the starting point for your VR project. \n\n\n\n\nNote\n\n\nMainScene\n extends from \nGVRMain\n, if you're creating your own entry point class, make sure to extend \nGVRMain\n\n\n\n\nThere are two functions in \nMainScene\n; both are important for the scene to work\n\n\n\n\nonInit() is called when the scene is being loaded, and can be used to perform actions like object creation, assets loading.\n\n\nonStep() called once per frame, can be used to perform things like animation, AI or user interactions\n\n\n\n\nAdd object\n\n\nAdding an object to the scene is simple, Just create the object and specify the material and add it to the scene\n\n\nFirst, let's add a new member variable for the Cube to the \nMainScene\n\n\n    \nGVRCubeSceneObject\n \nmCube\n\n\n\n\n\n\nThen add the cube to our scene with following code in \nonInit()\n function\n\n\n    \n//Create a cube\n\n    \nmCube\n \n=\n \nnew\n \nGVRCubeSceneObject\n(\ngvrContext\n,\n \ntrue\n,\n\n                 \nnew\n \nGVRMaterial\n(\ngvrContext\n,\n \nGVRMaterial\n.\nGVRShaderType\n.\nPhong\n.\nID\n);\n\n\n    \n//Set position of the cube at (0, -2, -3)\n\n    \nmCube\n.\ngetTransform\n().\nsetPosition\n(\n0\n,\n \n-\n2\n,\n \n-\n3\n);\n\n\n    \n//Add cube to the scene\n\n    \ngvrContext\n.\ngetMainScene\n().\naddSceneObject\n(\nmCube\n);\n\n\n\n\n\n\nBuild and run the app, you should be able to see a white cube on the screen\n\n\n\n\nNote\n\n\nIf you're using \"VR developer mode\" without headset the orientation might be different, you might need to turn around to see the cube\n\n\n\n\nMake it move\n\n\nNow let's make the cube rotate. Because we want to see the cube rotate continuously, we need to update its rotation every frame. So instead of the \nonInit()\n function we need to add the rotation logic into \nonStep()\n function\n\n\nAdd the following code to the \nonStep()\n function\n\n\n    \n//Rotate the cube along the Y axis\n\n    \nmCube\n.\ngetTransform\n().\nrotateByAxis\n(\n1\n,\n \n0\n,\n \n1\n,\n \n0\n);\n\n\n\n\n\n\nrotateByAxis()\n has 4 parameters, the first specifies the rotation angle while the rest 3 defines the rotation axis.\n\n\n\n\n\n\nBy \nopengl_tutorials\n, \nCC-BY-NC-ND\n\n\n\n\nBuild and run the app, you should be able to see a rotating cube.\n\n\nNow that you have a rotating cube in VR, feel free to try different things: change its color, make it scale up and down or move it around.\n\n\nSource Code\n\n\nComplete \nSource Code\n for this sample", 
            "title": "Simple VR app"
        }, 
        {
            "location": "/tutorials/simple_vr_app/#overview", 
            "text": "After setting up GearVR Framework, let's create our first VR app and learn a few very important concepts in the process.", 
            "title": "Overview"
        }, 
        {
            "location": "/tutorials/simple_vr_app/#create-project", 
            "text": "Create a GearVR Framework project by copying the  template project    Perform the following steps to make sure your project runs correctly   (if developing for Gear VR) Copy your  Oculus signature file  to  app/src/main/assets  folder.  Change the  applicationId  in  build.gradle  to a unique name to avoid naming conflict when you test the app later  Change the  app_name  in  res/values/strings.xml  to avoid confusion when you debug the app.", 
            "title": "Create Project"
        }, 
        {
            "location": "/tutorials/simple_vr_app/#project-structure", 
            "text": "Before we start, let's take a look at some essential parts of a GearVR Framework app   The template project contains two classes,  MainActivity  and  MainScene    MainActivity  is the entry point of the app, like  android.app.Activity  it handles the initialization and life cycle of a VR app.    MainScene  is the container of a scene, just like a scene in the movie, it contains things like camera, characters, visual effects etc, it is the place for all your VR content.    In the assets folder, there are two files:  gvr.xml  and  oculussig  file    gvr.xml  is where you config various behavior of GearVR framework, which we'll get into detail in future tutorials    oculussig  is the Oculus signing file which allows you to deploy debug apps to VR devices, so always make sure this you have a signing file in your project", 
            "title": "Project Structure"
        }, 
        {
            "location": "/tutorials/simple_vr_app/#scene", 
            "text": "Usually a VR app/game consists of one or more scenes. The template project already created one Scene called  MainScene  and it should be the starting point for your VR project.    Note  MainScene  extends from  GVRMain , if you're creating your own entry point class, make sure to extend  GVRMain   There are two functions in  MainScene ; both are important for the scene to work   onInit() is called when the scene is being loaded, and can be used to perform actions like object creation, assets loading.  onStep() called once per frame, can be used to perform things like animation, AI or user interactions", 
            "title": "Scene"
        }, 
        {
            "location": "/tutorials/simple_vr_app/#add-object", 
            "text": "Adding an object to the scene is simple, Just create the object and specify the material and add it to the scene  First, let's add a new member variable for the Cube to the  MainScene       GVRCubeSceneObject   mCube   Then add the cube to our scene with following code in  onInit()  function       //Create a cube \n     mCube   =   new   GVRCubeSceneObject ( gvrContext ,   true , \n                  new   GVRMaterial ( gvrContext ,   GVRMaterial . GVRShaderType . Phong . ID ); \n\n     //Set position of the cube at (0, -2, -3) \n     mCube . getTransform (). setPosition ( 0 ,   - 2 ,   - 3 ); \n\n     //Add cube to the scene \n     gvrContext . getMainScene (). addSceneObject ( mCube );   Build and run the app, you should be able to see a white cube on the screen   Note  If you're using \"VR developer mode\" without headset the orientation might be different, you might need to turn around to see the cube", 
            "title": "Add object"
        }, 
        {
            "location": "/tutorials/simple_vr_app/#make-it-move", 
            "text": "Now let's make the cube rotate. Because we want to see the cube rotate continuously, we need to update its rotation every frame. So instead of the  onInit()  function we need to add the rotation logic into  onStep()  function  Add the following code to the  onStep()  function       //Rotate the cube along the Y axis \n     mCube . getTransform (). rotateByAxis ( 1 ,   0 ,   1 ,   0 );   rotateByAxis()  has 4 parameters, the first specifies the rotation angle while the rest 3 defines the rotation axis.    By  opengl_tutorials ,  CC-BY-NC-ND   Build and run the app, you should be able to see a rotating cube.  Now that you have a rotating cube in VR, feel free to try different things: change its color, make it scale up and down or move it around.", 
            "title": "Make it move"
        }, 
        {
            "location": "/tutorials/simple_vr_app/#source-code", 
            "text": "Complete  Source Code  for this sample", 
            "title": "Source Code"
        }, 
        {
            "location": "/tutorials/play_with_material/", 
            "text": "Overview\n\n\nNow that you've created your first VR app with GearVR Framework. We are going to learn how to create things that look much better in VR.\n\n\nCreate Project\n\n\nCreate a GearVR Framework project by copying the \ntemplate project\n \n\n\nPerform the following steps to make sure your project runs correctly\n\n\n\n\n(if developing for Gear VR) Copy your \nOculus signature file\n to \napp/src/main/assets\n folder.\n\n\nChange the \napplicationId\n in \nbuild.gradle\n to a unique name to avoid naming conflict when you test the app later\n\n\nChange the \napp_name\n in \nres/values/strings.xml\n to avoid confusion when you debug the app.\n\n\n\n\nWhat is a Material\n\n\nIn 3D graphics term, a material is a set of textures and shader parameters that can be used to simulate different types of materials in real life.\n\n\nFor example with the correct material, you can make your 3D objects looks like bricks.\n\n\n\n\n\n\nBy CaliCoastReplay - Own work, \nCC BY 4.0\n\n\n\n\nCreate SceneObjects\n\n\nFirst, let's create one cube and one sphere\n\n\n    \nGVRCubeSceneObject\n \nmCube\n;\n\n    \nGVRSphereSceneObject\n \nmSphere\n;\n\n\n\n\n\n\nand initialize them in \nonInit\n function\n\n\n    \n//Create Shpere\n\n    \nmSphere\n \n=\n \nnew\n \nGVRSphereSceneObject\n(\ngvrContext\n);\n\n    \nmSphere\n.\ngetTransform\n().\nsetPosition\n(\n2\n,\n \n0\n,\n \n-\n3\n);\n\n    \ngvrContext\n.\ngetMainScene\n().\naddSceneObject\n(\nmSphere\n);\n\n\n    \n//Create Cube\n\n    \nmCube\n \n=\n \nnew\n \nGVRCubeSceneObject\n(\ngvrContext\n);\n\n    \nmCube\n.\ngetTransform\n().\nsetPosition\n(-\n2\n,\n \n0\n,\n \n-\n3\n);\n\n    \ngvrContext\n.\ngetMainScene\n().\naddSceneObject\n(\nmCube\n);\n\n\n\n\n\n\nMaterial with solid color\n\n\nMaterial with solid color can be used to create simple 3D objects, such as a red ball, a white cube.\n\n\nLet's create a red material with the following code.\n\n\n    \nGVRMaterial\n \nflatMaterial\n;\n\n    \nflatMaterial\n \n=\n \nnew\n \nGVRMaterial\n(\ngvrContext\n,\n \nGVRMaterial\n.\nGVRShaderType\n.\nPhong\n.\nID\n);\n\n    \nflatMaterial\n.\nsetColor\n(\n1.0f\n,\n \n0.0f\n,\n \n0.0f\n);\n\n\n\n\n\n\n\n\nNote\n\n\nGearVR Framework uses \nRGB color model\n and the range of each color is from 0 to 1.\n\n\n\n\nNow that we created the material, let's apply it to the sphere.\n\n\n    \nmSphere\n.\ngetRenderData\n().\nsetMaterial\n(\nflatMaterial\n);\n\n\n\n\n\n\nBuild and run your app and see if you can find the red sphere.\n\n\nMaterial with Texture\n\n\nSolid color objects are good, but what if I want to create things like a wooden box or brick wall? \n\n\nYes, we can do that with textures, let's learn how to do that.\n\n\nThe first step of creating a textured material is to have one texture. Which you can download it here\n\n\n\n\nPlace the texture under \napp\\src\\main\\res\\raw\n folder\n\n\nSync your project, and you should be able to load the texture using following code\n\n\n    \nGVRTexture\n \ntexture\n \n=\n \ngvrContext\n.\ngetAssetLoader\n().\nloadTexture\n(\nnew\n \nGVRAndroidResource\n(\ngvrContext\n,\n \nR\n.\nraw\n.\ncrate_wood\n));\n\n\n\n\n\n\nYou can create a material with texture using following code.\n\n\n    \nGVRMaterial\n \ntextureMaterial\n;\n\n    \ntextureMaterial\n \n=\n \nnew\n \nGVRMaterial\n(\ngvrContext\n,\n \nGVRMaterial\n.\nGVRShaderType\n.\nPhong\n.\nID\n);\n\n    \ntextureMaterial\n.\nsetTexture\n(\ndiffuseTexture\n,\n \ntexture\n);\n\n    \nmCube\n.\ngetRenderData\n().\nsetMaterial\n(\ntextureMaterial\n);\n\n\n\n\n\n\nBuild and run your app, you should see the wooden crate we just created.\n\n\nTurn on the light\n\n\nWe just created a wooden crate, but if you look at it closely, you'll find it's really bright and doesn't feel natural. \n\n\nIn this section, we're going to apply lighting to both objects to make it look a lot nicer.\n\n\nFirst, let's add a light to the scene using following code\n\n\n    \nGVRPointLight\n \npointLight\n;\n\n    \npointLight\n \n=\n \nnew\n \nGVRPointLight\n(\ngvrContext\n);\n\n    \npointLight\n.\nsetDiffuseIntensity\n(\n0.6f\n,\n \n0.5f\n,\n \n0.5f\n,\n \n1.0f\n);\n\n    \npointLight\n.\nsetSpecularIntensity\n(\n0.5f\n,\n \n0.5f\n,\n \n0.5f\n,\n \n1.0f\n);\n\n\n    \nGVRSceneObject\n \nlightNode\n \n=\n \nnew\n \nGVRSceneObject\n(\ngvrContext\n);\n\n    \nlightNode\n.\ngetTransform\n().\nsetPosition\n(\n0\n,\n0\n,\n0\n);\n\n    \nlightNode\n.\nattachLight\n(\npointLight\n);\n\n\n    \ngvrContext\n.\ngetMainScene\n().\naddSceneObject\n(\nlightNode\n);\n\n\n\n\n\n\nThe code snippet will create a light with diffuse color of (0.6, 0.5, 0.5) and specular color of (0.5, 0.5, 0.5), Which will give the scene a red tone.\n\n\nThere is one more thing we need to do to make the light work - ensure all the scene objects uses PhongShader and uses diffuse texture.\n\n\nAdd the following line to the cube object:\n\n\ntextureMaterial\n.\nsetTexture\n(\ndiffuseTexture\n,\n \ntexture\n);\n\n\n\n\n\n\nBuild and run the app, you should be able to see the lighting on the sphere and crate box, feel free to tweak with the light to make it look better.\n\n\nMake it move\n\n\nAdd the following code to the \nonStep()\n function to make the cube move.\n\n\n    \n//Rotate the cube along the Y axis\n\n    \nmCube\n.\ngetTransform\n().\nrotateByAxis\n(\n1\n,\n \n0\n,\n \n1\n,\n \n0\n);\n\n\n\n\n\n\nSource Code\n\n\nComplete \nSource Code\n for this sample", 
            "title": "Play With Materials"
        }, 
        {
            "location": "/tutorials/play_with_material/#overview", 
            "text": "Now that you've created your first VR app with GearVR Framework. We are going to learn how to create things that look much better in VR.", 
            "title": "Overview"
        }, 
        {
            "location": "/tutorials/play_with_material/#create-project", 
            "text": "Create a GearVR Framework project by copying the  template project    Perform the following steps to make sure your project runs correctly   (if developing for Gear VR) Copy your  Oculus signature file  to  app/src/main/assets  folder.  Change the  applicationId  in  build.gradle  to a unique name to avoid naming conflict when you test the app later  Change the  app_name  in  res/values/strings.xml  to avoid confusion when you debug the app.", 
            "title": "Create Project"
        }, 
        {
            "location": "/tutorials/play_with_material/#what-is-a-material", 
            "text": "In 3D graphics term, a material is a set of textures and shader parameters that can be used to simulate different types of materials in real life.  For example with the correct material, you can make your 3D objects looks like bricks.    By CaliCoastReplay - Own work,  CC BY 4.0", 
            "title": "What is a Material"
        }, 
        {
            "location": "/tutorials/play_with_material/#create-sceneobjects", 
            "text": "First, let's create one cube and one sphere       GVRCubeSceneObject   mCube ; \n     GVRSphereSceneObject   mSphere ;   and initialize them in  onInit  function       //Create Shpere \n     mSphere   =   new   GVRSphereSceneObject ( gvrContext ); \n     mSphere . getTransform (). setPosition ( 2 ,   0 ,   - 3 ); \n     gvrContext . getMainScene (). addSceneObject ( mSphere ); \n\n     //Create Cube \n     mCube   =   new   GVRCubeSceneObject ( gvrContext ); \n     mCube . getTransform (). setPosition (- 2 ,   0 ,   - 3 ); \n     gvrContext . getMainScene (). addSceneObject ( mCube );", 
            "title": "Create SceneObjects"
        }, 
        {
            "location": "/tutorials/play_with_material/#material-with-solid-color", 
            "text": "Material with solid color can be used to create simple 3D objects, such as a red ball, a white cube.  Let's create a red material with the following code.       GVRMaterial   flatMaterial ; \n     flatMaterial   =   new   GVRMaterial ( gvrContext ,   GVRMaterial . GVRShaderType . Phong . ID ); \n     flatMaterial . setColor ( 1.0f ,   0.0f ,   0.0f );    Note  GearVR Framework uses  RGB color model  and the range of each color is from 0 to 1.   Now that we created the material, let's apply it to the sphere.       mSphere . getRenderData (). setMaterial ( flatMaterial );   Build and run your app and see if you can find the red sphere.", 
            "title": "Material with solid color"
        }, 
        {
            "location": "/tutorials/play_with_material/#material-with-texture", 
            "text": "Solid color objects are good, but what if I want to create things like a wooden box or brick wall?   Yes, we can do that with textures, let's learn how to do that.  The first step of creating a textured material is to have one texture. Which you can download it here   Place the texture under  app\\src\\main\\res\\raw  folder  Sync your project, and you should be able to load the texture using following code       GVRTexture   texture   =   gvrContext . getAssetLoader (). loadTexture ( new   GVRAndroidResource ( gvrContext ,   R . raw . crate_wood ));   You can create a material with texture using following code.       GVRMaterial   textureMaterial ; \n     textureMaterial   =   new   GVRMaterial ( gvrContext ,   GVRMaterial . GVRShaderType . Phong . ID ); \n     textureMaterial . setTexture ( diffuseTexture ,   texture ); \n     mCube . getRenderData (). setMaterial ( textureMaterial );   Build and run your app, you should see the wooden crate we just created.", 
            "title": "Material with Texture"
        }, 
        {
            "location": "/tutorials/play_with_material/#turn-on-the-light", 
            "text": "We just created a wooden crate, but if you look at it closely, you'll find it's really bright and doesn't feel natural.   In this section, we're going to apply lighting to both objects to make it look a lot nicer.  First, let's add a light to the scene using following code       GVRPointLight   pointLight ; \n     pointLight   =   new   GVRPointLight ( gvrContext ); \n     pointLight . setDiffuseIntensity ( 0.6f ,   0.5f ,   0.5f ,   1.0f ); \n     pointLight . setSpecularIntensity ( 0.5f ,   0.5f ,   0.5f ,   1.0f ); \n\n     GVRSceneObject   lightNode   =   new   GVRSceneObject ( gvrContext ); \n     lightNode . getTransform (). setPosition ( 0 , 0 , 0 ); \n     lightNode . attachLight ( pointLight ); \n\n     gvrContext . getMainScene (). addSceneObject ( lightNode );   The code snippet will create a light with diffuse color of (0.6, 0.5, 0.5) and specular color of (0.5, 0.5, 0.5), Which will give the scene a red tone.  There is one more thing we need to do to make the light work - ensure all the scene objects uses PhongShader and uses diffuse texture.  Add the following line to the cube object:  textureMaterial . setTexture ( diffuseTexture ,   texture );   Build and run the app, you should be able to see the lighting on the sphere and crate box, feel free to tweak with the light to make it look better.", 
            "title": "Turn on the light"
        }, 
        {
            "location": "/tutorials/play_with_material/#make-it-move", 
            "text": "Add the following code to the  onStep()  function to make the cube move.       //Rotate the cube along the Y axis \n     mCube . getTransform (). rotateByAxis ( 1 ,   0 ,   1 ,   0 );", 
            "title": "Make it move"
        }, 
        {
            "location": "/tutorials/play_with_material/#source-code", 
            "text": "Complete  Source Code  for this sample", 
            "title": "Source Code"
        }, 
        {
            "location": "/tutorials/play_with_3d_models/", 
            "text": "Overview\n\n\nNow that you've learnt how to apply material to the VR app with GearVR Framework, we are going to learn how to play with 3D model.\n\n\nCreate Project\n\n\nCreate a GearVR Framework project by copying the \ntemplate project\n \n\n\nPerform the following steps to make sure your project runs correctly\n\n\n\n\n(if developing for Gear VR) Copy your \nOculus signature file\n to \napp/src/main/assets\n folder.\n\n\nChange the \napplicationId\n in \nbuild.gradle\n to a unique name to avoid naming conflict when you test the app later\n\n\nChange the \napp_name\n in \nres/values/strings.xml\n to avoid confusion when you debug the app.\n\n\n\n\nIntro\n\n\nOften times when we develop VR applications, we need to show complex 3D models. For example a plane, a castle or even an earth. We're going to learn how to achieve it with GearVR Framework\n\n\nBefore we start, we need to have a 3D model file. Here is a \n3D T-rex model\n and it's \ntexture\n we're going to use for this tutorial. You can preview them in \nFBX Review\n. \n\n\nIf you want to use 3D models that you prefer that's totally fine, just keep in mind it has to be one of the following file formats:\n\n\n\n\nOBJ\n\n\nFBX\n\n\nCollada(.dae)\n\n\nX3D\n\n\nAll formats supported by \nAssimp\n\n\n\n\nHow to load 3D models\n\n\nThe first step of loading a 3D model into your VR app is to place it correctly. Please make sure to copy the files to the following path.\n\n\n\n\nCopy \ntrex_mesh.fbx\n into \napp/src/main/assets\n\n\nCopy \ntrex_tex_diffuse\n into \napp/src/main/assets\n\n\n\n\nAfter copying the 3D model files, we can use \nGVRAssetLoader\n class to load them, it is accessible from the context by calling \nGVRContext.getAssetLoader()\n\n\nUsing the following code to load the fbx file and texture \n\n\nGVRMesh\n \ndinoMesh\n \n=\n \ngvrContext\n.\ngetAssetLoader\n().\nloadMesh\n(\nnew\n \nGVRAndroidResource\n(\ngvrContext\n,\n \ntrex_mesh.fbx\n));\n\n\nGVRTexture\n \ndinoTexture\n \n=\n \ngvrContext\n.\ngetAssetLoader\n().\nloadTexture\n(\nnew\n \nGVRAndroidResource\n(\ngvrContext\n,\n \ntrex_tex_diffuse.png\n));\n\n\n\n\n\n\n\n\nNote\n\n\nLoading methods have \nFuture\n in its API such as \nloadFutureTexture\n is deprecated\n\n\n\n\nAfter 3D model and texture both loaded, we can add them to the scene with a scene object\n\n\n    \nGVRSceneObject\n \ndinoObj\n \n=\n \nnew\n \nGVRSceneObject\n(\ngvrContext\n,\n \ndinoMesh\n,\n \ndinoTexture\n);\n\n\n    \ndinoObj\n.\ngetTransform\n().\nsetPosition\n(\n0\n,\n0\n,-\n10\n);\n\n    \ndinoObj\n.\ngetTransform\n().\nrotateByAxis\n(-\n90\n,\n \n1f\n,\n \n0f\n,\n \n0f\n);\n\n    \ngvrContext\n.\ngetMainScene\n().\naddSceneObject\n(\ndinoObj\n);\n\n\n\n\n\n\n\n\nNote\n\n\nWe create \nGVRSceneObject\n instead of use \nAssetLoader.loadModel()\n because \nloadModel()\n require fbx files to have correct path to texture file which a lot of 3D modeling software failed to produce.\n\n\n\n\n\n\nNote\n\n\nYou might need to rotate the model differently if you're using other models.\n\n\n\n\nBuild and run the app, you should be able to see a T-Rex!\n\n\nWork with 3D modeling tools\n\n\nFbx is the recommended format for the GearVR framework. Currently, all major 3D modeling tools support exporting to FBX format.\n\n\nSource Code\n\n\nComplete \nSource Code\n for this sample", 
            "title": "Play With 3D Models"
        }, 
        {
            "location": "/tutorials/play_with_3d_models/#overview", 
            "text": "Now that you've learnt how to apply material to the VR app with GearVR Framework, we are going to learn how to play with 3D model.", 
            "title": "Overview"
        }, 
        {
            "location": "/tutorials/play_with_3d_models/#create-project", 
            "text": "Create a GearVR Framework project by copying the  template project    Perform the following steps to make sure your project runs correctly   (if developing for Gear VR) Copy your  Oculus signature file  to  app/src/main/assets  folder.  Change the  applicationId  in  build.gradle  to a unique name to avoid naming conflict when you test the app later  Change the  app_name  in  res/values/strings.xml  to avoid confusion when you debug the app.", 
            "title": "Create Project"
        }, 
        {
            "location": "/tutorials/play_with_3d_models/#intro", 
            "text": "Often times when we develop VR applications, we need to show complex 3D models. For example a plane, a castle or even an earth. We're going to learn how to achieve it with GearVR Framework  Before we start, we need to have a 3D model file. Here is a  3D T-rex model  and it's  texture  we're going to use for this tutorial. You can preview them in  FBX Review .   If you want to use 3D models that you prefer that's totally fine, just keep in mind it has to be one of the following file formats:   OBJ  FBX  Collada(.dae)  X3D  All formats supported by  Assimp", 
            "title": "Intro"
        }, 
        {
            "location": "/tutorials/play_with_3d_models/#how-to-load-3d-models", 
            "text": "The first step of loading a 3D model into your VR app is to place it correctly. Please make sure to copy the files to the following path.   Copy  trex_mesh.fbx  into  app/src/main/assets  Copy  trex_tex_diffuse  into  app/src/main/assets   After copying the 3D model files, we can use  GVRAssetLoader  class to load them, it is accessible from the context by calling  GVRContext.getAssetLoader()  Using the following code to load the fbx file and texture   GVRMesh   dinoMesh   =   gvrContext . getAssetLoader (). loadMesh ( new   GVRAndroidResource ( gvrContext ,   trex_mesh.fbx ));  GVRTexture   dinoTexture   =   gvrContext . getAssetLoader (). loadTexture ( new   GVRAndroidResource ( gvrContext ,   trex_tex_diffuse.png ));    Note  Loading methods have  Future  in its API such as  loadFutureTexture  is deprecated   After 3D model and texture both loaded, we can add them to the scene with a scene object       GVRSceneObject   dinoObj   =   new   GVRSceneObject ( gvrContext ,   dinoMesh ,   dinoTexture ); \n\n     dinoObj . getTransform (). setPosition ( 0 , 0 ,- 10 ); \n     dinoObj . getTransform (). rotateByAxis (- 90 ,   1f ,   0f ,   0f ); \n     gvrContext . getMainScene (). addSceneObject ( dinoObj );    Note  We create  GVRSceneObject  instead of use  AssetLoader.loadModel()  because  loadModel()  require fbx files to have correct path to texture file which a lot of 3D modeling software failed to produce.    Note  You might need to rotate the model differently if you're using other models.   Build and run the app, you should be able to see a T-Rex!", 
            "title": "How to load 3D models"
        }, 
        {
            "location": "/tutorials/play_with_3d_models/#work-with-3d-modeling-tools", 
            "text": "Fbx is the recommended format for the GearVR framework. Currently, all major 3D modeling tools support exporting to FBX format.", 
            "title": "Work with 3D modeling tools"
        }, 
        {
            "location": "/tutorials/play_with_3d_models/#source-code", 
            "text": "Complete  Source Code  for this sample", 
            "title": "Source Code"
        }, 
        {
            "location": "/tutorials/play_with_animation/", 
            "text": "Overview\n\n\nNow that you've learnt how to load 3D model, we are going to learn how to play with animation in VR\n\n\nCreate Project\n\n\nCreate a GearVR Framework project by copying the \ntemplate project\n \n\n\nPerform the following steps to make sure your project runs correctly\n\n\n\n\n(if developing for Gear VR) Copy your \nOculus signature file\n to \napp/src/main/assets\n folder.\n\n\nChange the \napplicationId\n in \nbuild.gradle\n to a unique name to avoid naming conflict when you test the app later\n\n\nChange the \napp_name\n in \nres/values/strings.xml\n to avoid confusion when you debug the app.\n\n\n\n\nIntro\n\n\nBefore we start, we have to obtain a 3D model file with animation.\n\n\nGear VR Framework supports following formats\n\n FBX\n\n Collada(.dae)\n\n\nAnd here is one animated 3D model that you can download\n\n\n\n\n3D model with animation\n\n\nTexture\n\n\n\n\nHow to play animations\n\n\nMake sure to copy both files into \napp/src/main/assets\n folder\n\n\nYou can load the animated model with following code\n\n\n    \nGVRModelSceneObject\n \ncharacter\n \n=\n \ngvrContext\n.\ngetAssetLoader\n().\nloadModel\n(\nastro_boy.dae\n,\n                                                                                              \ngvrContext\n.\ngetMainScene\n());\n\n    \ncharacter\n.\ngetTransform\n().\nsetRotationByAxis\n(\n45.0f\n,\n \n0.0f\n,\n \n1.0f\n,\n \n0.0f\n);\n\n    \ncharacter\n.\ngetTransform\n().\nsetScale\n(\n3\n,\n \n3\n,\n \n3\n);\n\n    \ncharacter\n.\ngetTransform\n().\nsetPosition\n(\n0.0f\n,\n \n-\n0.4f\n,\n \n-\n0.5f\n);\n\n\n\n\n\n\nAnd play the animation with \nGVRAnimator\n, here we make sure the animation in looping forever with the \nsetRepeatCount\n set to -1\n\n\n    \nGVRAnimator\n \nanimator\n \n=\n \n(\nGVRAnimator\n)\ncharacter\n.\ngetComponent\n(\nGVRAnimator\n.\ngetComponentType\n());\n\n    \nanimator\n.\nsetRepeatCount\n(-\n1\n);\n\n    \nanimator\n.\nsetRepeatMode\n(\nGVRRepeatMode\n.\nREPEATED\n);\n\n    \nanimator\n.\nstart\n();\n\n\n\n\n\n\nWork with 3D modeling tools\n\n\nFbx is the recommended format for the GearVR framework. Currently, all major 3D modeling tools support exporting to FBX format.\n\n\nSource Code\n\n\nComplete \nSource Code\n for this sample", 
            "title": "Play With Animation"
        }, 
        {
            "location": "/tutorials/play_with_animation/#overview", 
            "text": "Now that you've learnt how to load 3D model, we are going to learn how to play with animation in VR", 
            "title": "Overview"
        }, 
        {
            "location": "/tutorials/play_with_animation/#create-project", 
            "text": "Create a GearVR Framework project by copying the  template project    Perform the following steps to make sure your project runs correctly   (if developing for Gear VR) Copy your  Oculus signature file  to  app/src/main/assets  folder.  Change the  applicationId  in  build.gradle  to a unique name to avoid naming conflict when you test the app later  Change the  app_name  in  res/values/strings.xml  to avoid confusion when you debug the app.", 
            "title": "Create Project"
        }, 
        {
            "location": "/tutorials/play_with_animation/#intro", 
            "text": "Before we start, we have to obtain a 3D model file with animation.  Gear VR Framework supports following formats  FBX  Collada(.dae)  And here is one animated 3D model that you can download   3D model with animation  Texture", 
            "title": "Intro"
        }, 
        {
            "location": "/tutorials/play_with_animation/#how-to-play-animations", 
            "text": "Make sure to copy both files into  app/src/main/assets  folder  You can load the animated model with following code       GVRModelSceneObject   character   =   gvrContext . getAssetLoader (). loadModel ( astro_boy.dae ,                                                                                                gvrContext . getMainScene ()); \n     character . getTransform (). setRotationByAxis ( 45.0f ,   0.0f ,   1.0f ,   0.0f ); \n     character . getTransform (). setScale ( 3 ,   3 ,   3 ); \n     character . getTransform (). setPosition ( 0.0f ,   - 0.4f ,   - 0.5f );   And play the animation with  GVRAnimator , here we make sure the animation in looping forever with the  setRepeatCount  set to -1       GVRAnimator   animator   =   ( GVRAnimator ) character . getComponent ( GVRAnimator . getComponentType ()); \n     animator . setRepeatCount (- 1 ); \n     animator . setRepeatMode ( GVRRepeatMode . REPEATED ); \n     animator . start ();", 
            "title": "How to play animations"
        }, 
        {
            "location": "/tutorials/play_with_animation/#work-with-3d-modeling-tools", 
            "text": "Fbx is the recommended format for the GearVR framework. Currently, all major 3D modeling tools support exporting to FBX format.", 
            "title": "Work with 3D modeling tools"
        }, 
        {
            "location": "/tutorials/play_with_animation/#source-code", 
            "text": "Complete  Source Code  for this sample", 
            "title": "Source Code"
        }, 
        {
            "location": "/tutorials/play_with_controller/", 
            "text": "Overview\n\n\nNow that you've learnt how to use 3D model and animation with GearVR Framework. We are going to learn how to use controllers to make VR app more interactive.\n\n\nCreate Project\n\n\nCreate a GearVR Framework project by copying the \ntemplate project\n \n\n\nPerform the following steps to make sure your project runs correctly\n\n\n\n\n(if developing for Gear VR) Copy your \nOculus signature file\n to \napp/src/main/assets\n folder.\n\n\nChange the \napplicationId\n in \nbuild.gradle\n to a unique name to avoid naming conflict when you test the app later\n\n\nChange the \napp_name\n in \nres/values/strings.xml\n to avoid confusion when you debug the app.\n\n\n\n\nIntro\n\n\nBeing able to interact with in the VR environment helps a lot with the immersion. Currently, there are two ways to interact with VR content\n\n\n\n\nGearVR controller\n\n\nGaze controller\n\n\n\n\nGear VR Controller\n\n\nGear VR controller provides 3 degrees of freedom orientation tracking and trackpad control, it's highly recommended for a more immersive VR experience\n\n\n\n\n1. Enable Controller\n\n\nAdd the following code in \ninit\n class to enable the controller\n\n\n        \nGVRInputManager\n \ninput\n \n=\n \ngvrContext\n.\ngetInputManager\n();\n\n        \ninput\n.\naddCursorControllerListener\n(\nlistener\n);\n\n\n        \n//Add controller if detected any\n\n        \nfor\n \n(\nGVRCursorController\n \ncursor\n \n:\n \ninput\n.\ngetCursorControllers\n())\n \n{\n\n            \nlistener\n.\nonCursorControllerAdded\n(\ncursor\n);\n\n        \n}\n\n\n\n\n\n\n2. Create the controller listener\n\n\nBy define the \nCursorControllerListener\n, you can specify what happens when controller gets connect/disconnected or when user presses a button.\n\n\nNotice we also created a \nGVRGearControllerSceneObject\n, it is a SceneObject that acts exactly like a GearVR controller in VR. It's highly recommended for a better user experience.\n\n\n    \n//Listener for controller event\n\n    \nprivate\n \nstatic\n \nGVRCursorController\n.\nControllerEventListener\n \ncontrollerEventListener\n \n=\n \nnew\n\n            \nGVRCursorController\n.\nControllerEventListener\n()\n \n{\n\n                \n@Override\n\n                \npublic\n \nvoid\n \nonEvent\n(\nGVRCursorController\n \ngvrCursorController\n)\n \n{\n\n                    \nKeyEvent\n \nkeyEvent\n \n=\n \ngvrCursorController\n.\ngetKeyEvent\n();\n\n                    \nif\n(\nkeyEvent\n \n!=\n \nnull\n){\n\n                        \n//TODO: add logic to handle controller key press here\n\n                    \n}\n\n                \n}\n\n            \n};\n\n\n    \n//Listener for add/removal of a controller\n\n    \nprivate\n \nstatic\n \nCursorControllerListener\n \nlistener\n \n=\n \nnew\n \nCursorControllerListener\n()\n \n{\n\n\n        \n@Override\n\n        \npublic\n \nvoid\n \nonCursorControllerAdded\n(\nGVRCursorController\n \ncontroller\n)\n \n{\n\n\n            \n//Setup GearVR Controller\n\n            \nif\n \n(\ncontroller\n.\ngetControllerType\n()\n \n==\n \nGVRControllerType\n.\nCONTROLLER\n)\n \n{\n\n                \n//Create cursor\n\n                \nGVRSceneObject\n \ncursor\n \n=\n \ncreateQuad\n(\n1f\n,\n \n1f\n,\n \nR\n.\nraw\n.\ncursor\n);\n\n                \ncursor\n.\ngetRenderData\n().\nsetDepthTest\n(\nfalse\n);\n\n                \ncursor\n.\ngetRenderData\n().\nsetRenderingOrder\n(\n100000\n);\n\n\n                \n//Create GearController\n\n                \nGVRGearControllerSceneObject\n \nctrObj\n \n=\n \nnew\n \nGVRGearControllerSceneObject\n(\ns_Context\n);\n\n                \nctrObj\n.\nsetCursorController\n(\ncontroller\n);\n\n                \nctrObj\n.\nsetCursor\n(\ncursor\n);\n\n\n                \n//Setup picking\n\n                \nctrObj\n.\nsetRayDepth\n(\n8.0f\n);\n\n\n                \ncontroller\n.\naddControllerEventListener\n(\ncontrollerEventListener\n);\n\n\n            \n}\nelse\n{\n\n                \ncontroller\n.\nsetEnable\n(\nfalse\n);\n\n            \n}\n\n        \n}\n\n\n        \n@Override\n\n        \npublic\n \nvoid\n \nonCursorControllerRemoved\n(\nGVRCursorController\n \ncontroller\n)\n \n{\n\n            \nif\n \n(\ncontroller\n.\ngetControllerType\n()\n \n==\n \nGVRControllerType\n.\nCONTROLLER\n)\n \n{\n\n                \ncontroller\n.\nremoveControllerEventListener\n(\ncontrollerEventListener\n);\n\n                \ncontroller\n.\nresetSceneObject\n();\n\n            \n}\n\n        \n}\n\n    \n};\n\n\n\n\n\n\n3. Build\n\n\nBuild and run the project, you should be able to see a Gear VR controller in VR that mirrors your move\n\n\nGaze Controller\n\n\nGaze controller is available by default from GearVR headset, you can enable gaze controller with following steps\n\n\n1. Enable Gaze Config\n\n\nAdd the following line to \ngvr.xml\n file, under \nvr-app-settings\n section\n\n\n        useGazeCursorController=\ntrue\n\n\n\n\n\n\nYour \ngvr.xml\n file should look like this\n\n\nlens\n \nname=\nN4\n \n\n    ...\n\n    \nvr-app-settings\n\n        \nuseGazeCursorController=\ntrue\n\n        \nuseSrgbFramebuffer=\nfalse\n \n\n\n    ...\n\n/lens\n\n\n\n\n\n\n2. Load Config file\n\n\nMake sure to load the \ngvr.xml\n in \nMainActivity.java\n\n\n        \nsetMain\n(\nnew\n \nMainScene\n(),\n \ngvr.xml\n);\n\n\n\n\n\n\n3. Controller cursor\n\n\nDownload the controller cursor texture \nhere\n\n\n\n\nPlace the cursor file under \napp\\src\\main\\res\\raw\n\n\n4. Controller Listener\n\n\nAdd following code to \nMainScene.java\n to create a controller listener. We use \nCursorControllerListener\n to show a cursor if we find a gaze controller.\n\n\n\n\nNote\n\n\nmake sure to place these code inside \nMainScene\n class\n\n\n\n\n    \nprivate\n \nGVRContext\n \nmContext\n;\n\n    \nprivate\n \nGVRScene\n \nmMainScene\n;\n\n    \nprivate\n \nstatic\n \nfinal\n \nfloat\n \nDEPTH\n \n=\n \n-\n1.5f\n;\n\n\n    \n//Listener for add/removal of a controller\n\n    \nprivate\n \nCursorControllerListener\n \nlistener\n \n=\n \nnew\n \nCursorControllerListener\n()\n \n{\n\n\n        \nprivate\n \nGVRSceneObject\n \ncursor\n;\n\n\n        \n@Override\n\n        \npublic\n \nvoid\n \nonCursorControllerAdded\n(\nGVRCursorController\n \ncontroller\n)\n \n{\n\n\n            \n// Gaze Controller\n\n            \nif\n \n(\ncontroller\n.\ngetControllerType\n()\n \n==\n \nGVRControllerType\n.\nGAZE\n)\n \n{\n\n\n                \n//Add controller cursor\n\n                \nGVRTexture\n \ncursor_texture\n \n=\n \nmContext\n.\ngetAssetLoader\n().\nloadTexture\n(\nnew\n \nGVRAndroidResource\n(\nmContext\n,\n \nR\n.\nraw\n.\ncursor\n));\n\n                \ncursor\n \n=\n \nnew\n \nGVRSceneObject\n(\nmContext\n,\n \nmContext\n.\ncreateQuad\n(\n0.1f\n,\n \n0.1f\n),\n \ncursor_texture\n);\n\n                \ncursor\n.\ngetTransform\n().\nsetPosition\n(\n0.0f\n,\n \n0.0f\n,\n \nDEPTH\n);\n\n                \nmMainScene\n.\ngetMainCameraRig\n().\naddChildObject\n(\ncursor\n);\n\n                \ncursor\n.\ngetRenderData\n().\nsetDepthTest\n(\nfalse\n);\n\n                \ncursor\n.\ngetRenderData\n().\nsetRenderingOrder\n(\n100000\n);\n\n\n                \n//Set controller position\n\n                \ncontroller\n.\nsetPosition\n(\n0.0f\n,\n \n0.0f\n,\n \nDEPTH\n);\n\n                \ncontroller\n.\nsetNearDepth\n(\nDEPTH\n);\n\n                \ncontroller\n.\nsetFarDepth\n(\nDEPTH\n);\n\n            \n}\n \nelse\n \n{\n\n                \n// disable all other types\n\n                \ncontroller\n.\nsetEnable\n(\nfalse\n);\n\n            \n}\n\n        \n}\n\n\n        \n@Override\n\n        \npublic\n \nvoid\n \nonCursorControllerRemoved\n(\nGVRCursorController\n \ncontroller\n)\n \n{\n\n            \nif\n \n(\ncontroller\n.\ngetControllerType\n()\n \n==\n \nGVRControllerType\n.\nGAZE\n)\n \n{\n\n                \nif\n \n(\ncursor\n \n!=\n \nnull\n)\n \n{\n\n                    \nmMainScene\n.\ngetMainCameraRig\n().\nremoveChildObject\n(\ncursor\n);\n\n                \n}\n\n                \ncontroller\n.\nsetEnable\n(\nfalse\n);\n\n            \n}\n\n        \n}\n\n    \n};\n\n\n\n\n\n\nAdd following code to the \nonInit\n function to initialize Gaze controller.\n\n\n        \nmContext\n \n=\n \ngvrContext\n;\n\n        \nmMainScene\n \n=\n \ngvrContext\n.\ngetMainScene\n();\n\n\n        \n//List controllers\n\n        \nGVRInputManager\n \ninput\n \n=\n \ngvrContext\n.\ngetInputManager\n();\n\n        \ninput\n.\naddCursorControllerListener\n(\nlistener\n);\n\n\n        \nfor\n \n(\nGVRCursorController\n \ncursor\n \n:\n \ninput\n.\ngetCursorControllers\n())\n \n{\n\n            \nlistener\n.\nonCursorControllerAdded\n(\ncursor\n);\n\n        \n}\n\n\n\n\n\n\n5. Build\n\n\nBuild and run the project, you should be able to see a cursor on the center of the screen\n\n\nSource Code\n\n\nComplete \nSource Code\n for this sample", 
            "title": "Play With Controller"
        }, 
        {
            "location": "/tutorials/play_with_controller/#overview", 
            "text": "Now that you've learnt how to use 3D model and animation with GearVR Framework. We are going to learn how to use controllers to make VR app more interactive.", 
            "title": "Overview"
        }, 
        {
            "location": "/tutorials/play_with_controller/#create-project", 
            "text": "Create a GearVR Framework project by copying the  template project    Perform the following steps to make sure your project runs correctly   (if developing for Gear VR) Copy your  Oculus signature file  to  app/src/main/assets  folder.  Change the  applicationId  in  build.gradle  to a unique name to avoid naming conflict when you test the app later  Change the  app_name  in  res/values/strings.xml  to avoid confusion when you debug the app.", 
            "title": "Create Project"
        }, 
        {
            "location": "/tutorials/play_with_controller/#intro", 
            "text": "Being able to interact with in the VR environment helps a lot with the immersion. Currently, there are two ways to interact with VR content   GearVR controller  Gaze controller", 
            "title": "Intro"
        }, 
        {
            "location": "/tutorials/play_with_controller/#gear-vr-controller", 
            "text": "Gear VR controller provides 3 degrees of freedom orientation tracking and trackpad control, it's highly recommended for a more immersive VR experience", 
            "title": "Gear VR Controller"
        }, 
        {
            "location": "/tutorials/play_with_controller/#1-enable-controller", 
            "text": "Add the following code in  init  class to enable the controller           GVRInputManager   input   =   gvrContext . getInputManager (); \n         input . addCursorControllerListener ( listener ); \n\n         //Add controller if detected any \n         for   ( GVRCursorController   cursor   :   input . getCursorControllers ())   { \n             listener . onCursorControllerAdded ( cursor ); \n         }", 
            "title": "1. Enable Controller"
        }, 
        {
            "location": "/tutorials/play_with_controller/#2-create-the-controller-listener", 
            "text": "By define the  CursorControllerListener , you can specify what happens when controller gets connect/disconnected or when user presses a button.  Notice we also created a  GVRGearControllerSceneObject , it is a SceneObject that acts exactly like a GearVR controller in VR. It's highly recommended for a better user experience.       //Listener for controller event \n     private   static   GVRCursorController . ControllerEventListener   controllerEventListener   =   new \n             GVRCursorController . ControllerEventListener ()   { \n                 @Override \n                 public   void   onEvent ( GVRCursorController   gvrCursorController )   { \n                     KeyEvent   keyEvent   =   gvrCursorController . getKeyEvent (); \n                     if ( keyEvent   !=   null ){ \n                         //TODO: add logic to handle controller key press here \n                     } \n                 } \n             }; \n\n     //Listener for add/removal of a controller \n     private   static   CursorControllerListener   listener   =   new   CursorControllerListener ()   { \n\n         @Override \n         public   void   onCursorControllerAdded ( GVRCursorController   controller )   { \n\n             //Setup GearVR Controller \n             if   ( controller . getControllerType ()   ==   GVRControllerType . CONTROLLER )   { \n                 //Create cursor \n                 GVRSceneObject   cursor   =   createQuad ( 1f ,   1f ,   R . raw . cursor ); \n                 cursor . getRenderData (). setDepthTest ( false ); \n                 cursor . getRenderData (). setRenderingOrder ( 100000 ); \n\n                 //Create GearController \n                 GVRGearControllerSceneObject   ctrObj   =   new   GVRGearControllerSceneObject ( s_Context ); \n                 ctrObj . setCursorController ( controller ); \n                 ctrObj . setCursor ( cursor ); \n\n                 //Setup picking \n                 ctrObj . setRayDepth ( 8.0f ); \n\n                 controller . addControllerEventListener ( controllerEventListener ); \n\n             } else { \n                 controller . setEnable ( false ); \n             } \n         } \n\n         @Override \n         public   void   onCursorControllerRemoved ( GVRCursorController   controller )   { \n             if   ( controller . getControllerType ()   ==   GVRControllerType . CONTROLLER )   { \n                 controller . removeControllerEventListener ( controllerEventListener ); \n                 controller . resetSceneObject (); \n             } \n         } \n     };", 
            "title": "2. Create the controller listener"
        }, 
        {
            "location": "/tutorials/play_with_controller/#3-build", 
            "text": "Build and run the project, you should be able to see a Gear VR controller in VR that mirrors your move", 
            "title": "3. Build"
        }, 
        {
            "location": "/tutorials/play_with_controller/#gaze-controller", 
            "text": "Gaze controller is available by default from GearVR headset, you can enable gaze controller with following steps", 
            "title": "Gaze Controller"
        }, 
        {
            "location": "/tutorials/play_with_controller/#1-enable-gaze-config", 
            "text": "Add the following line to  gvr.xml  file, under  vr-app-settings  section          useGazeCursorController= true   Your  gvr.xml  file should look like this  lens   name= N4   \n    ...\n\n     vr-app-settings \n         useGazeCursorController= true \n         useSrgbFramebuffer= false   \n\n    ... /lens", 
            "title": "1. Enable Gaze Config"
        }, 
        {
            "location": "/tutorials/play_with_controller/#2-load-config-file", 
            "text": "Make sure to load the  gvr.xml  in  MainActivity.java           setMain ( new   MainScene (),   gvr.xml );", 
            "title": "2. Load Config file"
        }, 
        {
            "location": "/tutorials/play_with_controller/#3-controller-cursor", 
            "text": "Download the controller cursor texture  here   Place the cursor file under  app\\src\\main\\res\\raw", 
            "title": "3. Controller cursor"
        }, 
        {
            "location": "/tutorials/play_with_controller/#4-controller-listener", 
            "text": "Add following code to  MainScene.java  to create a controller listener. We use  CursorControllerListener  to show a cursor if we find a gaze controller.   Note  make sure to place these code inside  MainScene  class        private   GVRContext   mContext ; \n     private   GVRScene   mMainScene ; \n     private   static   final   float   DEPTH   =   - 1.5f ; \n\n     //Listener for add/removal of a controller \n     private   CursorControllerListener   listener   =   new   CursorControllerListener ()   { \n\n         private   GVRSceneObject   cursor ; \n\n         @Override \n         public   void   onCursorControllerAdded ( GVRCursorController   controller )   { \n\n             // Gaze Controller \n             if   ( controller . getControllerType ()   ==   GVRControllerType . GAZE )   { \n\n                 //Add controller cursor \n                 GVRTexture   cursor_texture   =   mContext . getAssetLoader (). loadTexture ( new   GVRAndroidResource ( mContext ,   R . raw . cursor )); \n                 cursor   =   new   GVRSceneObject ( mContext ,   mContext . createQuad ( 0.1f ,   0.1f ),   cursor_texture ); \n                 cursor . getTransform (). setPosition ( 0.0f ,   0.0f ,   DEPTH ); \n                 mMainScene . getMainCameraRig (). addChildObject ( cursor ); \n                 cursor . getRenderData (). setDepthTest ( false ); \n                 cursor . getRenderData (). setRenderingOrder ( 100000 ); \n\n                 //Set controller position \n                 controller . setPosition ( 0.0f ,   0.0f ,   DEPTH ); \n                 controller . setNearDepth ( DEPTH ); \n                 controller . setFarDepth ( DEPTH ); \n             }   else   { \n                 // disable all other types \n                 controller . setEnable ( false ); \n             } \n         } \n\n         @Override \n         public   void   onCursorControllerRemoved ( GVRCursorController   controller )   { \n             if   ( controller . getControllerType ()   ==   GVRControllerType . GAZE )   { \n                 if   ( cursor   !=   null )   { \n                     mMainScene . getMainCameraRig (). removeChildObject ( cursor ); \n                 } \n                 controller . setEnable ( false ); \n             } \n         } \n     };   Add following code to the  onInit  function to initialize Gaze controller.           mContext   =   gvrContext ; \n         mMainScene   =   gvrContext . getMainScene (); \n\n         //List controllers \n         GVRInputManager   input   =   gvrContext . getInputManager (); \n         input . addCursorControllerListener ( listener ); \n\n         for   ( GVRCursorController   cursor   :   input . getCursorControllers ())   { \n             listener . onCursorControllerAdded ( cursor ); \n         }", 
            "title": "4. Controller Listener"
        }, 
        {
            "location": "/tutorials/play_with_controller/#5-build", 
            "text": "Build and run the project, you should be able to see a cursor on the center of the screen", 
            "title": "5. Build"
        }, 
        {
            "location": "/tutorials/play_with_controller/#source-code", 
            "text": "Complete  Source Code  for this sample", 
            "title": "Source Code"
        }, 
        {
            "location": "/tutorials/play_with_vr_video/", 
            "text": "Overview\n\n\nNow that you've learned how to use controllers with GearVR Framework. We are going to learn how to play VR video in VR\n\n\nCreate Project\n\n\nCreate a GearVR Framework project by copying the \ntemplate project\n \n\n\nPerform the following steps to make sure your project runs correctly\n\n\n\n\n(if developing for Gear VR) Copy your \nOculus signature file\n to \napp/src/main/assets\n folder.\n\n\nChange the \napplicationId\n in \nbuild.gradle\n to a unique name to avoid naming conflict when you test the app later\n\n\nChange the \napp_name\n in \nres/values/strings.xml\n to avoid confusion when you debug the app.\n\n\n\n\nIntro\n\n\nVR video is the most popular content in VR.\n\n\nHowever implementing VR videos is not that straight forward, since currently there are more than \n20 different types\n of VR video types each with different pros and cons.\n\n\nHere we're going to learn how to play the most common type: \nMonoscopic 360 video\n\n\nPrepare VR video file\n\n\nYou can download a monoscopic 360 video \nhere\n.\n\n\nAnd copy it to \napp\\src\\main\\assets\n folder\n\n\n\n\nNote\n\n\nFeel free to use any monoscopic 360 video\n\n\n\n\nCreate Video Player\n\n\nCreate a new class named \nGVRVideoPlayerObject\n and replace the default generated code with the following code.\n\n\nThis \nGVRVideoPlayerObject\n class will create a sphere and apply the video to the sphere as material.\n\n\npackage\n \ncom.example.org.gvrfapplication\n;\n\n\n\nimport\n \nandroid.content.res.AssetFileDescriptor\n;\n\n\nimport\n \nandroid.media.MediaPlayer\n;\n\n\n\nimport\n \norg.gearvrf.GVRContext\n;\n\n\nimport\n \norg.gearvrf.GVRMesh\n;\n\n\nimport\n \norg.gearvrf.GVRSceneObject\n;\n\n\nimport\n \norg.gearvrf.scene_objects.GVRSphereSceneObject\n;\n\n\nimport\n \norg.gearvrf.scene_objects.GVRVideoSceneObject\n;\n\n\nimport\n \norg.gearvrf.scene_objects.GVRVideoSceneObjectPlayer\n;\n\n\n\nimport\n \njava.io.IOException\n;\n\n\n\npublic\n \nclass\n \nGVRVideoPlayerObject\n \nextends\n \nGVRSceneObject\n{\n\n\n    \nprivate\n \nfinal\n \nGVRVideoSceneObjectPlayer\n?\n \nmPlayer\n;\n\n    \nprivate\n \nfinal\n \nMediaPlayer\n \nmMediaPlayer\n;\n\n\n    \npublic\n \nGVRVideoPlayerObject\n(\nGVRContext\n \ngvrContext\n)\n \n{\n\n        \nsuper\n(\ngvrContext\n);\n\n\n        \nGVRSphereSceneObject\n \nsphere\n \n=\n \nnew\n \nGVRSphereSceneObject\n(\ngvrContext\n,\n \n72\n,\n \n144\n,\n \nfalse\n);\n\n        \nGVRMesh\n \nmesh\n \n=\n \nsphere\n.\ngetRenderData\n().\ngetMesh\n();\n\n\n        \nmMediaPlayer\n \n=\n \nnew\n \nMediaPlayer\n();\n\n        \nmPlayer\n \n=\n \nGVRVideoSceneObject\n.\nmakePlayerInstance\n(\nmMediaPlayer\n);\n\n\n        \nGVRVideoSceneObject\n \nvideo\n \n=\n \nnew\n \nGVRVideoSceneObject\n(\ngvrContext\n,\n \nmesh\n,\n \nmPlayer\n,\n \nGVRVideoSceneObject\n.\nGVRVideoType\n.\nMONO\n);\n\n        \nvideo\n.\ngetTransform\n().\nsetScale\n(\n100\nf\n,\n \n100\nf\n,\n \n100\nf\n);\n\n\n        \naddChildObject\n(\nvideo\n);\n\n    \n}\n\n\n    \npublic\n \nvoid\n \nloadVideo\n(\nString\n \nfileName\n)\n \n{\n\n        \nfinal\n \nAssetFileDescriptor\n \nafd\n;\n\n        \ntry\n \n{\n\n            \nafd\n \n=\n \nthis\n.\ngetGVRContext\n().\ngetContext\n().\ngetAssets\n().\nopenFd\n(\nfileName\n);\n\n            \nmMediaPlayer\n.\nsetDataSource\n(\nafd\n.\ngetFileDescriptor\n(),\n \nafd\n.\ngetStartOffset\n(),\n \nafd\n.\ngetLength\n());\n\n            \nafd\n.\nclose\n();\n\n            \nmMediaPlayer\n.\nprepare\n();\n\n\n        \n}\n \ncatch\n \n(\nIOException\n \ne\n)\n \n{\n\n            \ne\n.\nprintStackTrace\n();\n\n        \n}\n\n    \n}\n\n\n    \npublic\n \nvoid\n \nplay\n()\n \n{\n\n        \nif\n(\nmMediaPlayer\n \n!=\n \nnull\n)\n \n{\n\n            \nmMediaPlayer\n.\nstart\n();\n\n        \n}\n\n    \n}\n\n\n    \npublic\n \nvoid\n \nsetLooping\n(\nboolean\n \nvalue\n)\n \n{\n\n        \nmMediaPlayer\n.\nsetLooping\n(\nvalue\n);\n\n    \n}\n\n\n    \npublic\n \nvoid\n \nonPause\n()\n \n{\n\n        \nmMediaPlayer\n.\npause\n();\n\n    \n}\n\n\n    \npublic\n \nvoid\n \nonResume\n()\n \n{\n\n        \nmMediaPlayer\n.\nstart\n();\n\n    \n}\n\n\n}\n\n\n\n\n\n\nUse video player\n\n\nAdd the following code to the \nonInit\n function of \nMainScene.java\n to load and play the video\n\n\nCreate \nmPlayerObject\n\n\n    \nprivate\n \nGVRVideoPlayerObject\n \nmPlayerObj\n \n=\n \nnull\n;\n\n\n\n\n\n\nLoad and play VR video\n\n\n        \nmPlayerObj\n \n=\n \nnew\n \nGVRVideoPlayerObject\n(\ngvrContext\n);\n\n        \nmPlayerObj\n.\nloadVideo\n(\nvideos_s_3.mp4\n);\n\n        \nmPlayerObj\n.\nsetLooping\n(\ntrue\n);\n\n        \nmPlayerObj\n.\nplay\n();\n\n\n        \ngvrContext\n.\ngetMainScene\n().\naddSceneObject\n(\nmPlayerObj\n);\n\n\n\n\n\n\nAdd \nonResume\n and \nonPause\n functions to the \nMainScene\n class\n\n\n    \npublic\n \nvoid\n \nonResume\n()\n \n{\n\n        \nif\n(\nmPlayerObj\n \n!=\n \nnull\n)\n\n            \nmPlayerObj\n.\nonResume\n();\n\n    \n}\n\n\n    \npublic\n \nvoid\n \nonPause\n()\n \n{\n\n        \nif\n \n(\nmPlayerObj\n \n!=\n \nnull\n)\n\n            \nmPlayerObj\n.\nonPause\n();\n\n    \n}\n\n\n\n\n\n\nOverride \nonResume\n and \nonPause\n functions in the \nMainActivity\n\n\n    \n@Override\n\n    \nprotected\n \nvoid\n \nonResume\n()\n \n{\n\n        \nsuper\n.\nonResume\n();\n\n        \nmain\n.\nonResume\n();\n\n    \n}\n\n\n    \n@Override\n\n    \nprotected\n \nvoid\n \nonPause\n()\n \n{\n\n        \nsuper\n.\nonPause\n();\n\n        \nmain\n.\nonPause\n();\n\n    \n}\n\n\n\n\n\n\nBuild and Run\n\n\nBuild and run the VR app, you should be able to watch the VR video on your device.\n\n\nSource Code\n\n\nComplete \nSource Code\n for this sample", 
            "title": "Play With VR Video"
        }, 
        {
            "location": "/tutorials/play_with_vr_video/#overview", 
            "text": "Now that you've learned how to use controllers with GearVR Framework. We are going to learn how to play VR video in VR", 
            "title": "Overview"
        }, 
        {
            "location": "/tutorials/play_with_vr_video/#create-project", 
            "text": "Create a GearVR Framework project by copying the  template project    Perform the following steps to make sure your project runs correctly   (if developing for Gear VR) Copy your  Oculus signature file  to  app/src/main/assets  folder.  Change the  applicationId  in  build.gradle  to a unique name to avoid naming conflict when you test the app later  Change the  app_name  in  res/values/strings.xml  to avoid confusion when you debug the app.", 
            "title": "Create Project"
        }, 
        {
            "location": "/tutorials/play_with_vr_video/#intro", 
            "text": "VR video is the most popular content in VR.  However implementing VR videos is not that straight forward, since currently there are more than  20 different types  of VR video types each with different pros and cons.  Here we're going to learn how to play the most common type:  Monoscopic 360 video", 
            "title": "Intro"
        }, 
        {
            "location": "/tutorials/play_with_vr_video/#prepare-vr-video-file", 
            "text": "You can download a monoscopic 360 video  here .  And copy it to  app\\src\\main\\assets  folder   Note  Feel free to use any monoscopic 360 video", 
            "title": "Prepare VR video file"
        }, 
        {
            "location": "/tutorials/play_with_vr_video/#create-video-player", 
            "text": "Create a new class named  GVRVideoPlayerObject  and replace the default generated code with the following code.  This  GVRVideoPlayerObject  class will create a sphere and apply the video to the sphere as material.  package   com.example.org.gvrfapplication ;  import   android.content.res.AssetFileDescriptor ;  import   android.media.MediaPlayer ;  import   org.gearvrf.GVRContext ;  import   org.gearvrf.GVRMesh ;  import   org.gearvrf.GVRSceneObject ;  import   org.gearvrf.scene_objects.GVRSphereSceneObject ;  import   org.gearvrf.scene_objects.GVRVideoSceneObject ;  import   org.gearvrf.scene_objects.GVRVideoSceneObjectPlayer ;  import   java.io.IOException ;  public   class   GVRVideoPlayerObject   extends   GVRSceneObject { \n\n     private   final   GVRVideoSceneObjectPlayer ?   mPlayer ; \n     private   final   MediaPlayer   mMediaPlayer ; \n\n     public   GVRVideoPlayerObject ( GVRContext   gvrContext )   { \n         super ( gvrContext ); \n\n         GVRSphereSceneObject   sphere   =   new   GVRSphereSceneObject ( gvrContext ,   72 ,   144 ,   false ); \n         GVRMesh   mesh   =   sphere . getRenderData (). getMesh (); \n\n         mMediaPlayer   =   new   MediaPlayer (); \n         mPlayer   =   GVRVideoSceneObject . makePlayerInstance ( mMediaPlayer ); \n\n         GVRVideoSceneObject   video   =   new   GVRVideoSceneObject ( gvrContext ,   mesh ,   mPlayer ,   GVRVideoSceneObject . GVRVideoType . MONO ); \n         video . getTransform (). setScale ( 100 f ,   100 f ,   100 f ); \n\n         addChildObject ( video ); \n     } \n\n     public   void   loadVideo ( String   fileName )   { \n         final   AssetFileDescriptor   afd ; \n         try   { \n             afd   =   this . getGVRContext (). getContext (). getAssets (). openFd ( fileName ); \n             mMediaPlayer . setDataSource ( afd . getFileDescriptor (),   afd . getStartOffset (),   afd . getLength ()); \n             afd . close (); \n             mMediaPlayer . prepare (); \n\n         }   catch   ( IOException   e )   { \n             e . printStackTrace (); \n         } \n     } \n\n     public   void   play ()   { \n         if ( mMediaPlayer   !=   null )   { \n             mMediaPlayer . start (); \n         } \n     } \n\n     public   void   setLooping ( boolean   value )   { \n         mMediaPlayer . setLooping ( value ); \n     } \n\n     public   void   onPause ()   { \n         mMediaPlayer . pause (); \n     } \n\n     public   void   onResume ()   { \n         mMediaPlayer . start (); \n     }  }", 
            "title": "Create Video Player"
        }, 
        {
            "location": "/tutorials/play_with_vr_video/#use-video-player", 
            "text": "Add the following code to the  onInit  function of  MainScene.java  to load and play the video  Create  mPlayerObject       private   GVRVideoPlayerObject   mPlayerObj   =   null ;   Load and play VR video           mPlayerObj   =   new   GVRVideoPlayerObject ( gvrContext ); \n         mPlayerObj . loadVideo ( videos_s_3.mp4 ); \n         mPlayerObj . setLooping ( true ); \n         mPlayerObj . play (); \n\n         gvrContext . getMainScene (). addSceneObject ( mPlayerObj );   Add  onResume  and  onPause  functions to the  MainScene  class       public   void   onResume ()   { \n         if ( mPlayerObj   !=   null ) \n             mPlayerObj . onResume (); \n     } \n\n     public   void   onPause ()   { \n         if   ( mPlayerObj   !=   null ) \n             mPlayerObj . onPause (); \n     }   Override  onResume  and  onPause  functions in the  MainActivity       @Override \n     protected   void   onResume ()   { \n         super . onResume (); \n         main . onResume (); \n     } \n\n     @Override \n     protected   void   onPause ()   { \n         super . onPause (); \n         main . onPause (); \n     }", 
            "title": "Use video player"
        }, 
        {
            "location": "/tutorials/play_with_vr_video/#build-and-run", 
            "text": "Build and run the VR app, you should be able to watch the VR video on your device.", 
            "title": "Build and Run"
        }, 
        {
            "location": "/tutorials/play_with_vr_video/#source-code", 
            "text": "Complete  Source Code  for this sample", 
            "title": "Source Code"
        }, 
        {
            "location": "/tutorials/play_with_billboard/", 
            "text": "Overview\n\n\nIn VR applications there are some elements that needs to always facing the user, such as text, menus.\nHere we're going to introduce an easy way to implement it using \nGVRBillboard\n component.\n\n\n\n\nNote\n\n\nBillboard\n in 3D graphics term is something always facing the camera. \n\n\n\n\nCreate Project\n\n\nCreate a GearVR Framework project by copying the \ntemplate project\n \n\n\nPerform the following steps to make sure your project runs correctly\n\n\n\n\n(if developing for Gear VR) Copy your \nOculus signature file\n to \napp/src/main/assets\n folder.\n\n\nChange the \napplicationId\n in \nbuild.gradle\n to a unique name to avoid naming conflict when you test the app later\n\n\nChange the \napp_name\n in \nres/values/strings.xml\n to avoid confusion when you debug the app.\n\n\n\n\nIntro\n\n\nBefore we start, there is a \n3D T-rex model\n and it's \ntexture\n we're going to use for this tutorial. \n\n\nHow to use Billboard\n\n\nMake sure to copy both files into \napp/src/main/assets\n folder\n\n\nYou can learn how to load 3D models with following \nplay with 3D models tutorial\n, here we'll just highlight the code for billboard\n\n\nmTrexObj\n \n=\n \ngvrContext\n.\ngetAssetLoader\n().\nloadModel\n(\ntrex_mesh.fbx\n,\n \ngvrContext\n.\ngetMainScene\n());\n\n\nmTrexObj\n.\ngetTransform\n().\nsetPosition\n(\n4\n,-\n6\n,-\n8\n);\n\n\nmTrexObj\n.\nattachComponent\n(\nnew\n \nGVRBillboard\n(\ngvrContext\n,\n \nnew\n \nVector3f\n(\n0f\n,\n1f\n,\n0f\n)));\n\n\n\n\n\n\n\n\nNote\n\n\nThe second parameter of \nGVRBillboard\n is the up vector of the billboard, it indicates which direction is up so the billboard will rotate accordingly.\n\n\n\n\nSource Code\n\n\nComplete \nSource Code\n for this sample", 
            "title": "Play With Billboard"
        }, 
        {
            "location": "/tutorials/play_with_billboard/#overview", 
            "text": "In VR applications there are some elements that needs to always facing the user, such as text, menus.\nHere we're going to introduce an easy way to implement it using  GVRBillboard  component.   Note  Billboard  in 3D graphics term is something always facing the camera.", 
            "title": "Overview"
        }, 
        {
            "location": "/tutorials/play_with_billboard/#create-project", 
            "text": "Create a GearVR Framework project by copying the  template project    Perform the following steps to make sure your project runs correctly   (if developing for Gear VR) Copy your  Oculus signature file  to  app/src/main/assets  folder.  Change the  applicationId  in  build.gradle  to a unique name to avoid naming conflict when you test the app later  Change the  app_name  in  res/values/strings.xml  to avoid confusion when you debug the app.", 
            "title": "Create Project"
        }, 
        {
            "location": "/tutorials/play_with_billboard/#intro", 
            "text": "Before we start, there is a  3D T-rex model  and it's  texture  we're going to use for this tutorial.", 
            "title": "Intro"
        }, 
        {
            "location": "/tutorials/play_with_billboard/#how-to-use-billboard", 
            "text": "Make sure to copy both files into  app/src/main/assets  folder  You can learn how to load 3D models with following  play with 3D models tutorial , here we'll just highlight the code for billboard  mTrexObj   =   gvrContext . getAssetLoader (). loadModel ( trex_mesh.fbx ,   gvrContext . getMainScene ());  mTrexObj . getTransform (). setPosition ( 4 ,- 6 ,- 8 );  mTrexObj . attachComponent ( new   GVRBillboard ( gvrContext ,   new   Vector3f ( 0f , 1f , 0f )));    Note  The second parameter of  GVRBillboard  is the up vector of the billboard, it indicates which direction is up so the billboard will rotate accordingly.", 
            "title": "How to use Billboard"
        }, 
        {
            "location": "/tutorials/play_with_billboard/#source-code", 
            "text": "Complete  Source Code  for this sample", 
            "title": "Source Code"
        }, 
        {
            "location": "/programming_guide/overview/", 
            "text": "GearVR Framework Development Overview\n\n\nIntroduction to GearVRf integration and VR app development\n\n\nGearVRf provides tools to speed up development of advanced features in high quality VR applications. Available EGL extensions (including dual scan, front buffer, MSAA, OVR multiview and tile rendering) allow the best render quality.\n\n\nGearVRf is a native code 3D rendering engine with an Android library interface. You can build non-trivial content using only built-in objects. You can add new objects (such as scene objects with or without shaders) derived from classes or by overriding some methods - GearVRf takes care of all hardware handholding. You can do just about everything in Java - all source code is published, so you can easily add to or tweak native code.\n\n\nAnatomy of GearVRf Applications\n\n\nGearVRf is a framework which controls how and when your code is executed. Subclassing GearVRf objects allows you to add your own code. You can also listen to GearVRf events and provide callbacks that respond to them.\n\n\nA 3D scene is represented as a hierarchy of GearVRf scene objects. Each visible object has a triangle mesh describing its shape, a material describing its appearance properties and a transformation matrix controlling its position in the 3D world. You do not explicitly call the OpenGL or Vulkan API when using GearVRf. Instead, the GearVRF framework manages all rendering, providing a higher level abstraction for graphics.\n\n\nWhen constructing an Android application, you subclass the Activity class. Similarly, when constructing a GearVRF application you subclass GVRActivity, providing initialization code to create a GVRMain to set up the initial 3D scene and handle input events.\n\n\nDuring initialization, GVRActivity creates a GVRViewManager which does all the heavy lifting. This class is responsible for task scheduling, 3D rendering, animation and asset loading.\n\n\n\n\nThread Management\n\n\nOne key constraint of embedded GPU programming is that there is only one GL context. That is, all GPU commands must come from the same thread - the GL thread. The GPU should always be busy; therefore, the graphics thread cannot be the main GUI thread. In the future, GearVRf will eventually relax this restriction when using the Vulkan API because that graphics interface permits multiple threads to simultaneously submit work.\n\n\nWhen starting GearVRf, your Android app creates the GL thread, puts the phone into stereoscopic mode, and supplies a pair of callback methods that run the app's startup and per-frame code on the graphics thread. GearVRf provides methods for any thread to schedule runnable callbacks to the graphics thread. All these callbacks mean that GearVRf programming is event-oriented on the graphics thread in just the same way that Android programming is event-oriented on the GUI thread. Running two independent event systems on two independent threads does mean that you have to think about IPC whenever your Android Activity code on the GUI thread interacts with the GearVRf code on the graphics thread. However, dual-thread operation also creates another huge section of your application that can take advantage of event atomicity. That is, callback events are method calls from a main loop - neither the GUI thread nor the graphics thread ever runs more than one callback at one time, and each callback has to run to completion before any other callback can start on that thread. Your graphics callbacks do not have to write code to keep other graphics callbacks from seeing data structures in a partially updated state.\n\n\nScene Graph and Scene Objects\n\n\nYour startup code builds a scene graph made up of scene objects, and your per-frame code then manipulates the scene graph in real time. Each scene object has a 4x4 matrix that describes its position, orientation, and zoom relative to its parent. Each scene object can parent other scene objects, so complex objects can be composed of multiple small objects, each with its own shape and appearance, with all changing in synchrony. Each scene object provides methods to change its components using a lazy update pattern, which means that multiple updates per event cost very little more than a single update.\n\n\nYou make a scene object visible by adding a surface geometry and a skin. The geometry is a mesh of 3D triangles. GearVRf provides methods to build simple rectangular quads, and to load more complex meshes from files built by 3D model editors.\n\n\nEach material class contains the shader type, values for all shader parameters, texture, and other uniform mappings. Each shader has two parts: a vertex shader and a fragment shader. The vertex shader is called for each vertex of each visible triangle and can compute triangle-specific values that are passed to the fragment shader, which draws each pixel of each visible triangle. GearVRf contains standard shaders that provide methods, such as simply sampling a texture (a bitmap image in GPU memory), without applying any lighting effects. You can create custom shaders by supplying vertex and fragment shaders and by declaring names to bind Java values to. The GLSL shader language is very simple and C-like; you can learn a lot by reading a few of the shaders in the sample applications. GearVRf also supports sharing specially constructed shaders between OpenGL and Vulkan.\n\n\nScene Graph\n\n\nThe scene graph describes the spatial relationship between objects in the scene. Each scene object has a 4x4 transformation matrix to position and orient it locally. The scene objects may be nested so that the transformations of the parent nodes are inherited by the children. This allows objects to be easily positioned and animated relative to one another.\n\n\nHere we see a scene graph for a butterfly with a body and two wings. Each scene object has a position and an orientation. The left and right wings can share the same mesh but it is positioned and oriented differently for each wing. The initial translation on the body is inherited by the wings. \n\n\n\n\nThe form of your scene graph can have implications for the performance of your application. Typically, having lots of small objects performs poorly compared to several large objects with a similar total vertex count. This is because there is a considerable amount of overhead in rendering a single object. GearVRf attempts to batch objects together that do not move in order to improve performance.\n\n\nTypes of Scene Objects\n\n\nYou can have invisible scene objects. These have a location and a set of child objects. This can be useful to move a set of scene objects as a unit preserving their relative geometry.\n\n\nVisible scene objects have a render data component attached which contains the geometry defining the shape of the object and a \nmaterial\n describing its appearance. The material contains the data that will be passed to the shader used by the GPU to draw the mesh.\n\n\nIn addition to displaying geometry, a scene object can display text, 360 photos, 360 video, normal photos and video, Android application view and internet browser views.\n\n\n\n\n\n\n\n\nScene Object Class\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGVRSphereSceneObject\n\n\nconstructs sphere geometry\n\n\n\n\n\n\nGVRCubeSceneObject\n\n\nconstruct cube geometry\n\n\n\n\n\n\nGVRConeSceneObject\n\n\nconstructs cone geometry\n\n\n\n\n\n\nGVRCylinderSceneObject\n\n\nconstructs cylinder geometry\n\n\n\n\n\n\nGVRTextViewSceneObject\n\n\ndisplays text\n\n\n\n\n\n\nGVRVideoSceneObject\n\n\ndisplays a video\n\n\n\n\n\n\nGVRWebViewSceneObject\n\n\ndisplays an internet browser window\n\n\n\n\n\n\nGVRCameraSceneObject\n\n\ndisplays video from the phone camera\n\n\n\n\n\n\nGVRKeyboardSceneObject\n\n\ndisplays 3D interactive keyboard\n\n\n\n\n\n\n\n\nScene Construction Example\n\n\nConstructing the initial GearVRF scene usually involves importing a set of assets and placing them relative to one another. In this example we make a simple butterfly with an ellipsoid for a body and textured planes for wings.\n\n\n    \nGVRContext\n \ncontext\n;\n\n    \nGVRTexture\n \nwingtex\n \n=\n \ncontext\n.\ngetAssetLoader\n().\nloadTexture\n(\n\n        \nnew\n \nGVRAndroidResource\n(\ncontext\n,\n \nR\n.\ndrawable\n.\nwingtex\n));\n\n    \nGVRSceneObject\n \nbody\n \n=\n \nnew\n \nGVRSphereObject\n(\ncontext\n);\n\n    \nGVRSceneObject\n \nleftwing\n \n=\n \nnew\n \nGVRSceneObject\n(\ncontext\n,\n \nwingtex\n);\n\n    \nGVRSceneObject\n \nrightwing\n \n=\n \nnew\n \nGVRSceneObject\n(\ncontext\n,\n \nwingtex\n);\n\n    \nleftwing\n.\ngetTransform\n().\nsetPosition\n(-\n1\n,\n \n0\n,\n \n0\n);\n\n    \nrightwing\n.\ngetTransform\n().\nsetPosition\n(\n1\n,\n \n0\n,\n \n0\n);\n\n    \nrightwing\n.\ngetTransform\n().\nsetRotationY\n(\n180\n);\n\n\n\n\n\n\nScene Object Components\n\n\nA scene object can have one or more components attached which provide additional capabilities. All scene objects have a GVRTransform component which supplies the 4x4 matrix used to position and orient the object in the scene. Attaching a GVRRenderData component referencing geometry and material properties will cause the geometry to be displayed in the scene.\n\n\nThe following components can be attached to a GVRSceneObject:\n\n\n\n\nGVRTransform - 4x4 transformation matrix\n\n\nGVRRenderData - geometry with material properties\n\n\nGVRLightBase - illumination source\n\n\nGVRCamera - camera\n\n\nGVRCameraRig - stereoscopic camera rig\n\n\nGVRCollider - collision geometry\n\n\nGVRBehavior - user defined component\n\n\nGVRPicker - pick from this viewpoint\n\n\nGVRBaseSensor - marshal pick events from children\n\n\nGVRRenderTarget - render to texture\n\n\n\n\nEach scene object can only have one component of a particular type. For example, you cannot attach two lights or two cameras to a single scene object. Components are retrieved and removed based on their type. When a component is attached to a scene object, it derives its position and orientation from the GVRTransform attached to that scene object.\n\n\n\n\n\n\n\n\nGVRSceneObject  function\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGVRComponent getComponent(long type)\n\n\nGet the component of the specified class attached to the owner scene object.\n\n\n\n\n\n\nvoid attachComponent(GVRComponent)\n\n\nAttach the given component to the scene object.\n\n\n\n\n\n\nvoid detachComponent(GVRComponent)\n\n\nDetach the given component from the scene object.\n\n\n\n\n\n\nvoid detachComponenet(long type)\n\n\nDetach the component of the specified type from the scene object.\n\n\n\n\n\n\nList getAllComponents(long type)\n\n\nGet all components of the given type from the scene object and its children.\n\n\n\n\n\n\nvoid forAllComponents(ComponentVisitor visitor, long type)\n\n\nVisit all components of the given type from the scene object and its children.", 
            "title": "Overview"
        }, 
        {
            "location": "/programming_guide/overview/#gearvr-framework-development-overview", 
            "text": "Introduction to GearVRf integration and VR app development  GearVRf provides tools to speed up development of advanced features in high quality VR applications. Available EGL extensions (including dual scan, front buffer, MSAA, OVR multiview and tile rendering) allow the best render quality.  GearVRf is a native code 3D rendering engine with an Android library interface. You can build non-trivial content using only built-in objects. You can add new objects (such as scene objects with or without shaders) derived from classes or by overriding some methods - GearVRf takes care of all hardware handholding. You can do just about everything in Java - all source code is published, so you can easily add to or tweak native code.", 
            "title": "GearVR Framework Development Overview"
        }, 
        {
            "location": "/programming_guide/overview/#anatomy-of-gearvrf-applications", 
            "text": "GearVRf is a framework which controls how and when your code is executed. Subclassing GearVRf objects allows you to add your own code. You can also listen to GearVRf events and provide callbacks that respond to them.  A 3D scene is represented as a hierarchy of GearVRf scene objects. Each visible object has a triangle mesh describing its shape, a material describing its appearance properties and a transformation matrix controlling its position in the 3D world. You do not explicitly call the OpenGL or Vulkan API when using GearVRf. Instead, the GearVRF framework manages all rendering, providing a higher level abstraction for graphics.  When constructing an Android application, you subclass the Activity class. Similarly, when constructing a GearVRF application you subclass GVRActivity, providing initialization code to create a GVRMain to set up the initial 3D scene and handle input events.  During initialization, GVRActivity creates a GVRViewManager which does all the heavy lifting. This class is responsible for task scheduling, 3D rendering, animation and asset loading.", 
            "title": "Anatomy of GearVRf Applications"
        }, 
        {
            "location": "/programming_guide/overview/#thread-management", 
            "text": "One key constraint of embedded GPU programming is that there is only one GL context. That is, all GPU commands must come from the same thread - the GL thread. The GPU should always be busy; therefore, the graphics thread cannot be the main GUI thread. In the future, GearVRf will eventually relax this restriction when using the Vulkan API because that graphics interface permits multiple threads to simultaneously submit work.  When starting GearVRf, your Android app creates the GL thread, puts the phone into stereoscopic mode, and supplies a pair of callback methods that run the app's startup and per-frame code on the graphics thread. GearVRf provides methods for any thread to schedule runnable callbacks to the graphics thread. All these callbacks mean that GearVRf programming is event-oriented on the graphics thread in just the same way that Android programming is event-oriented on the GUI thread. Running two independent event systems on two independent threads does mean that you have to think about IPC whenever your Android Activity code on the GUI thread interacts with the GearVRf code on the graphics thread. However, dual-thread operation also creates another huge section of your application that can take advantage of event atomicity. That is, callback events are method calls from a main loop - neither the GUI thread nor the graphics thread ever runs more than one callback at one time, and each callback has to run to completion before any other callback can start on that thread. Your graphics callbacks do not have to write code to keep other graphics callbacks from seeing data structures in a partially updated state.", 
            "title": "Thread Management"
        }, 
        {
            "location": "/programming_guide/overview/#scene-graph-and-scene-objects", 
            "text": "Your startup code builds a scene graph made up of scene objects, and your per-frame code then manipulates the scene graph in real time. Each scene object has a 4x4 matrix that describes its position, orientation, and zoom relative to its parent. Each scene object can parent other scene objects, so complex objects can be composed of multiple small objects, each with its own shape and appearance, with all changing in synchrony. Each scene object provides methods to change its components using a lazy update pattern, which means that multiple updates per event cost very little more than a single update.  You make a scene object visible by adding a surface geometry and a skin. The geometry is a mesh of 3D triangles. GearVRf provides methods to build simple rectangular quads, and to load more complex meshes from files built by 3D model editors.  Each material class contains the shader type, values for all shader parameters, texture, and other uniform mappings. Each shader has two parts: a vertex shader and a fragment shader. The vertex shader is called for each vertex of each visible triangle and can compute triangle-specific values that are passed to the fragment shader, which draws each pixel of each visible triangle. GearVRf contains standard shaders that provide methods, such as simply sampling a texture (a bitmap image in GPU memory), without applying any lighting effects. You can create custom shaders by supplying vertex and fragment shaders and by declaring names to bind Java values to. The GLSL shader language is very simple and C-like; you can learn a lot by reading a few of the shaders in the sample applications. GearVRf also supports sharing specially constructed shaders between OpenGL and Vulkan.", 
            "title": "Scene Graph and Scene Objects"
        }, 
        {
            "location": "/programming_guide/overview/#scene-graph", 
            "text": "The scene graph describes the spatial relationship between objects in the scene. Each scene object has a 4x4 transformation matrix to position and orient it locally. The scene objects may be nested so that the transformations of the parent nodes are inherited by the children. This allows objects to be easily positioned and animated relative to one another.  Here we see a scene graph for a butterfly with a body and two wings. Each scene object has a position and an orientation. The left and right wings can share the same mesh but it is positioned and oriented differently for each wing. The initial translation on the body is inherited by the wings.    The form of your scene graph can have implications for the performance of your application. Typically, having lots of small objects performs poorly compared to several large objects with a similar total vertex count. This is because there is a considerable amount of overhead in rendering a single object. GearVRf attempts to batch objects together that do not move in order to improve performance.", 
            "title": "Scene Graph"
        }, 
        {
            "location": "/programming_guide/overview/#types-of-scene-objects", 
            "text": "You can have invisible scene objects. These have a location and a set of child objects. This can be useful to move a set of scene objects as a unit preserving their relative geometry.  Visible scene objects have a render data component attached which contains the geometry defining the shape of the object and a  material  describing its appearance. The material contains the data that will be passed to the shader used by the GPU to draw the mesh.  In addition to displaying geometry, a scene object can display text, 360 photos, 360 video, normal photos and video, Android application view and internet browser views.     Scene Object Class  Description      GVRSphereSceneObject  constructs sphere geometry    GVRCubeSceneObject  construct cube geometry    GVRConeSceneObject  constructs cone geometry    GVRCylinderSceneObject  constructs cylinder geometry    GVRTextViewSceneObject  displays text    GVRVideoSceneObject  displays a video    GVRWebViewSceneObject  displays an internet browser window    GVRCameraSceneObject  displays video from the phone camera    GVRKeyboardSceneObject  displays 3D interactive keyboard", 
            "title": "Types of Scene Objects"
        }, 
        {
            "location": "/programming_guide/overview/#scene-construction-example", 
            "text": "Constructing the initial GearVRF scene usually involves importing a set of assets and placing them relative to one another. In this example we make a simple butterfly with an ellipsoid for a body and textured planes for wings.       GVRContext   context ; \n     GVRTexture   wingtex   =   context . getAssetLoader (). loadTexture ( \n         new   GVRAndroidResource ( context ,   R . drawable . wingtex )); \n     GVRSceneObject   body   =   new   GVRSphereObject ( context ); \n     GVRSceneObject   leftwing   =   new   GVRSceneObject ( context ,   wingtex ); \n     GVRSceneObject   rightwing   =   new   GVRSceneObject ( context ,   wingtex ); \n     leftwing . getTransform (). setPosition (- 1 ,   0 ,   0 ); \n     rightwing . getTransform (). setPosition ( 1 ,   0 ,   0 ); \n     rightwing . getTransform (). setRotationY ( 180 );", 
            "title": "Scene Construction Example"
        }, 
        {
            "location": "/programming_guide/overview/#scene-object-components", 
            "text": "A scene object can have one or more components attached which provide additional capabilities. All scene objects have a GVRTransform component which supplies the 4x4 matrix used to position and orient the object in the scene. Attaching a GVRRenderData component referencing geometry and material properties will cause the geometry to be displayed in the scene.  The following components can be attached to a GVRSceneObject:   GVRTransform - 4x4 transformation matrix  GVRRenderData - geometry with material properties  GVRLightBase - illumination source  GVRCamera - camera  GVRCameraRig - stereoscopic camera rig  GVRCollider - collision geometry  GVRBehavior - user defined component  GVRPicker - pick from this viewpoint  GVRBaseSensor - marshal pick events from children  GVRRenderTarget - render to texture   Each scene object can only have one component of a particular type. For example, you cannot attach two lights or two cameras to a single scene object. Components are retrieved and removed based on their type. When a component is attached to a scene object, it derives its position and orientation from the GVRTransform attached to that scene object.     GVRSceneObject  function  Description      GVRComponent getComponent(long type)  Get the component of the specified class attached to the owner scene object.    void attachComponent(GVRComponent)  Attach the given component to the scene object.    void detachComponent(GVRComponent)  Detach the given component from the scene object.    void detachComponenet(long type)  Detach the component of the specified type from the scene object.    List getAllComponents(long type)  Get all components of the given type from the scene object and its children.    void forAllComponents(ComponentVisitor visitor, long type)  Visit all components of the given type from the scene object and its children.", 
            "title": "Scene Object Components"
        }, 
        {
            "location": "/programming_guide/features/scene_graph/", 
            "text": "Setting Up Cameras\n\n\nGearVRf will create the camera rig by default. Its parameters do not need to be adjusted; however, applications may move the camera rig.\n\n\nThe HMD sensor automatically adjusts camera orientation; your app does not need code for this. Camera background color can be set for each eye; however, they are typically the same. Camera background color and post-effect data can be set by the application. Post-effects are applied to each camera.\n\n\nTo set the background of the cameras and the position of the camera rig:\n\n\n    \n// set camera background color\n\n    \nGVRCameraRig\n \ncameraRig\n \n=\n \nmGVRContext\n.\ngetMainScene\n().\ngetMainCameraRig\n();\n\n    \ncameraRig\n.\ngetLeftCamera\n().\nsetBackgroundColor\n(\n0.0f\n,\n \n0.0f\n,\n \n0.0f\n,\n \n1.0f\n);\n\n    \ncameraRig\n.\ngetRightCamera\n().\nsetBackgroundColor\n(\n0.0f\n,\n \n0.0f\n,\n \n0.0f\n,\n \n1.0f\n);\n\n\n    \n// set up camerarig position (default)\n\n    \ncameraRig\n.\ngetHeadTransform\n().\nsetPosition\n(\n0.0f\n,\n \n0.0f\n,\n \n0.0f\n);\n\n\n\n\n\n\nScene Graph\n\n\nThe scene graph - the VR world - is a hierarchical tree of scene objects. Each scene object is a tree node with one parent and one or more child scene objects. Applications must build a scene graph. Your app needs to set up cameraRig for the root scene object of the scene graph, but does not need to set up cameraRig for each lower-level scene object. To create a scene graph at initialization time, get the GearVRf main scene (the root scene object) from GVRContext.\n\n\nTo create the scene graph by getting its root scene object:\n\n\n    \nGVRScene\n \nscene\n \n=\n \nmGVRContext\n.\ngetMainScene\n();\n\n\n\n\n\n\nCreating the Scene Objects\n\n\nPopulate your VR world's scene graph scene object tree by adding scene objects to the root scene object and to other lower-level scene objects.\n\n\nThe most common way is to load models using the GearVRf wrapped Assimp library.Assimp can load many 3D file formats and construct GearVRf scene object hierarchies from them. The materials, textures and shaders are automatically attached to the appropriate scene object and the model is added to the scene. The \nasset loader\n uses the GVRPhongShader class as the shader template for all imported objects.\n\n\nTo create a scene object from from a file:\n\n\n    \n// load mesh using assimp\n\n    \nGVRSceneObject\n \nmodel\n \n=\n \ngvrContext\n.\ngetAssetLoader\n().\nloadModel\n(\nsphere.obj\n,\n \nGVRResourceVolume\n.\nVolumeType\n.\nANDROID_ASSETS\n,\n \ngvrScene\n);\n\n\n\n\n\n\nUsually it is more efficient to let the asset loader create the meshes and textures for you. Bu you can also load only a mesh and construct the scene object, material and render data programmatically. \n\n\nTo create a scene object with shader-only material via render data:\n\n\n    \n// load mesh object\n\n    \nGVRMesh\n \nsphereMesh\n \n=\n \ngvrContext\n.\ngetAssetLoader\n().\nloadMesh\n(\nsphere.obj\n);\n\n\n    \n// get material\n\n    \nGVRMaterial\n \nsphereMaterial\n \n=\n \nnew\n \nGVRMaterial\n(\ngvrContext\n,\n \nmScreenShader\n.\ngetShaderId\n());\n\n\n    \n// create render data\n\n    \nGVRRenderData\n \nsphereRenderData\n \n=\n \nnew\n \nGVRRenderData\n(\ngvrContext\n);\n\n\n    \n// set mesh and material for render data\n\n    \nsphereRenderData\n.\nsetMesh\n(\nsphereMesh\n);\n\n    \nsphereRenderData\n.\nsetMaterial\n(\nsphereMaterial\n);\n\n\n    \n// create scene object\n\n    \nsphereObject\n \n=\n \nnew\n \nGVRSceneObject\n(\ngvrContext\n);\n\n    \nsphereObject\n.\nattachRenderData\n(\nsphereRenderData\n);\n\n\n\n\n\n\nManaging Transforms in a Scene Graph\n\n\nAfter scene objects are added to the scene graph, each scene object can be controlled by transforms.\n\n\nTo set the position of a scene object and rotate it about an axis with a pivot point:\n\n\n    \nGVRSceneObject\n \nrotator\n \n=\n \nnew\n \nGVRSceneObject\n(\nmGVRContext\n,\n \n2.0f\n,\n \n1.0f\n,\n \nrotatorTextures\n.\nget\n(\ni\n));\n\n    \nrotator\n.\ngetTransform\n().\nsetPosition\n(\n0.0f\n,\n \n0.0f\n,\n \n-\n5.0f\n);\n\n    \nfloat\n \ndegree\n \n=\n \n360.0f\n \n*\n \ni\n \n/\n \n(\nrotatorTextures\n.\nsize\n()\n \n+\n \n1\n);\n\n    \nrotator\n.\ngetTransform\n().\nrotateByAxisWithPivot\n(\ndegree\n,\n \n0.0f\n,\n \n1.0f\n,\n \n0.0f\n,\n \n0.0f\n,\n \n0.0f\n,\n \n0.0f\n);", 
            "title": "Scene Graph"
        }, 
        {
            "location": "/programming_guide/features/scene_graph/#setting-up-cameras", 
            "text": "GearVRf will create the camera rig by default. Its parameters do not need to be adjusted; however, applications may move the camera rig.  The HMD sensor automatically adjusts camera orientation; your app does not need code for this. Camera background color can be set for each eye; however, they are typically the same. Camera background color and post-effect data can be set by the application. Post-effects are applied to each camera.  To set the background of the cameras and the position of the camera rig:       // set camera background color \n     GVRCameraRig   cameraRig   =   mGVRContext . getMainScene (). getMainCameraRig (); \n     cameraRig . getLeftCamera (). setBackgroundColor ( 0.0f ,   0.0f ,   0.0f ,   1.0f ); \n     cameraRig . getRightCamera (). setBackgroundColor ( 0.0f ,   0.0f ,   0.0f ,   1.0f ); \n\n     // set up camerarig position (default) \n     cameraRig . getHeadTransform (). setPosition ( 0.0f ,   0.0f ,   0.0f );", 
            "title": "Setting Up Cameras"
        }, 
        {
            "location": "/programming_guide/features/scene_graph/#scene-graph", 
            "text": "The scene graph - the VR world - is a hierarchical tree of scene objects. Each scene object is a tree node with one parent and one or more child scene objects. Applications must build a scene graph. Your app needs to set up cameraRig for the root scene object of the scene graph, but does not need to set up cameraRig for each lower-level scene object. To create a scene graph at initialization time, get the GearVRf main scene (the root scene object) from GVRContext.  To create the scene graph by getting its root scene object:       GVRScene   scene   =   mGVRContext . getMainScene ();", 
            "title": "Scene Graph"
        }, 
        {
            "location": "/programming_guide/features/scene_graph/#creating-the-scene-objects", 
            "text": "Populate your VR world's scene graph scene object tree by adding scene objects to the root scene object and to other lower-level scene objects.  The most common way is to load models using the GearVRf wrapped Assimp library.Assimp can load many 3D file formats and construct GearVRf scene object hierarchies from them. The materials, textures and shaders are automatically attached to the appropriate scene object and the model is added to the scene. The  asset loader  uses the GVRPhongShader class as the shader template for all imported objects.  To create a scene object from from a file:       // load mesh using assimp \n     GVRSceneObject   model   =   gvrContext . getAssetLoader (). loadModel ( sphere.obj ,   GVRResourceVolume . VolumeType . ANDROID_ASSETS ,   gvrScene );   Usually it is more efficient to let the asset loader create the meshes and textures for you. Bu you can also load only a mesh and construct the scene object, material and render data programmatically.   To create a scene object with shader-only material via render data:       // load mesh object \n     GVRMesh   sphereMesh   =   gvrContext . getAssetLoader (). loadMesh ( sphere.obj ); \n\n     // get material \n     GVRMaterial   sphereMaterial   =   new   GVRMaterial ( gvrContext ,   mScreenShader . getShaderId ()); \n\n     // create render data \n     GVRRenderData   sphereRenderData   =   new   GVRRenderData ( gvrContext ); \n\n     // set mesh and material for render data \n     sphereRenderData . setMesh ( sphereMesh ); \n     sphereRenderData . setMaterial ( sphereMaterial ); \n\n     // create scene object \n     sphereObject   =   new   GVRSceneObject ( gvrContext ); \n     sphereObject . attachRenderData ( sphereRenderData );", 
            "title": "Creating the Scene Objects"
        }, 
        {
            "location": "/programming_guide/features/scene_graph/#managing-transforms-in-a-scene-graph", 
            "text": "After scene objects are added to the scene graph, each scene object can be controlled by transforms.  To set the position of a scene object and rotate it about an axis with a pivot point:       GVRSceneObject   rotator   =   new   GVRSceneObject ( mGVRContext ,   2.0f ,   1.0f ,   rotatorTextures . get ( i )); \n     rotator . getTransform (). setPosition ( 0.0f ,   0.0f ,   - 5.0f ); \n     float   degree   =   360.0f   *   i   /   ( rotatorTextures . size ()   +   1 ); \n     rotator . getTransform (). rotateByAxisWithPivot ( degree ,   0.0f ,   1.0f ,   0.0f ,   0.0f ,   0.0f ,   0.0f );", 
            "title": "Managing Transforms in a Scene Graph"
        }, 
        {
            "location": "/programming_guide/features/rendering/", 
            "text": "Opaque scene objects are drawn front-to-back, in render order. Transparent objects are drawn back-to-front. The renderer will automatically sort scene object data. The asset loader sets the rendering order based on the characteristics of the asset being imported. GearVRf detects transparent textures and will automatically change objects marked as \nGEOMETRY\n to \nTRANSPARENT\n if a transparent texture is used.\n\n\nTransparent objects typically use alpha blending of some type. To enable transparency, you must call \nGVRRenderData.setAlphaBlend\n as well as setting the rendering order. Similarly, to use the stencil buffer you usually need to call \nGVRRenderData.setStencilOp\n and \nGVRRenderData.setStencilFunc\n.\n\n\n|GVRRenderingOrder Value|Render Order|Used For|\n|-|-|\n|GVRRenderingOrder.STENCIL | Stencil | rendered to stencil buffer |\n|GVRRenderingOrder.BACKGROUND | First| skybox, background |\n|GVRRenderingOrder.GEOMETRY | Second| opaque objects |\n|GVRRenderingOrder.TRANSPARENT | Third| transparent objects |\n|GVRRenderingOrder.OVERLAY | Last| GUI and HUD overlays, cursors |\n\n\nAfter your startup code has built a scene graph, GearVRf enters its event loop. On each frame, GearVRf starts its render pipeline, which consists of four main steps. The first three steps run your Java callbacks on the GL thread. The final step is managed by GearVRf.\n\n\n\n\n\n\nGearVRf executes any Runnable you added to the run-once queue.\nQueue operations are thread-safe. You can use the GVRContext.runOnGlThread() method from the GUI or background threads in the same way you use Activity.runOnUiThread() from non-GUI threads. The analogy is not exact: runOnGlThread() always enqueues its Runnable, even when called from the GL thread.\n\n\n\n\n\n\nGearVRf executes each frame listener you added to the on-frame list.\nGearVRf includes animation and periodic engines that use frame listeners to run time-based code on the GL thread, but you may add frame listeners directly. A frame listener is like a Runnable that gets a parameter telling you how long it has been since the last frame. An animation runs every frame until it stops, and morphs a scene object from one state to another. A periodic callback runs a standard Runnable at a specified time (or times) as a runOnGlThread() callback. You can run a sequence of animations either by starting each new animation in the previous animation's optional on-finish callback or by starting each new animation at set times from a periodic callback.\n\n\n\n\n\n\nGearVRf runs your onStep() callback, which is the place to process Android or cloud events and make changes to your scene graph. As a rule of thumb, an animation changes a single scene object's properties; onStep() changes the scene graph itself. Of course, you can use onStep() to start an animation that will make a smooth change.\n\n\n\n\n\n\nGearVRf renders the scene graph twice, once for each eye.\n\n\n\n\nRendering determines what a camera can see and draws each visible triangle to a buffer in GPU memory.\n\n\nAny post-effects you have registered for an eye are applied the buffer for step 4.a in registration order. (Typically, you will use per-eye registration to run the same effect with different per-eye parameters, but you can run different effects on each eye, perhaps adding different debugging info to each eye.).\nA post-effect is a shader that is a lot like the shaders that draw scene objects' skins; the big difference is that while a material shader's vertex shader may be quite complex (adding in lighting and reflections), a post-effect is a 2D effect, and all the action is in the fragment shader, which draws each pixel. GearVRf includes pre-defined post-effect shaders, and it is easy to add your own.\n\n\nOne last shader applies barrel distortion to the render buffer, and draws the barrel distortion to the screen. When the user views the screen though a fish-eye lens, the undistorted image will (nearly) fill the field of view. This step is not programmable, except in so far as you provide an XML file with screen size information at start-up time.", 
            "title": "Rendering"
        }, 
        {
            "location": "/programming_guide/features/loading_assets/", 
            "text": "GearVRf supports loading of 3D content files both synchronously and asynchronously. Your application may issue a blocking load and wait for the asset or get a callback when the asset loading is finished. GearVRf can import .OBJ, .FBX, Collada  (.dae) and X3D file formats, as well as all file formats supported by \nAssimp\n. GearVRf can also read all commonly used bitmap file formats.\n\n\nLoading a 3D Model\n\n\nLoading models is handled by the GVRAssetLoader class which is accessible from the context by calling GVRContext.getAssetLoader(). The asset loader can load models from a variety of places. If you are providing a file name, a prefix on the name indicates the origin of the file:\n\n\n\n\n\"sd:\" designates the model on the phone SD card.\n\n\n\"http:\" or \"https:\" designates the model is on the internet.\n\n\nNo prefix meaqns the filename is relative to the \"assets\" directory.\n\n\n\n\nFor more flexibility, you can use the \nGVRAndroidResource\n class which lets you import assets from resources in your application or from an already open Android stream. Both models and textures can be loaded from GVRAndroidResource objects.\n\n\n\n\n\n\n\n\nGVRAndroidResource Constructors\n\n\nVolume Type\n\n\nInput source\n\n\n\n\n\n\n\n\n\n\nGVRAndroidResource(String path)\n\n\nResourceType.LINUX_FILESYSTEM\n\n\nSD card\n\n\n\n\n\n\nGVRAndroidResource(File file)\n\n\nResourceType.LINUX_FILESYSTEM\n\n\nSD card\n\n\n\n\n\n\nGVRAndroidResource(GVRContext, int resourceID)\n\n\nResourceType.ANDROID_RESOURCE\n\n\nres directory\n\n\n\n\n\n\nGVRAndroidResource(Context, int resourceID)\n\n\nResourceType.ANDROID_RESOURCE\n\n\nres directory\n\n\n\n\n\n\nGVRAndroidResource(GVRContext, String path)\n\n\nResourceType.ANDROID_ASSETS\n\n\nassets directory\n\n\n\n\n\n\nGVRAndroidResource(GVRContext, URL url)\n\n\nResourceType.NETWORK\n\n\ninternet\n\n\n\n\n\n\nGVRAndroidResource(GVRContext, InputStream stream)\n\n\nResourceType.INPUT_STREAM\n\n\ninput stream\n\n\n\n\n\n\n\n\nThe \nGVRAssetLoader.loadModel\n function loads a model from a device and returns as soon as the model geometry has been parsed and accumulated. This model may not have been added to the scene yet. If you pass the current GVRScene as an argument, the asset loader will wait until all of the textures in the model have been loaded and then add it to the scene. If you omit the argument, the model is not added to the scene and you will need to add it in your own code.\n\n\nModel Loading Examples\n\n\nThis example shows how to load a model from a URL and another from the application resources. The textures are loaded in the background in another thread. The model will be added to the scene when all of its textures have completed loading. The model returned may not be completely loaded but all of the geometry will be accessible. Usually assets are loaded in the onInit function of your main script.\n\n\npublic\n \nvoid\n \nonInit\n(\nGVRContext\n \ncontext\n)\n\n\n{\n\n    \nGVRScene\n \nscene\n \n=\n \ncontext\n.\ngetMainScene\n();\n\n    \ntry\n\n    \n{\n\n        \nString\n \nurl\n \n=\n \nhttps://raw.githubusercontent.com/gearvrf/GearVRf-Demos/master/gvrjassimpmodelloader/assets/trees/trees9.3ds\n;\n\n        \nGVRSceneObject\n \nmodel1\n \n=\n \ncontext\n.\ngetAssetLoader\n().\nloadModel\n(\nurl\n,\n \nscene\n);\n\n        \nmodel1\n.\ngetTransform\n().\nsetPosition\n(\n0.0f\n,\n \n-\n4.0f\n,\n \n-\n20.0f\n);\n\n\n        \nGVRAndroidResource\n \nresource\n \n=\n \nnew\n \nGVRAndroidResource\n(\ncontext\n,\n \nR\n.\nraw\n.\nspaceship\n);\n\n        \nEnumSet\nGVRImportSettings\n \nsettings\n \n=\n \nGVRImportSettings\n.\ngetRecommendedSettings\n();\n\n        \nGVRSceneObject\n \nmodel2\n \n=\n \ncontext\n.\ngetAssetLoader\n().\nloadModel\n(\nresource\n,\n \nsettings\n,\n \ntrue\n,\n \nscene\n);\n\n        \nmodel2\n.\ngetTransform\n().\nsetPositionZ\n(-\n10.0f\n);\n\n    \n}\n\n    \ncatch\n \n(\nIOException\n \ne\n)\n\n    \n{\n\n        \nLog\n.\ne\n(\nERROR\n,\n \nFailed to load model: %s\n,\n \ne\n);\n\n    \n}\n\n\n}\n\n\n\n\n\n\nImport Settings\n\n\nThe \nGVRImportSettings\n object controls how the asset is imported. Depending on what your application is going to do with the model, import settings can help optimize performance. By default, all models are imported in full fidelity - all vertex components, light sources, cameras and textures are included. If you are not using light sources, it is more efficient to omit normals from the meshes and to not import any light sources in the model. Similarly, if you do not plan to animate the model, importing without animation will be faster and will suppress bone indices and bone weights from your meshes, which will cause GearVRf to use a more efficient shader.\n\n\n\n\n\n\n\n\nSetting\n\n\nWhat it does\n\n\n\n\n\n\n\n\n\n\nNO_LIGHTING\n\n\nSuppress import of normals and light sources\n\n\n\n\n\n\nNO_ANIMATION\n\n\nSuppress import of animation (bones, bone indices, bone weights and animation clips)\n\n\n\n\n\n\nNO_TEXTURING\n\n\nSuppress import of textures and texture coordinates\n\n\n\n\n\n\nSTART_ANIMATIONS\n\n\nAutomatically start animations\n\n\n\n\n\n\nCALCULATE_TANGENTS\n\n\nCalculate tangents required for normal mapping\n\n\n\n\n\n\nJOIN_IDENTICAL_VERTICES*\n\n\nJoin vertices when possible\n\n\n\n\n\n\nTRIANGULATE*\n\n\nTriangulate all meshes (required for GearVRf)\n\n\n\n\n\n\nCALCULATE_NORMALS\n\n\nCalculate normals if not present\n\n\n\n\n\n\nCALCULATE_SMOOTH_NORMALS\n\n\nCalculate and smooth normals\n\n\n\n\n\n\nLIMIT_BONE_WEIGHT*\n\n\nLimit bone weight to 4 (required for GearVRf)\n\n\n\n\n\n\nIMPROVE_VERTEX_CACHE_LOCALITY\n\n\nreorder vertices for cache locality\n\n\n\n\n\n\nSORTBY_PRIMITIVE_TYPE*\n\n\nSplit meshes by primitive type\n\n\n\n\n\n\nOPTIMIZE_MESHES\n\n\nCombine meshes when possible\n\n\n\n\n\n\nOPTIMIZE_GRAPH\n\n\nMerge nodes without bones\n\n\n\n\n\n\nFLIP_UV*\n\n\nFlip UV coordinates\n\n\n\n\n\n\n\n\nTypically you will get the recommended settings (they are starred in the table) and add to them since a few of these settings are required for GearVRf to function properly. Only indexed triangle meshes are supported and meshes may only have four bones.\n\n\nAsynchronous Loading\n\n\nThe asset loader can also asynchronously load geometry and textures. You have the option of providing an asset event handler which will notify your application when textures and geometry are loaded and when the asset and all of its textures have finished loading. In this case, a model is placed in the scene and the asset loader adds the imported scene objects as children. You can also request the asset loader to replace the entire scene.\n\n\nThe asynchronous forms of \nloadModel\n specify the input asset as a \nGVRResourceVolume\n object. This object allows multiple assets to be loaded from a single input stream, such as a ZIP file. You can also subclass \nGVRResourceVolume\n to get complete control over the asset loading process.\n\n\nThis example waits for a model to be loaded from the assets directory and then centers it before adding it to the scene.\n\n\nGVRScene\n \nscene\n;\n\n\nGVRSceneObject\n \nroot\n \n=\n \nnew\n \nGVRSceneObject\n(\ncontext\n);\n\n\nGVRResourceVolume\n \nvolume\n \n=\n \nnew\n \nGVRResourceVolume\n(\ncontext\n,\n \nmodels/mymodel.fbx\n);\n\n\n\ncontext\n.\ngetAssetLoader\n().\nloadModel\n(\nroot\n,\n \nvolume\n,\n \nnew\n \nIAssetEvents\n()\n\n\n{\n\n    \npublic\n \nvoid\n \nonAssetLoaded\n(\nGVRContext\n \ncontext\n,\n \nGVRSceneObject\n \nmodel\n,\n \nString\n \nfilePath\n,\n \nString\n \nerrors\n);\n\n    \n{\n\n        \nBoundingVolume\n \nbv\n \n=\n \nmodel\n.\ngetBoundingVolume\n();\n\n        \nVector3f\n \nc\n \n=\n \nbv\n.\ncenter\n();\n\n        \nmodel\n.\ngetTransform\n().\nsetPosition\n(-\nc\n.\nx\n,\n \n-\nc\n.\ny\n,\n \n-\nc\n.\nz\n \n-\n \n1.0f\n);\n\n    \n}\n\n    \npublic\n \nvoid\n \nonModelLoaded\n(\nGVRContext\n \ncontext\n,\n \nGVRSceneObject\n \nmodel\n,\n \nString\n \nfilePath\n)\n \n{\n \n}\n\n    \npublic\n \nvoid\n \nonTextureLoaded\n(\nGVRContext\n \ncontext\n,\n \nGVRTexture\n \ntexture\n,\n \nString\n \nfilePath\n)\n \n{\n \n}\n\n    \npublic\n \nvoid\n \nonModelError\n(\nGVRContext\n \ncontext\n,\n \nString\n \nerror\n,\n \nString\n \nfilePath\n)\n \n{\n \n}\n\n    \npublic\n \nvoid\n \nonTextureError\n(\nGVRContext\n \ncontext\n,\n \nString\n \nerror\n,\n \nString\n \nfilePath\n)\n \n{\n \n}\n\n\n});\n\n\n}\n\n\n\n\n\n\nLoading Textures\n\n\nImported 3D assets tyically create bitmap textures but the asset loader can import textures directly in many other formats. You can import cubemaps, compressed cubemaps, floating point textures as well as compressed and uncompressed bitmaps.\n\n\nAll textures are loaded asynchronously in a background thread. The asset loader immediately returns a \nGVRTexture\n object but the GVRImage providing the data for the texture may not be available yet. GearVRf will not render a mesh until all of the textures it requires are loaded. Typically your application does not need to know when textures are loaded but, if this is necessary, the asset loader provides a callback mechanism for this purpose.\n\n\nThis example shows how to load a cubemap texture from a ZIP file and apply use it as a skybox.\n\n\nGVRTexture\n \ntex\n \n=\n \nctx\n.\ngetAssetLoader\n().\nloadCubemapTexture\n(\nnew\n \nGVRAndroidResource\n(\nctx\n,\n \nR\n.\nraw\n.\nbeach\n));\n\n\nGVRMaterial\n \ncubeMapMtl\n \n=\n \nnew\n \nGVRMaterial\n(\nctx\n,\n \nGVRMaterial\n.\nGVRShaderType\n.\nCubemap\n.\nID\n);\n\n\nGVRSceneObject\n \nskybox\n \n=\n \nnew\n \nGVRCubeSceneObject\n(\nctx\n,\n \nfalse\n,\n \ncubeMapMtl\n);\n\n\n\ncubeMapMtl\n.\nsetTexture\n(\nu_texture\n,\n \ntex\n);\n\n\nskybox\n.\ngetTransform\n().\nsetScale\n(\n10\n,\n \n10\n,\n \n10\n);\n\n\nskybox\n.\nsetName\n(\nbackground\n);\n\n\nscene\n.\naddSceneObject\n(\nskybox\n);", 
            "title": "Loading Assets"
        }, 
        {
            "location": "/programming_guide/features/loading_assets/#loading-a-3d-model", 
            "text": "Loading models is handled by the GVRAssetLoader class which is accessible from the context by calling GVRContext.getAssetLoader(). The asset loader can load models from a variety of places. If you are providing a file name, a prefix on the name indicates the origin of the file:   \"sd:\" designates the model on the phone SD card.  \"http:\" or \"https:\" designates the model is on the internet.  No prefix meaqns the filename is relative to the \"assets\" directory.   For more flexibility, you can use the  GVRAndroidResource  class which lets you import assets from resources in your application or from an already open Android stream. Both models and textures can be loaded from GVRAndroidResource objects.     GVRAndroidResource Constructors  Volume Type  Input source      GVRAndroidResource(String path)  ResourceType.LINUX_FILESYSTEM  SD card    GVRAndroidResource(File file)  ResourceType.LINUX_FILESYSTEM  SD card    GVRAndroidResource(GVRContext, int resourceID)  ResourceType.ANDROID_RESOURCE  res directory    GVRAndroidResource(Context, int resourceID)  ResourceType.ANDROID_RESOURCE  res directory    GVRAndroidResource(GVRContext, String path)  ResourceType.ANDROID_ASSETS  assets directory    GVRAndroidResource(GVRContext, URL url)  ResourceType.NETWORK  internet    GVRAndroidResource(GVRContext, InputStream stream)  ResourceType.INPUT_STREAM  input stream     The  GVRAssetLoader.loadModel  function loads a model from a device and returns as soon as the model geometry has been parsed and accumulated. This model may not have been added to the scene yet. If you pass the current GVRScene as an argument, the asset loader will wait until all of the textures in the model have been loaded and then add it to the scene. If you omit the argument, the model is not added to the scene and you will need to add it in your own code.", 
            "title": "Loading a 3D Model"
        }, 
        {
            "location": "/programming_guide/features/loading_assets/#model-loading-examples", 
            "text": "This example shows how to load a model from a URL and another from the application resources. The textures are loaded in the background in another thread. The model will be added to the scene when all of its textures have completed loading. The model returned may not be completely loaded but all of the geometry will be accessible. Usually assets are loaded in the onInit function of your main script.  public   void   onInit ( GVRContext   context )  { \n     GVRScene   scene   =   context . getMainScene (); \n     try \n     { \n         String   url   =   https://raw.githubusercontent.com/gearvrf/GearVRf-Demos/master/gvrjassimpmodelloader/assets/trees/trees9.3ds ; \n         GVRSceneObject   model1   =   context . getAssetLoader (). loadModel ( url ,   scene ); \n         model1 . getTransform (). setPosition ( 0.0f ,   - 4.0f ,   - 20.0f ); \n\n         GVRAndroidResource   resource   =   new   GVRAndroidResource ( context ,   R . raw . spaceship ); \n         EnumSet GVRImportSettings   settings   =   GVRImportSettings . getRecommendedSettings (); \n         GVRSceneObject   model2   =   context . getAssetLoader (). loadModel ( resource ,   settings ,   true ,   scene ); \n         model2 . getTransform (). setPositionZ (- 10.0f ); \n     } \n     catch   ( IOException   e ) \n     { \n         Log . e ( ERROR ,   Failed to load model: %s ,   e ); \n     }  }", 
            "title": "Model Loading Examples"
        }, 
        {
            "location": "/programming_guide/features/loading_assets/#import-settings", 
            "text": "The  GVRImportSettings  object controls how the asset is imported. Depending on what your application is going to do with the model, import settings can help optimize performance. By default, all models are imported in full fidelity - all vertex components, light sources, cameras and textures are included. If you are not using light sources, it is more efficient to omit normals from the meshes and to not import any light sources in the model. Similarly, if you do not plan to animate the model, importing without animation will be faster and will suppress bone indices and bone weights from your meshes, which will cause GearVRf to use a more efficient shader.     Setting  What it does      NO_LIGHTING  Suppress import of normals and light sources    NO_ANIMATION  Suppress import of animation (bones, bone indices, bone weights and animation clips)    NO_TEXTURING  Suppress import of textures and texture coordinates    START_ANIMATIONS  Automatically start animations    CALCULATE_TANGENTS  Calculate tangents required for normal mapping    JOIN_IDENTICAL_VERTICES*  Join vertices when possible    TRIANGULATE*  Triangulate all meshes (required for GearVRf)    CALCULATE_NORMALS  Calculate normals if not present    CALCULATE_SMOOTH_NORMALS  Calculate and smooth normals    LIMIT_BONE_WEIGHT*  Limit bone weight to 4 (required for GearVRf)    IMPROVE_VERTEX_CACHE_LOCALITY  reorder vertices for cache locality    SORTBY_PRIMITIVE_TYPE*  Split meshes by primitive type    OPTIMIZE_MESHES  Combine meshes when possible    OPTIMIZE_GRAPH  Merge nodes without bones    FLIP_UV*  Flip UV coordinates     Typically you will get the recommended settings (they are starred in the table) and add to them since a few of these settings are required for GearVRf to function properly. Only indexed triangle meshes are supported and meshes may only have four bones.", 
            "title": "Import Settings"
        }, 
        {
            "location": "/programming_guide/features/loading_assets/#asynchronous-loading", 
            "text": "The asset loader can also asynchronously load geometry and textures. You have the option of providing an asset event handler which will notify your application when textures and geometry are loaded and when the asset and all of its textures have finished loading. In this case, a model is placed in the scene and the asset loader adds the imported scene objects as children. You can also request the asset loader to replace the entire scene.  The asynchronous forms of  loadModel  specify the input asset as a  GVRResourceVolume  object. This object allows multiple assets to be loaded from a single input stream, such as a ZIP file. You can also subclass  GVRResourceVolume  to get complete control over the asset loading process.  This example waits for a model to be loaded from the assets directory and then centers it before adding it to the scene.  GVRScene   scene ;  GVRSceneObject   root   =   new   GVRSceneObject ( context );  GVRResourceVolume   volume   =   new   GVRResourceVolume ( context ,   models/mymodel.fbx );  context . getAssetLoader (). loadModel ( root ,   volume ,   new   IAssetEvents ()  { \n     public   void   onAssetLoaded ( GVRContext   context ,   GVRSceneObject   model ,   String   filePath ,   String   errors ); \n     { \n         BoundingVolume   bv   =   model . getBoundingVolume (); \n         Vector3f   c   =   bv . center (); \n         model . getTransform (). setPosition (- c . x ,   - c . y ,   - c . z   -   1.0f ); \n     } \n     public   void   onModelLoaded ( GVRContext   context ,   GVRSceneObject   model ,   String   filePath )   {   } \n     public   void   onTextureLoaded ( GVRContext   context ,   GVRTexture   texture ,   String   filePath )   {   } \n     public   void   onModelError ( GVRContext   context ,   String   error ,   String   filePath )   {   } \n     public   void   onTextureError ( GVRContext   context ,   String   error ,   String   filePath )   {   }  });  }", 
            "title": "Asynchronous Loading"
        }, 
        {
            "location": "/programming_guide/features/loading_assets/#loading-textures", 
            "text": "Imported 3D assets tyically create bitmap textures but the asset loader can import textures directly in many other formats. You can import cubemaps, compressed cubemaps, floating point textures as well as compressed and uncompressed bitmaps.  All textures are loaded asynchronously in a background thread. The asset loader immediately returns a  GVRTexture  object but the GVRImage providing the data for the texture may not be available yet. GearVRf will not render a mesh until all of the textures it requires are loaded. Typically your application does not need to know when textures are loaded but, if this is necessary, the asset loader provides a callback mechanism for this purpose.  This example shows how to load a cubemap texture from a ZIP file and apply use it as a skybox.  GVRTexture   tex   =   ctx . getAssetLoader (). loadCubemapTexture ( new   GVRAndroidResource ( ctx ,   R . raw . beach ));  GVRMaterial   cubeMapMtl   =   new   GVRMaterial ( ctx ,   GVRMaterial . GVRShaderType . Cubemap . ID );  GVRSceneObject   skybox   =   new   GVRCubeSceneObject ( ctx ,   false ,   cubeMapMtl );  cubeMapMtl . setTexture ( u_texture ,   tex );  skybox . getTransform (). setScale ( 10 ,   10 ,   10 );  skybox . setName ( background );  scene . addSceneObject ( skybox );", 
            "title": "Loading Textures"
        }, 
        {
            "location": "/programming_guide/features/render_data/", 
            "text": "The render data component is what makes a scene object visible. It provides both geometry and appearance properties. The geometry is a single GVRMesh object which contains a set of indexed vertices. The appearance is a GVRMaterial object which contains a set of key/value pairs defining the variables to be sent to the shader. The shader is a program that executes on the GPU. During rendering, GearVRf manages data flow between your application and the GPU, sending the meshes and materials to the GPU as they are needed. This may require GearVRf to compile and load a shader into the GPU while your application is running. This may happen when you add something to the scene using GVRScene.addSceneObject. The render data component also controls how your mesh is rendered. You can enable or disable lighting, display an object only on one eye and control the order of rendering using its functions.\n\n\n\n\n\n\n\n\nGVRRenderData Function\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nenableLighting\n\n\nEnable light sources in the shader\n\n\n\n\n\n\ndisableLighting\n\n\nDisable light sources in the shader\n\n\n\n\n\n\nsetAlphaBlend\n\n\nEnable / disable alpha blending\n\n\n\n\n\n\nsetAlphaToCoverage\n\n\nEnable / disable alpha to coverage\n\n\n\n\n\n\nsetAlphaBlendFunc\n\n\nSet alpha blend functions\n\n\n\n\n\n\nsetCullTest\n\n\nEnable / disable backface culling\n\n\n\n\n\n\nsetCullFace\n\n\nDesignate back or front faces for culling\n\n\n\n\n\n\nsetDepthTest\n\n\nEnable / disable depth testing (Z buffer)\n\n\n\n\n\n\nsetDepthMask\n\n\nEnable / disable depth mask\n\n\n\n\n\n\nsetDrawMode\n\n\nDesignate triangles, lines or points\n\n\n\n\n\n\nsetRenderMask\n\n\nDesignate rendering left, right or both eyes\n\n\n\n\n\n\nsetRenderingOrder\n\n\nEstablish rendering order\n\n\n\n\n\n\nsetSampleCoverage\n\n\nSpecifies coverage of modification mask\n\n\n\n\n\n\nsetInvertCoverageMask\n\n\nDesignates whether modification mask is inverted\n\n\n\n\n\n\nsetOffset\n\n\nEnables /disables polygon fill offset\n\n\n\n\n\n\nsetOffsetFactor\n\n\nSpecifies polygon fill offset factor\n\n\n\n\n\n\nsetOffsetUnits\n\n\nSpecifies polygon fill offset units\n\n\n\n\n\n\nsetCastShadows\n\n\nEnable / disable shadow casting\n\n\n\n\n\n\nsetMesh\n\n\nDesignate the mesh to render\n\n\n\n\n\n\nsetMaterial\n\n\nSpecify material properties for shader\n\n\n\n\n\n\n\n\nRender Passes\n\n\nA render pass lets you render the same scene object multiple times with different settings. This is useful to achieve effects like cartoon rendering or adding glow around an object. The benefit of using a render pass as opposed to duplicating the object is that culling, transformation and skinning are only performed once. A render pass encapsulates the material and rendering properties (but not the mesh).\n\n\nThis example shows how to implement a multi-sided material using render passes. It uses a red material for the front faces and a blue material for the back faces. \n\n\nGVRSceneObject\n \ncube\n \n=\n \nnew\n \nGVRCubeSceneObject\n(\ngvrContext\n);\n\n\nGVRRenderData\n \nrdata\n \n=\n \ncube\n.\ngetRenderData\n();\n\n\nGVRMaterial\n \nred\n \n=\n \nrdata\n.\ngetMaterial\n();\n\n\nGVRMaterial\n \nblue\n \n=\n \nnew\n \nGVRMaterial\n(\ngvrContext\n);\n\n\nGVRRenderPass\n \npass\n \n=\n \nnew\n \nGVRRenderPass\n(\ngvrContext\n);\n\n\n\nred\n.\nsetDiffuseColor\n(\n1\n,\n \n0\n,\n \n0\n,\n \n1\n);\n\n\nblue\n.\nsetDiffuseColor\n(\n0\n,\n \n0\n,\n \n1\n,\n \n0\n);\n\n\nrdata\n.\nsetCullFace\n(\nGVRCullFaceEnum\n.\nFront\n);\n\n\npass\n.\nsetMaterial\n(\nblue\n);\n\n\npass\n.\nsetCullFace\n(\nGVRCullFaceEnum\n.\nBack\n);\n\n\nrdata\n.\naddPass\n(\npass\n);", 
            "title": "Render Data"
        }, 
        {
            "location": "/programming_guide/features/render_data/#render-passes", 
            "text": "A render pass lets you render the same scene object multiple times with different settings. This is useful to achieve effects like cartoon rendering or adding glow around an object. The benefit of using a render pass as opposed to duplicating the object is that culling, transformation and skinning are only performed once. A render pass encapsulates the material and rendering properties (but not the mesh).  This example shows how to implement a multi-sided material using render passes. It uses a red material for the front faces and a blue material for the back faces.   GVRSceneObject   cube   =   new   GVRCubeSceneObject ( gvrContext );  GVRRenderData   rdata   =   cube . getRenderData ();  GVRMaterial   red   =   rdata . getMaterial ();  GVRMaterial   blue   =   new   GVRMaterial ( gvrContext );  GVRRenderPass   pass   =   new   GVRRenderPass ( gvrContext );  red . setDiffuseColor ( 1 ,   0 ,   0 ,   1 );  blue . setDiffuseColor ( 0 ,   0 ,   1 ,   0 );  rdata . setCullFace ( GVRCullFaceEnum . Front );  pass . setMaterial ( blue );  pass . setCullFace ( GVRCullFaceEnum . Back );  rdata . addPass ( pass );", 
            "title": "Render Passes"
        }, 
        {
            "location": "/programming_guide/features/meshes/", 
            "text": "VR mesh types, access, and examples\n\n\nIndexed triangle meshes are the only shape definition currently supported by GearVRF. Each mesh contains a set of vertices with the 3D locations of the triangle coordinates. Typically these are unique locations to maximize vertex sharing but this is not a requirement. A triangle has three indices designating which vertices are used by that triangle.\n\n\nIn addition to positions, a mesh may have normals and texture coordinates as well. These arrays, if present, must follow the same ordering as the vertices. There is only one set of triangle indices to reference the position, normal and texture coordinate. This is unlike some systems which permit multiple index tables. \n\n\n\n\nSkinned Meshes\n\n\nSkinned meshes have vertex bone data to indicate which bones affect which vertices in the mesh. A bone is a transform matrix which affects a subset of vertices in the mesh. Each vertex can be influenced by up to four bones.\n\n\nA mesh also contains a list of the bone transforms (GVRBone objects) that influence its vertices. The bone indices in the vertex array reference the bones in this list.\n\n\nGearVRf executes skinning on the GPU but it calculates the bone matrices on the CPU.\n\n\n\n\nAccessing Mesh Components\n\n\nThe vertex shader used to render the mesh determines which vertex components are required. The GearVRf built-in shaders rely on positions, normals, texture coordinates and bone information. You can write your own shaders which use other vertex components.\n\n\nEach vertex component has a unique name and type. GearVRf vertex components are vectors containing between one and four floats. Each component has a function wh|ich can get or set that component for the entire vertex array. \nGVRMesh\n provides convenience functions for the built-in types. Reading or writing the vertex array is a high overhead operation and should not be done every frame.\n\n\nThe index array describes an indexed triangle list. Each triangle has three consecutive indices in the array designating the vertices from the vertex array that represent that triangle. The index array may either be 16-bit or 32-bit.\n\n\n\n\n\n\n\n\nAttribute Name\n\n\nGVRMesh Setter\n\n\nGVRMesh Getter\n\n\n\n\n\n\n\n\n\n\na_position\n\n\nsetVertices(float[])\n\n\nfloat[] getVertices()\n\n\n\n\n\n\na_normal\n\n\nsetNormals(float[])\n\n\nfloat[] getNormals()\n\n\n\n\n\n\na_texcoord\n\n\nsetTexCoords(float[])\n\n\nfloat[] getTexCoords()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGVRMesh Setter\n\n\nGVRMesh Getter\n\n\n\n\n\n\n\n\n\n\nsetFloatArray(String name, float[])\n\n\nfloat[] getFloatArray(String name)\n\n\n\n\n\n\nsetFloatVec(String name, FloatBuffer)\n\n\ngetFloatVec(String name, FloatBuffer)\n\n\n\n\n\n\nsetIntArray(String name, int[])\n\n\nint[] getIntArray(String name)\n\n\n\n\n\n\nsetIntVec(String name, IntBuffer)\n\n\ngetIntVec(String name, IntBuffer)\n\n\n\n\n\n\nsetIndices(int[])\n\n\nint[] getIndices()\n\n\n\n\n\n\nsetTriangles(char[])\n\n\nchar[] getTriangles()\n\n\n\n\n\n\n\n\nMesh Construction Example\n\n\nMost of the time your code will obtain meshes by loading asset files. You can also construct or modify meshes programmatically. A mesh may contain positions, normal and texture coordinates. Depending on the shader used to display the mesh, some of these vertex components may not be used. For example, a shader which does not do lighting will typically not need normals. You can omit the normals and texture coordinate arrays if your shader doesn't need them.\n\n\nThis function constructs a mesh of two triangles with only positions and normals. (If you try to use a textured shader with this mesh, you will get an error.)\n\n\nGVRMesh\n \ncreateMesh\n(\nGVRContext\n \ngvrContext\n)\n\n\n{\n\n    \nGVRMesh\n \nmesh\n \n=\n \nnew\n \nGVRMesh\n(\ngvrContext\n);\n\n    \nfloat\n[]\n \nvertices\n \n=\n\n    \n{\n\n        \n-\n1.0f\n,\n \n0.0f\n,\n \n0.0f\n,\n\n        \n0.0f\n,\n \n1.0f\n,\n \n0.0f\n,\n\n        \n1.0f\n,\n \n0.0f\n,\n \n0.0f\n,\n\n        \n0.0f\n,\n \n-\n1.0f\n,\n \n0.0f\n\n    \n};\n\n    \nfloat\n[]\n \nnormals\n \n=\n\n    \n{\n\n        \n0\n,\n \n0\n,\n \n1\n,\n\n        \n0\n,\n \n0\n,\n \n1\n,\n\n        \n0\n,\n \n0\n,\n \n1\n,\n\n        \n0\n,\n \n0\n,\n \n1\n\n    \n};\n\n    \nchar\n[]\n \ntriangles\n \n=\n \n{\n \n0\n,\n \n1\n,\n \n2\n,\n \n2\n,\n \n3\n,\n \n0\n \n};\n\n    \nmesh\n.\nsetVertices\n(\nvertices\n);\n\n    \nmesh\n.\nsetNormals\n(\nnormals\n);\n\n    \nmesh\n.\nsetTriangles\n(\ntriangles\n);\n\n    \nreturn\n \nmesh\n;\n\n\n}\n\n\n\n\n\n\nYou need to attach your mesh to a GVRSceneObject before it can be displayed. The GVRRenderData object holds both a mesh and a material. Each visible scene object must have render data. This code adds the newly constructed mesh to the scene. Here we assume the GVRMaterial has already been constructed.\n\n\nGVRMesh\n \nmesh\n \n=\n \ncreateMesh\n(\ngvrContext\n);\n\n\nGVRSceneObject\n \nobj\n \n=\n \nnew\n \nGVRSceneObject\n(\ngvrContext\n,\n \nmesh\n);\n\n\nGVRRenderData\n \nrdata\n \n=\n \nobj\n.\ngetRenderData\n();\n\n\nrdata\n.\nsetMaterial\n(\nmaterial\n);\n\n\n\n\n\n\nVertex and Index Buffers\n\n\nThe vertices and indices of the mesh are actually kept as separate GearVRf objects and they can be shared across meshes. \nGVRVertexBuffer\n contains a set of vertices that can be used by a mesh. \nGVRIndexBuffer\n contains a set of face indices.\n\n\nThe layout of the vertices in the vertex buffer is established at construction time and cannot be changed. The vertex descriptor string describes the name and type of each vertex component. The supported types are \nfloat, float2, float3, float4, int, int2, int3 and int4\n. The default vertex descriptor if none is specified is \"float3 a_position float2 a_texcoord float3 a_normal\" which will work with all of the built-in shaders.\n\n\nThe size of the indices in the index buffer is also fixed at construction time. Indices may be either 2 bytes or 4 bytes.\n\n\nIn this example, we create two scene objects representing different faces of a cube which share the same vertex array.\n\n\nfloat\n[]\n \npos\n \n=\n \nnew\n \nfloat\n[]\n \n{\n \n1\n,\n \n1\n,\n \n1\n,\n \n1\n,\n \n-\n1\n,\n \n1\n,\n \n-\n1\n,\n \n1\n,\n \n1\n,\n \n-\n1\n,\n \n-\n1\n,\n \n1\n,\n\n                            \n1\n,\n \n1\n,\n \n-\n1\n,\n \n1\n,\n \n-\n1\n,\n \n-\n1\n,\n \n-\n1\n,\n \n1\n,\n \n-\n1\n,\n \n-\n1\n,\n \n-\n1\n,\n \n-\n1\n,};\n\n\nfloat\n[]\n \nuv\n \n=\n \nnew\n \nfloat\n[]\n \n{\n \n0\n,\n \n0\n,\n \n0\n,\n \n1\n,\n \n0\n,\n \n1\n,\n \n1\n,\n \n0\n,\n \n0\n,\n \n0\n,\n \n0\n,\n \n1\n,\n \n0\n,\n \n1\n,\n \n1\n,\n \n0\n \n};\n\n\nGVRVertexBuffer\n \ncubeVerts\n \n=\n \nnew\n \nGVRVertexBuffer\n(\ncontext\n,\n \nfloat3 a_position float2 a_texcoord\n);\n\n\nGVRIndexBuffer\n \nface1Tris\n \n=\n \nnew\n \nGVRIndexBuffer\n(\ncontext\n,\n \n2\n,\n \n6\n);\n\n\nGVRIndexBuffer\n \nface2Tris\n \n=\n \nnew\n \nGVRIndexBuffer\n(\ncontext\n,\n \n2\n,\n \n6\n);\n\n\nGVRMesh\n \nmesh1\n \n=\n \nnew\n \nGVRMesh\n(\ncubeVerts\n,\n \nface1Tris\n);\n\n\nGVRMesh\n \nmesh2\n \n=\n \nnew\n \nGVRMesh\n(\ncubeVerts\n,\n \nface2Tris\n);\n\n\n\ncubeVerts\n.\nsetFloatArray\n(\na_position\n,\n \npos\n);\n\n\ncubeVerts\n.\nsetFloatArray\n(\na_texcoord\n,\n \nuv\n);\n\n\nface1Tris\n.\nsetShortVec\n(\nnew\n \nchar\n[]\n \n{\n \n0\n,\n \n1\n,\n \n2\n,\n \n2\n,\n \n1\n,\n \n3\n \n});\n\n\nface2Tris\n.\nsetShortVec\n(\nnew\n \nchar\n[]\n \n{\n \n4\n,\n \n5\n,\n \n6\n,\n \n6\n,\n \n5\n,\n \n7\n \n});\n\n\n\nGVRSceneObject\n \nobject1\n \n=\n \nnew\n \nGVRSceneObject\n(\ncontext\n,\n \nmesh1\n);\n\n\nGVRSceneObject\n \nobject2\n \n=\n \nnew\n \nGVRSceneObject\n(\ncontext\n,\n \nmesh2\n);", 
            "title": "Meshes"
        }, 
        {
            "location": "/programming_guide/features/meshes/#vr-mesh-types-access-and-examples", 
            "text": "Indexed triangle meshes are the only shape definition currently supported by GearVRF. Each mesh contains a set of vertices with the 3D locations of the triangle coordinates. Typically these are unique locations to maximize vertex sharing but this is not a requirement. A triangle has three indices designating which vertices are used by that triangle.  In addition to positions, a mesh may have normals and texture coordinates as well. These arrays, if present, must follow the same ordering as the vertices. There is only one set of triangle indices to reference the position, normal and texture coordinate. This is unlike some systems which permit multiple index tables.", 
            "title": "VR mesh types, access, and examples"
        }, 
        {
            "location": "/programming_guide/features/meshes/#skinned-meshes", 
            "text": "Skinned meshes have vertex bone data to indicate which bones affect which vertices in the mesh. A bone is a transform matrix which affects a subset of vertices in the mesh. Each vertex can be influenced by up to four bones.  A mesh also contains a list of the bone transforms (GVRBone objects) that influence its vertices. The bone indices in the vertex array reference the bones in this list.  GearVRf executes skinning on the GPU but it calculates the bone matrices on the CPU.", 
            "title": "Skinned Meshes"
        }, 
        {
            "location": "/programming_guide/features/meshes/#accessing-mesh-components", 
            "text": "The vertex shader used to render the mesh determines which vertex components are required. The GearVRf built-in shaders rely on positions, normals, texture coordinates and bone information. You can write your own shaders which use other vertex components.  Each vertex component has a unique name and type. GearVRf vertex components are vectors containing between one and four floats. Each component has a function wh|ich can get or set that component for the entire vertex array.  GVRMesh  provides convenience functions for the built-in types. Reading or writing the vertex array is a high overhead operation and should not be done every frame.  The index array describes an indexed triangle list. Each triangle has three consecutive indices in the array designating the vertices from the vertex array that represent that triangle. The index array may either be 16-bit or 32-bit.     Attribute Name  GVRMesh Setter  GVRMesh Getter      a_position  setVertices(float[])  float[] getVertices()    a_normal  setNormals(float[])  float[] getNormals()    a_texcoord  setTexCoords(float[])  float[] getTexCoords()        GVRMesh Setter  GVRMesh Getter      setFloatArray(String name, float[])  float[] getFloatArray(String name)    setFloatVec(String name, FloatBuffer)  getFloatVec(String name, FloatBuffer)    setIntArray(String name, int[])  int[] getIntArray(String name)    setIntVec(String name, IntBuffer)  getIntVec(String name, IntBuffer)    setIndices(int[])  int[] getIndices()    setTriangles(char[])  char[] getTriangles()", 
            "title": "Accessing Mesh Components"
        }, 
        {
            "location": "/programming_guide/features/meshes/#mesh-construction-example", 
            "text": "Most of the time your code will obtain meshes by loading asset files. You can also construct or modify meshes programmatically. A mesh may contain positions, normal and texture coordinates. Depending on the shader used to display the mesh, some of these vertex components may not be used. For example, a shader which does not do lighting will typically not need normals. You can omit the normals and texture coordinate arrays if your shader doesn't need them.  This function constructs a mesh of two triangles with only positions and normals. (If you try to use a textured shader with this mesh, you will get an error.)  GVRMesh   createMesh ( GVRContext   gvrContext )  { \n     GVRMesh   mesh   =   new   GVRMesh ( gvrContext ); \n     float []   vertices   = \n     { \n         - 1.0f ,   0.0f ,   0.0f , \n         0.0f ,   1.0f ,   0.0f , \n         1.0f ,   0.0f ,   0.0f , \n         0.0f ,   - 1.0f ,   0.0f \n     }; \n     float []   normals   = \n     { \n         0 ,   0 ,   1 , \n         0 ,   0 ,   1 , \n         0 ,   0 ,   1 , \n         0 ,   0 ,   1 \n     }; \n     char []   triangles   =   {   0 ,   1 ,   2 ,   2 ,   3 ,   0   }; \n     mesh . setVertices ( vertices ); \n     mesh . setNormals ( normals ); \n     mesh . setTriangles ( triangles ); \n     return   mesh ;  }   You need to attach your mesh to a GVRSceneObject before it can be displayed. The GVRRenderData object holds both a mesh and a material. Each visible scene object must have render data. This code adds the newly constructed mesh to the scene. Here we assume the GVRMaterial has already been constructed.  GVRMesh   mesh   =   createMesh ( gvrContext );  GVRSceneObject   obj   =   new   GVRSceneObject ( gvrContext ,   mesh );  GVRRenderData   rdata   =   obj . getRenderData ();  rdata . setMaterial ( material );", 
            "title": "Mesh Construction Example"
        }, 
        {
            "location": "/programming_guide/features/meshes/#vertex-and-index-buffers", 
            "text": "The vertices and indices of the mesh are actually kept as separate GearVRf objects and they can be shared across meshes.  GVRVertexBuffer  contains a set of vertices that can be used by a mesh.  GVRIndexBuffer  contains a set of face indices.  The layout of the vertices in the vertex buffer is established at construction time and cannot be changed. The vertex descriptor string describes the name and type of each vertex component. The supported types are  float, float2, float3, float4, int, int2, int3 and int4 . The default vertex descriptor if none is specified is \"float3 a_position float2 a_texcoord float3 a_normal\" which will work with all of the built-in shaders.  The size of the indices in the index buffer is also fixed at construction time. Indices may be either 2 bytes or 4 bytes.  In this example, we create two scene objects representing different faces of a cube which share the same vertex array.  float []   pos   =   new   float []   {   1 ,   1 ,   1 ,   1 ,   - 1 ,   1 ,   - 1 ,   1 ,   1 ,   - 1 ,   - 1 ,   1 , \n                             1 ,   1 ,   - 1 ,   1 ,   - 1 ,   - 1 ,   - 1 ,   1 ,   - 1 ,   - 1 ,   - 1 ,   - 1 ,};  float []   uv   =   new   float []   {   0 ,   0 ,   0 ,   1 ,   0 ,   1 ,   1 ,   0 ,   0 ,   0 ,   0 ,   1 ,   0 ,   1 ,   1 ,   0   };  GVRVertexBuffer   cubeVerts   =   new   GVRVertexBuffer ( context ,   float3 a_position float2 a_texcoord );  GVRIndexBuffer   face1Tris   =   new   GVRIndexBuffer ( context ,   2 ,   6 );  GVRIndexBuffer   face2Tris   =   new   GVRIndexBuffer ( context ,   2 ,   6 );  GVRMesh   mesh1   =   new   GVRMesh ( cubeVerts ,   face1Tris );  GVRMesh   mesh2   =   new   GVRMesh ( cubeVerts ,   face2Tris );  cubeVerts . setFloatArray ( a_position ,   pos );  cubeVerts . setFloatArray ( a_texcoord ,   uv );  face1Tris . setShortVec ( new   char []   {   0 ,   1 ,   2 ,   2 ,   1 ,   3   });  face2Tris . setShortVec ( new   char []   {   4 ,   5 ,   6 ,   6 ,   5 ,   7   });  GVRSceneObject   object1   =   new   GVRSceneObject ( context ,   mesh1 );  GVRSceneObject   object2   =   new   GVRSceneObject ( context ,   mesh2 );", 
            "title": "Vertex and Index Buffers"
        }, 
        {
            "location": "/programming_guide/features/materials/", 
            "text": "VR materials, access, and examples\n\n\nA material dictates how a scene object will be colored and textured. The visual appearance of a mesh in the scene is controlled by a fragment shader program in the GPU. GearVRf will automatically construct a shader for common use cases but you can also provide your own shader code.\n\n\nThe GVRMaterial object encapsulates the fragment shader and all of its associated data. This usually includes one or more texture maps and lighting properties that affect how the surface reacts to lights in the scene. Because you can write custom shaders, you can attach your own custom data to a material that your shader can use in computations. This data can be scalar numbers (float or int), arrays or textures. Each custom data element has a string key used to look up or modify its value. When the object is rendered, these values are passed to the shader program.\n\n\nThis picture shows a material with two custom data parameters - a diffuse color with an alpha (4 floats) and an integer enabling transparency. \n\n\n\n\nAccessing Shader Uniforms\n\n\nThe GVRMaterial object provides access to the fragment shader uniforms by name. Setting the value of a material parameter will set the correspondingly named uniform variable in the fragment shader associated with that material. GVRMaterial has a set of functions that read and write the material parameters based on type.\n\n\nYou also use GVRMaterial to bind textures to samplers in the shader. Adding a texture by name to a material binds that texture to the sampler of the same name in the shader.\n\n\n\n\n\n\n\n\nShader Type\n\n\nGVRMaterial Setter\n\n\nGVRMaterial Getter\n\n\n\n\n\n\n\n\n\n\nfloat\n\n\nsetFloat(String name, float v)\n\n\nfloat getFloat(String name)\n\n\n\n\n\n\nfloat2\n\n\nsetVec2(String name, float[] v)\n\n\nfloat[] getVec2(String name)\n\n\n\n\n\n\nfloat3\n\n\nsetVec3(String name, float[] v)\n\n\nfloat[] getVec3(String name)\n\n\n\n\n\n\nfloat4\n\n\nsetVec4(String name, float[] v)\n\n\nfloat[] getVec4(String name)\n\n\n\n\n\n\nfloat[]\n\n\nsetFloatArray(String name, float[] v)\n\n\nfloat[] getFloatVec(String name)\n\n\n\n\n\n\nint[]\n\n\nsetIntArray(String name, int[] v)\n\n\nint[] getIntVec(String name)\n\n\n\n\n\n\nint\n\n\nsetInt(String name, int v)\n\n\nint getInt(String name)\n\n\n\n\n\n\nmat4\n\n\nsetMat4(String name, float m00, .. float m33)\n\n\nfloat[] getFloatArray(String name)\n\n\n\n\n\n\nsampler2D\n\n\nsetTexture(String name, GVRTexture t)\n\n\nGVRTexture getTexture(String name)\n\n\n\n\n\n\n\n\nMaterial Construction Example\n\n\nA material contains all the data required for the specific shader you are going to use. You will set up different parameters for different shaders. GearVRF has several GearVRfDeveloperGuide.LegacyShaders which can render meshes in a variety of ways. All of them use the same GVRMaterial object but with different properties.\n\n\nIn this example we construct a GVRMaterial to be used with the phong shader. This shader requires us to provide a diffuse texture as well as material and lighting properties.\n\n\nGVRMaterial\n \ncreateMaterial\n(\nGVRContext\n \ngvrContext\n)\n\n\n{\n\n   \nGVRMaterial\n \nmaterial\n \n=\n \ncreateMaterial\n(\ngvrContext\n);\n\n   \nGVRTexture\n \ntex\n \n=\n \ngvrContext\n.\ngetAssetLoader\n().\nloadTexture\n(\n\n        \nnew\n \nGVRAndroidResource\n(\ngvrContext\n,\nR\n.\ndrawable\n.\ngearvr_logo\n));\n\n\n   \nmaterial\n.\nsetVec4\n(\ndiffuse_color\n,\n \n0.5f\n,\n \n0.5f\n,\n \n0.5f\n,\n \n1.0f\n);\n\n   \nmaterial\n.\nsetVec4\n(\nambient_color\n,\n \n0.2f\n,\n \n0.2f\n,\n \n0.2f\n,\n \n1.0f\n);\n\n   \nmaterial\n.\nsetVec4\n(\nspecular_color\n,\n \n1.0f\n,\n \n1.0f\n,\n \n1.0f\n,\n \n1.0f\n);\n\n   \nmaterial\n.\nsetFloat\n(\n128.0f\n);\n\n   \nmaterial\n.\nsetTexture\n(\ndiffuseTexture\n,\n \ntex\n);\n\n   \nreturn\n \nmaterial\n;\n\n\n}", 
            "title": "Materials"
        }, 
        {
            "location": "/programming_guide/features/materials/#vr-materials-access-and-examples", 
            "text": "A material dictates how a scene object will be colored and textured. The visual appearance of a mesh in the scene is controlled by a fragment shader program in the GPU. GearVRf will automatically construct a shader for common use cases but you can also provide your own shader code.  The GVRMaterial object encapsulates the fragment shader and all of its associated data. This usually includes one or more texture maps and lighting properties that affect how the surface reacts to lights in the scene. Because you can write custom shaders, you can attach your own custom data to a material that your shader can use in computations. This data can be scalar numbers (float or int), arrays or textures. Each custom data element has a string key used to look up or modify its value. When the object is rendered, these values are passed to the shader program.  This picture shows a material with two custom data parameters - a diffuse color with an alpha (4 floats) and an integer enabling transparency.", 
            "title": "VR materials, access, and examples"
        }, 
        {
            "location": "/programming_guide/features/materials/#accessing-shader-uniforms", 
            "text": "The GVRMaterial object provides access to the fragment shader uniforms by name. Setting the value of a material parameter will set the correspondingly named uniform variable in the fragment shader associated with that material. GVRMaterial has a set of functions that read and write the material parameters based on type.  You also use GVRMaterial to bind textures to samplers in the shader. Adding a texture by name to a material binds that texture to the sampler of the same name in the shader.     Shader Type  GVRMaterial Setter  GVRMaterial Getter      float  setFloat(String name, float v)  float getFloat(String name)    float2  setVec2(String name, float[] v)  float[] getVec2(String name)    float3  setVec3(String name, float[] v)  float[] getVec3(String name)    float4  setVec4(String name, float[] v)  float[] getVec4(String name)    float[]  setFloatArray(String name, float[] v)  float[] getFloatVec(String name)    int[]  setIntArray(String name, int[] v)  int[] getIntVec(String name)    int  setInt(String name, int v)  int getInt(String name)    mat4  setMat4(String name, float m00, .. float m33)  float[] getFloatArray(String name)    sampler2D  setTexture(String name, GVRTexture t)  GVRTexture getTexture(String name)", 
            "title": "Accessing Shader Uniforms"
        }, 
        {
            "location": "/programming_guide/features/materials/#material-construction-example", 
            "text": "A material contains all the data required for the specific shader you are going to use. You will set up different parameters for different shaders. GearVRF has several GearVRfDeveloperGuide.LegacyShaders which can render meshes in a variety of ways. All of them use the same GVRMaterial object but with different properties.  In this example we construct a GVRMaterial to be used with the phong shader. This shader requires us to provide a diffuse texture as well as material and lighting properties.  GVRMaterial   createMaterial ( GVRContext   gvrContext )  { \n    GVRMaterial   material   =   createMaterial ( gvrContext ); \n    GVRTexture   tex   =   gvrContext . getAssetLoader (). loadTexture ( \n         new   GVRAndroidResource ( gvrContext , R . drawable . gearvr_logo )); \n\n    material . setVec4 ( diffuse_color ,   0.5f ,   0.5f ,   0.5f ,   1.0f ); \n    material . setVec4 ( ambient_color ,   0.2f ,   0.2f ,   0.2f ,   1.0f ); \n    material . setVec4 ( specular_color ,   1.0f ,   1.0f ,   1.0f ,   1.0f ); \n    material . setFloat ( 128.0f ); \n    material . setTexture ( diffuseTexture ,   tex ); \n    return   material ;  }", 
            "title": "Material Construction Example"
        }, 
        {
            "location": "/programming_guide/features/lighting/", 
            "text": "VR lights, classes, shadows, uniforms, and examples\n\n\nLights control the illumination of visible scene objects. Depending on the color, intensity and position of the lights, objects may appear lighter, darker or shadowed. The final color of an object depends on both materials and lighting. Together they provide everything the fragment shader needs to compute the final color. The fragment shader combines the contributions of all the lights in the scene to compute illumination per pixel. At this time, GearVRf does not support vertex lighting.\n\n\nAll lights in the scene are global - they illuminate all the scene objects that have the lighting effect enabled. A light must be attached to a scene object before it can illuminate anything. The scene object determines the position and direction of the light. By default, a light with no transformation points down the positive Z axis towards the viewer.\n\n\nBuilt-in Light Classes\n\n\nThe light's class determines what lighting algorithm is used by the GPU. Three built-in light classes are provided by GearVRF which implement the Phong shading model per pixel. These work together with the Phong surface shader class GVRPhongShader.\n\n\n\n\nGVRDirectLight: A directional light is infinitely far away and illuminates only from a specific direction. This light has color and direction properties.\n\n\nGVRPointLight: A point light illuminates in all directions from a specific position. This light has color and position properties.\n\n\nGVRSpotLight: A spot light illuminates in a cone radiating from a point. It has color, position and direction.\n\n\n\n\nAll lights have an enabled uniform which enables or disables the light. The point and spot lights support attenuation factors which control how the light falls off with distance according to this equation:\n\n\nAttenuation = 1 / (attenuation_constant + attenuation_linear * distance + attenuation_quadratic * distance * distance)\n\n\nThe light object contains the data used by the fragment shader. It is accessed in terms of key / value pairs where the key is a string containing the name of the uniform and the value is a scalar or vector. GearVRF automatically loads these values into the fragment shader uniforms for you.\n\n\nShadows\n\n\nGearVRf can calculate shadow maps for directional and spot lights. You can enable shadow mapping by calling GVRLightBase.setCastShadow with a true parameter. Shadow mapping involves considerable overhead per frame because it renders the scene from the viewpoint of the light to calculate the shadow map. It also uses up more uniform variables. If you disable shadow mapping on all lights, GearVRf will free up all resources used for shadow mapping and return to normal performance.\n\n\nBuilt-in Light Uniforms\n\n\nThis table describes the uniforms used by the built-in Phong lighting implementation.\n\n\n\n\n\n\n\n\nuniform name\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nint\n\n\n1 = light is enabled, 0 = disabled\n\n\n\n\n\n\nworld_position\n\n\nvec3\n\n\nposition of light in world space\n\n\n\n\n\n\nworld_direction\n\n\nvec3\n\n\norientation of light in world space\n\n\n\n\n\n\nspecular_exponent\n\n\nvec4\n\n\ncolor reflected by specular light\n\n\n\n\n\n\nambient_intensity\n\n\nvec4\n\n\nintensity of ambient light\n\n\n\n\n\n\ndiffuse_intensity\n\n\nvec4\n\n\nintensity of diffuse light\n\n\n\n\n\n\nspecular_intensity\n\n\nvec4\n\n\nintensity of specular light\n\n\n\n\n\n\nattenuation_constant\n\n\nfloat\n\n\nconstant attenuation factor\n\n\n\n\n\n\nattenuation_linear\n\n\nvec4\n\n\nlinear attenuation factor\n\n\n\n\n\n\nattenuation_quadratic\n\n\nfloat\n\n\nquadratic attenuation\n\n\n\n\n\n\ninner_cone_angle\n\n\nfloat\n\n\nspotlight inner cone angle\n\n\n\n\n\n\nouter_cone_angle\n\n\nfloat\n\n\nspotlight outer cone angle\n\n\n\n\n\n\n\n\nLight Construction Example\n\n\nA light is a component that is attached to a scene object which gives it both a position and a direction. An individual light can be enabled and disabled programmatically without causing shader compilation. All other light attributes are implementation specific. This example uses the lights built-into GearVRF which implement the Phong lighting model. Here we constructs a red spot light.\n\n\nGVRSpotLight\n \ncreateSpotLight\n(\nGVRContext\n \ngvrContext\n)\n\n\n{\n\n    \nGVRSpotLight\n \nlight\n \n=\n \nnew\n \nGVRSpotLight\n(\ngvrContext\n);\n\n\n    \nlight\n.\nsetDiffuseIntensity\n(\n1\n,\n \n0\n,\n \n0\n);\n\n    \nlight\n.\nsetSpecularIntensity\n(\n1\n,\n \n0\n,\n \n0\n);\n\n    \nlight\n.\nsetInnerCone\n(\n10\n);\n\n    \nlight\n.\nsetOuterCone\n(\n15\n);\n\n    \nreturn\n \nlight\n;\n\n\n}\n\n\n\n\n\n\nYou need to attach your light to a GVRSceneObject before it can illuminate anything. To enable multiple lighting support the GVRPhongShader template must be selected. You can also turn the lighting effect on and off for a particular mesh. In this example we light a sphere with the spot light created above.\n\n\nGVRLightBase\n \nlight\n \n=\n \ncreateSpotLight\n(\ngvrContext\n);\n\n\nGVRSceneObject\n \nlightNode\n \n=\n \nnew\n \nGVRSceneObject\n(\ngvrContext\n);\n\n\nGVRMaterial\n \nmaterial\n \n=\n \nnew\n \nnew\n \nGVRMaterial\n(\ngvrContext\n,\n \nGVRMaterial\n.\nGVRShaderType\n.\nPhong\n.\nID\n);\n\n\nGVRSceneObject\n \nsphereNode\n \n=\n \nnew\n \nGVRSphereSceneObject\n(\ngvrContext\n,\n \nmaterial\n);\n\n\n\nmaterial\n.\nsetVec4\n(\ndiffuse_color\n,\n \n1.0f\n,\n \n0.8f\n,\n \n0.5f\n,\n \n1.0f\n);\n\n\nmaterial\n.\nsetVec4\n(\nspecular_color\n,\n \n1.0\n,\n \n1.0\n,\n \n1.0\n,\n \n1.0f\n);\n\n\nmaterial\n.\nsetFloat\n(\nspecular_exponent\n,\n \n5.0f\n);\n\n\nsphereNode\n.\nattachComponent\n(\nlight\n);\n\n\nGVRRenderData\n \nrdata\n \n=\n \nsphereNode\n.\ngetRenderData\n();\n\n\nrdata\n.\nenableLight\n();", 
            "title": "Lighting"
        }, 
        {
            "location": "/programming_guide/features/lighting/#vr-lights-classes-shadows-uniforms-and-examples", 
            "text": "Lights control the illumination of visible scene objects. Depending on the color, intensity and position of the lights, objects may appear lighter, darker or shadowed. The final color of an object depends on both materials and lighting. Together they provide everything the fragment shader needs to compute the final color. The fragment shader combines the contributions of all the lights in the scene to compute illumination per pixel. At this time, GearVRf does not support vertex lighting.  All lights in the scene are global - they illuminate all the scene objects that have the lighting effect enabled. A light must be attached to a scene object before it can illuminate anything. The scene object determines the position and direction of the light. By default, a light with no transformation points down the positive Z axis towards the viewer.", 
            "title": "VR lights, classes, shadows, uniforms, and examples"
        }, 
        {
            "location": "/programming_guide/features/lighting/#built-in-light-classes", 
            "text": "The light's class determines what lighting algorithm is used by the GPU. Three built-in light classes are provided by GearVRF which implement the Phong shading model per pixel. These work together with the Phong surface shader class GVRPhongShader.   GVRDirectLight: A directional light is infinitely far away and illuminates only from a specific direction. This light has color and direction properties.  GVRPointLight: A point light illuminates in all directions from a specific position. This light has color and position properties.  GVRSpotLight: A spot light illuminates in a cone radiating from a point. It has color, position and direction.   All lights have an enabled uniform which enables or disables the light. The point and spot lights support attenuation factors which control how the light falls off with distance according to this equation:  Attenuation = 1 / (attenuation_constant + attenuation_linear * distance + attenuation_quadratic * distance * distance)  The light object contains the data used by the fragment shader. It is accessed in terms of key / value pairs where the key is a string containing the name of the uniform and the value is a scalar or vector. GearVRF automatically loads these values into the fragment shader uniforms for you.", 
            "title": "Built-in Light Classes"
        }, 
        {
            "location": "/programming_guide/features/lighting/#shadows", 
            "text": "GearVRf can calculate shadow maps for directional and spot lights. You can enable shadow mapping by calling GVRLightBase.setCastShadow with a true parameter. Shadow mapping involves considerable overhead per frame because it renders the scene from the viewpoint of the light to calculate the shadow map. It also uses up more uniform variables. If you disable shadow mapping on all lights, GearVRf will free up all resources used for shadow mapping and return to normal performance.", 
            "title": "Shadows"
        }, 
        {
            "location": "/programming_guide/features/lighting/#built-in-light-uniforms", 
            "text": "This table describes the uniforms used by the built-in Phong lighting implementation.     uniform name  type  description      enabled  int  1 = light is enabled, 0 = disabled    world_position  vec3  position of light in world space    world_direction  vec3  orientation of light in world space    specular_exponent  vec4  color reflected by specular light    ambient_intensity  vec4  intensity of ambient light    diffuse_intensity  vec4  intensity of diffuse light    specular_intensity  vec4  intensity of specular light    attenuation_constant  float  constant attenuation factor    attenuation_linear  vec4  linear attenuation factor    attenuation_quadratic  float  quadratic attenuation    inner_cone_angle  float  spotlight inner cone angle    outer_cone_angle  float  spotlight outer cone angle", 
            "title": "Built-in Light Uniforms"
        }, 
        {
            "location": "/programming_guide/features/lighting/#light-construction-example", 
            "text": "A light is a component that is attached to a scene object which gives it both a position and a direction. An individual light can be enabled and disabled programmatically without causing shader compilation. All other light attributes are implementation specific. This example uses the lights built-into GearVRF which implement the Phong lighting model. Here we constructs a red spot light.  GVRSpotLight   createSpotLight ( GVRContext   gvrContext )  { \n     GVRSpotLight   light   =   new   GVRSpotLight ( gvrContext ); \n\n     light . setDiffuseIntensity ( 1 ,   0 ,   0 ); \n     light . setSpecularIntensity ( 1 ,   0 ,   0 ); \n     light . setInnerCone ( 10 ); \n     light . setOuterCone ( 15 ); \n     return   light ;  }   You need to attach your light to a GVRSceneObject before it can illuminate anything. To enable multiple lighting support the GVRPhongShader template must be selected. You can also turn the lighting effect on and off for a particular mesh. In this example we light a sphere with the spot light created above.  GVRLightBase   light   =   createSpotLight ( gvrContext );  GVRSceneObject   lightNode   =   new   GVRSceneObject ( gvrContext );  GVRMaterial   material   =   new   new   GVRMaterial ( gvrContext ,   GVRMaterial . GVRShaderType . Phong . ID );  GVRSceneObject   sphereNode   =   new   GVRSphereSceneObject ( gvrContext ,   material );  material . setVec4 ( diffuse_color ,   1.0f ,   0.8f ,   0.5f ,   1.0f );  material . setVec4 ( specular_color ,   1.0 ,   1.0 ,   1.0 ,   1.0f );  material . setFloat ( specular_exponent ,   5.0f );  sphereNode . attachComponent ( light );  GVRRenderData   rdata   =   sphereNode . getRenderData ();  rdata . enableLight ();", 
            "title": "Light Construction Example"
        }, 
        {
            "location": "/programming_guide/features/phong_shader_template/", 
            "text": "Phong shader tempates, vertex and fragment shaders, and examples\n\n\nThe phong reflectance model is used to calculate how objects reflect light. This model assumes that reflected light is most intense at an angle perpendicular to the light source and falls off in a lobe based on angle from the viewer. The base surface material is assumed to reflect evenly but texture maps can be used to modify the normal per pixel to provide bumps or control reflection per pixel. \n\n\n\n\nInstead of implementing an extremely complex single shader to handle all of the many combinations of texture maps, lighting and materials, GearVRF supports the concept of shader templates. A shader template is a complex shader with a lot of #ifdef statements which allow it to be compiled in different ways depending on what features are required to render an object. GearVRF will examine your meshes, materials and lights and set the #ifdefs to generate a custom shader for each case.\n\n\nThe asset loader uses the phong shader template for all imported assets. This means that, if you import an asset containing new lights, other objects in your scene may be affected and will use different custom shaders that support the newly added lights. Similarly, importing an object that uses lightmapping if it has not been used before in the scene might cause a new shader to be generated.\n\n\nTo use the phong shader template programmatically, call GVRRenderData.setShaderTemplate(GVRPhongShader.class). This tells GearVRF to defer selecting a specific shader until the scene has been composed. After GVRActivity.onInit completes, the shader templates are used to generate and compile the needed shaders. If you import assets while the application is running, the asset loader will take care of binding the shaders. But if you program creatively and add objects to the scene in other ways, you may have to call GVRScene.bindShaders to make sure the proper shaders are generated.\n\n\nThe phong model separates light into several different types and allows different colors for each. The components are combined with corresponding material uniforms to independently control illumination for each type.\n\n\n\n\nAmbient light reflects uniformly everywhere in the scene and is added to all objects\n\n\nDiffuse light reflects at many angles and is stronger in the direction of the surface normal\n\n\nSpecular light reflects towards the viewer\n\n\n\n\nEach light type has a corresponding color uniform to define the overall object color and a texture sampler to provide a color at each pixel.\n\n\nPhong Shader Example\n\n\n    \nGVRTexture\n \ntex\n \n=\n \ncontext\n.\ngetAssetLoader\n().\nloadTexture\n(\nnew\n \nGVRAndroidResource\n(\nmGVRContext\n,\n \nR\n.\ndrawable\n.\ngearvrflogo\n));\n\n    \nGVRMaterial\n \nmaterial\n \n=\n \nnew\n \nGVRMaterial\n(\ncontext\n,\n \nGVRMaterial\n.\nGVRShaderType\n.\nPhong\n.\nID\n);\n\n    \nGVRSceneObject\n \nplane\n \n=\n \nnew\n \nGVRSceneObject\n(\ncontext\n,\n \n10.0f\n,\n \n4.0f\n,\n\n                                             \nfloat3 a_position float2 a_texcoord float3 a_normal\n,\n \nmaterial\n);\n\n\n    \nmaterial\n.\nsetVec4\n(\ndiffuse_color\n,\n \n0.8f\n,\n \n0.8f\n,\n \n0.8f\n,\n \n1.0f\n);\n\n    \nmaterial\n.\nsetVec4\n(\nambient_color\n,\n \n0.3f\n,\n \n0.3f\n,\n \n0.3f\n,\n \n1.0f\n);\n\n    \nmaterial\n.\nsetVec4\n(\nspecular_color\n,\n \n1.0f\n,\n \n1.0f\n,\n \n1.0f\n,\n \n1.0f\n);\n\n    \nmaterial\n.\nsetVec4\n(\nemissive_color\n,\n \n0.0f\n,\n \n0.0f\n,\n \n0.0f\n,\n \n0.0f\n);\n\n    \nmaterial\n.\nsetFloat\n(\nspecular_exponent\n,\n \n10.0f\n);\n\n    \nmaterial\n.\nsetTexture\n(\ndiffuseTexture\n,\n \ntex\n);\n\n\n\n\n\n\nVertex Shader\n\n\nThe phong vertex shader supports lighting with multiple light sources, normal mapping and skinning with up to 60 bones. Lighting calculations are done per pixel.\n\n\nPhong Shader Vertex Attributes\n\n\n\n\n\n\n\n\nAttribute\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\na_position\n\n\nvec3\n\n\nX, Y, Z position in model space\n\n\n\n\n\n\na_normal\n\n\nvec3\n\n\nnormal vector in model space\n\n\n\n\n\n\na_texcoord\n\n\nvec2\n\n\nfirst U, V texture coordinate set\n\n\n\n\n\n\na_texcoord1\n\n\nvec2\n\n\nsecond U, V texture coordinate set\n\n\n\n\n\n\na_texcoord2\n\n\nvec2\n\n\nthird U, V texture coordinate set\n\n\n\n\n\n\na_texcoord3\n\n\nvec2\n\n\nfourth U, V texture coordinate set\n\n\n\n\n\n\na_tangent\n\n\nvec3\n\n\ntangent for normal mapping\n\n\n\n\n\n\na_bitangent\n\n\nvec3\n\n\nbitangent for normal mapping\n\n\n\n\n\n\na_bone_weights\n\n\nvec4\n\n\nweights for 4 bones for skinning\n\n\n\n\n\n\na_bone_indices\n\n\nivec4\n\n\nbone matrix indices for 4 bones\n\n\n\n\n\n\n\n\nThe vertex shader uses one or more matrices calculated each frame by GearVRF. These matrices are supplied to all shaders so they are available for you to use in your own vertex and fragment shader code.\n\n\nPhong Shader Matrix Uniforms\n\n\n\n\n\n\n\n\nUniform\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nu_model\n\n\nmat4\n\n\nmodel matrix (model -\n world)\n\n\n\n\n\n\nu_view\n\n\nmat4\n\n\nview matrix (world -\n camera)\n\n\n\n\n\n\nu_mvp\n\n\nmat4\n\n\nmodel, view, projection (model -\n screen)\n\n\n\n\n\n\nu_mv\n\n\nmat4\n\n\nmodel, view (model -\n camera)\n\n\n\n\n\n\nu_mv_it\n\n\nmat4\n\n\ninverse transpose of model, view (for lighting)\n\n\n\n\n\n\n\n\nFragment Shader\n\n\nThe fragment shader performs the lighting calculations at each pixel. Shadow mapping is supported for multiple light sources but should be used sparingly because it is computationally expensive. It renders the scene from the viewpoint of each light that casts shadows.\n\n\nMany different types of texture maps are supporting by the phong fragment shader template but usually a scene object only uses one or two. Each texture map contributes differently to the overall color and reacts differently to the lighting in the scene.\n\n\nPhong Texture Maps\n\n\n\n\n\n\n\n\nSampler\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndiffuseTexture\n\n\nSupplies diffuse color per pixel\n\n\n\n\n\n\nambientTexture\n\n\nSupplies ambient color per pixel\n\n\n\n\n\n\nopacityTexture\n\n\nSupplies alpha per pixel\n\n\n\n\n\n\nspecularTexture\n\n\nSupplies specular color per pixel\n\n\n\n\n\n\nemissiveTexture\n\n\nSupplies emissive color per pixel\n\n\n\n\n\n\nnormalTexture\n\n\nSupplies normal per pixel\n\n\n\n\n\n\nlightmapTexture\n\n\nSupplies lighting per pixel\n\n\n\n\n\n\n\n\nMaterials used with the phong shader template support these uniforms. Each different type of light has its own set of uniforms used to define the light properties.\n\n\nPhong Material Uniforms\n\n\n\n\n\n\n\n\nUniform\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndiffuse_color\n\n\nvec4\n\n\ncolor reflected by diffuse light, alpha component has overall opacity\n\n\n\n\n\n\nambient_color\n\n\nvec4\n\n\ncolor reflected by ambient light\n\n\n\n\n\n\nspecular_color\n\n\nvec4\n\n\ncolor reflected by specular light (towards viewer)\n\n\n\n\n\n\nemissive_color\n\n\nvec4\n\n\nlight color emitted by object\n\n\n\n\n\n\nspecular_exponent\n\n\nfloat\n\n\nspecular exponent (shininess)\n\n\n\n\n\n\nu_lightmap_offset\n\n\nvec2\n\n\ntexture coordinate offset for lightmap texture\n\n\n\n\n\n\nu_lightmap_scale\n\n\nvec2\n\n\ntexture coordinate scale for lightmap texture", 
            "title": "Phong Shader Template"
        }, 
        {
            "location": "/programming_guide/features/phong_shader_template/#phong-shader-tempates-vertex-and-fragment-shaders-and-examples", 
            "text": "The phong reflectance model is used to calculate how objects reflect light. This model assumes that reflected light is most intense at an angle perpendicular to the light source and falls off in a lobe based on angle from the viewer. The base surface material is assumed to reflect evenly but texture maps can be used to modify the normal per pixel to provide bumps or control reflection per pixel.    Instead of implementing an extremely complex single shader to handle all of the many combinations of texture maps, lighting and materials, GearVRF supports the concept of shader templates. A shader template is a complex shader with a lot of #ifdef statements which allow it to be compiled in different ways depending on what features are required to render an object. GearVRF will examine your meshes, materials and lights and set the #ifdefs to generate a custom shader for each case.  The asset loader uses the phong shader template for all imported assets. This means that, if you import an asset containing new lights, other objects in your scene may be affected and will use different custom shaders that support the newly added lights. Similarly, importing an object that uses lightmapping if it has not been used before in the scene might cause a new shader to be generated.  To use the phong shader template programmatically, call GVRRenderData.setShaderTemplate(GVRPhongShader.class). This tells GearVRF to defer selecting a specific shader until the scene has been composed. After GVRActivity.onInit completes, the shader templates are used to generate and compile the needed shaders. If you import assets while the application is running, the asset loader will take care of binding the shaders. But if you program creatively and add objects to the scene in other ways, you may have to call GVRScene.bindShaders to make sure the proper shaders are generated.  The phong model separates light into several different types and allows different colors for each. The components are combined with corresponding material uniforms to independently control illumination for each type.   Ambient light reflects uniformly everywhere in the scene and is added to all objects  Diffuse light reflects at many angles and is stronger in the direction of the surface normal  Specular light reflects towards the viewer   Each light type has a corresponding color uniform to define the overall object color and a texture sampler to provide a color at each pixel.", 
            "title": "Phong shader tempates, vertex and fragment shaders, and examples"
        }, 
        {
            "location": "/programming_guide/features/phong_shader_template/#phong-shader-example", 
            "text": "GVRTexture   tex   =   context . getAssetLoader (). loadTexture ( new   GVRAndroidResource ( mGVRContext ,   R . drawable . gearvrflogo )); \n     GVRMaterial   material   =   new   GVRMaterial ( context ,   GVRMaterial . GVRShaderType . Phong . ID ); \n     GVRSceneObject   plane   =   new   GVRSceneObject ( context ,   10.0f ,   4.0f , \n                                              float3 a_position float2 a_texcoord float3 a_normal ,   material ); \n\n     material . setVec4 ( diffuse_color ,   0.8f ,   0.8f ,   0.8f ,   1.0f ); \n     material . setVec4 ( ambient_color ,   0.3f ,   0.3f ,   0.3f ,   1.0f ); \n     material . setVec4 ( specular_color ,   1.0f ,   1.0f ,   1.0f ,   1.0f ); \n     material . setVec4 ( emissive_color ,   0.0f ,   0.0f ,   0.0f ,   0.0f ); \n     material . setFloat ( specular_exponent ,   10.0f ); \n     material . setTexture ( diffuseTexture ,   tex );", 
            "title": "Phong Shader Example"
        }, 
        {
            "location": "/programming_guide/features/phong_shader_template/#vertex-shader", 
            "text": "The phong vertex shader supports lighting with multiple light sources, normal mapping and skinning with up to 60 bones. Lighting calculations are done per pixel.  Phong Shader Vertex Attributes     Attribute  Type  Description      a_position  vec3  X, Y, Z position in model space    a_normal  vec3  normal vector in model space    a_texcoord  vec2  first U, V texture coordinate set    a_texcoord1  vec2  second U, V texture coordinate set    a_texcoord2  vec2  third U, V texture coordinate set    a_texcoord3  vec2  fourth U, V texture coordinate set    a_tangent  vec3  tangent for normal mapping    a_bitangent  vec3  bitangent for normal mapping    a_bone_weights  vec4  weights for 4 bones for skinning    a_bone_indices  ivec4  bone matrix indices for 4 bones     The vertex shader uses one or more matrices calculated each frame by GearVRF. These matrices are supplied to all shaders so they are available for you to use in your own vertex and fragment shader code.  Phong Shader Matrix Uniforms     Uniform  Type  Description      u_model  mat4  model matrix (model -  world)    u_view  mat4  view matrix (world -  camera)    u_mvp  mat4  model, view, projection (model -  screen)    u_mv  mat4  model, view (model -  camera)    u_mv_it  mat4  inverse transpose of model, view (for lighting)", 
            "title": "Vertex Shader"
        }, 
        {
            "location": "/programming_guide/features/phong_shader_template/#fragment-shader", 
            "text": "The fragment shader performs the lighting calculations at each pixel. Shadow mapping is supported for multiple light sources but should be used sparingly because it is computationally expensive. It renders the scene from the viewpoint of each light that casts shadows.  Many different types of texture maps are supporting by the phong fragment shader template but usually a scene object only uses one or two. Each texture map contributes differently to the overall color and reacts differently to the lighting in the scene.  Phong Texture Maps     Sampler  Description      diffuseTexture  Supplies diffuse color per pixel    ambientTexture  Supplies ambient color per pixel    opacityTexture  Supplies alpha per pixel    specularTexture  Supplies specular color per pixel    emissiveTexture  Supplies emissive color per pixel    normalTexture  Supplies normal per pixel    lightmapTexture  Supplies lighting per pixel     Materials used with the phong shader template support these uniforms. Each different type of light has its own set of uniforms used to define the light properties.  Phong Material Uniforms     Uniform  Type  Description      diffuse_color  vec4  color reflected by diffuse light, alpha component has overall opacity    ambient_color  vec4  color reflected by ambient light    specular_color  vec4  color reflected by specular light (towards viewer)    emissive_color  vec4  light color emitted by object    specular_exponent  float  specular exponent (shininess)    u_lightmap_offset  vec2  texture coordinate offset for lightmap texture    u_lightmap_scale  vec2  texture coordinate scale for lightmap texture", 
            "title": "Fragment Shader"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/", 
            "text": "GearVRf provides a set of built-in shaders for you to use in your applications. Although you can write custom shaders, for many applications these built-in shaders will be all that you need.\n\n\nGVRMaterial.GVRShaderType.Phong\n\n\nThe Phong shader implements the phong lighting model for multiple light sources. All of the models imported by the asset loader use the phong shader so they will automatically respond to the light sources in the scene. This shader maps multiple textures onto a mesh illuminated by any light sources in the scene. Although it supports many different uniforms, you will typically only use a few of them.\n\n\nDepending on which uniforms you use and the format of your mesh, GearVRf will generate a custom native shader optimized for your specific usage. Adding or removing a light source from the scene can cause a new native shader to be recompiled.\n\n\n\n\n\n\n\n\nuniform\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nambientTexture\n\n\nsampler2D\n\n\nambient texture\n\n\n\n\n\n\ndiffuseTexture\n\n\nsampler2D\n\n\ndiffuse texture\n\n\n\n\n\n\nspecularTexture\n\n\nsampler2D\n\n\nspecular texture\n\n\n\n\n\n\nopacityTexture\n\n\nsampler2D\n\n\nopacity texture (alpha channel)\n\n\n\n\n\n\nnormalTexture\n\n\nsampler2D\n\n\nnormal texture\n\n\n\n\n\n\nemissiveTexture\n\n\nsampler2D\n\n\nemissive texture\n\n\n\n\n\n\nlightmapTexture\n\n\nsampler2D\n\n\nlightmap texture\n\n\n\n\n\n\nambient_color\n\n\nvec4\n\n\nambient color\n\n\n\n\n\n\ndiffuse_color\n\n\nvec4\n\n\ndiffuse color\n\n\n\n\n\n\nspecular_color\n\n\nvec4\n\n\nspecular color\n\n\n\n\n\n\nemissive_color\n\n\nvec4\n\n\nemissive color\n\n\n\n\n\n\nspecular_exponent\n\n\nfloat\n\n\nspecular exponent\n\n\n\n\n\n\n\n\nGVRMaterial.GVRShaderType.PhongLayered\n\n\nThis shader performs the same computations as the Phong shader but allows layering of ambient, diffuse, specular, opacity and emissive texture maps. This shader supports up to two textures of each type and blends them at run time. It is primarily used for supporting FBX files with layered textures. Unless your application needs this functionality, the Phong shader is more efficient.\n\n\nThe following blend operations are supported:\n\n\n\n\n\n\n\n\nvalue\n\n\noperation\n\n\n\n\n\n\n\n\n\n\n0\n\n\nmultiply\n\n\n\n\n\n\n1\n\n\nadd\n\n\n\n\n\n\n2\n\n\nsubtract\n\n\n\n\n\n\n3\n\n\ndivice\n\n\n\n\n\n\n4\n\n\nsmooth add\n\n\n\n\n\n\n5\n\n\nsigned add\n\n\n\n\n\n\n\n\nThis table gives the additional uniforms for the Phong Layered shader. It supports all of the uniforms for the Phong shader as well.\n\n\n\n\n\n\n\n\nuniform\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nambientTexture1\n\n\nsampler2D\n\n\nsecond ambient texture\n\n\n\n\n\n\ndiffuseTexture1\n\n\nsampler2D\n\n\nsecond diffuse texture\n\n\n\n\n\n\nspecularTexture1\n\n\nsampler2D\n\n\nsecond specular texture\n\n\n\n\n\n\nopacityTexture1\n\n\nsampler2D\n\n\nsecond opacity texture\n\n\n\n\n\n\nemissiveTexture1\n\n\nsampler2D\n\n\nsecond emissive texture\n\n\n\n\n\n\nlightmapTexture1\n\n\nsampler2D\n\n\nsecond lightmap texture\n\n\n\n\n\n\nambient_color\n\n\nvec4\n\n\nambient color\n\n\n\n\n\n\ndiffuse_color\n\n\nvec4\n\n\ndiffuse color\n\n\n\n\n\n\nspecular_color\n\n\nvec4\n\n\nspecular color\n\n\n\n\n\n\nemissive_color\n\n\nvec4\n\n\nemissive color\n\n\n\n\n\n\nspecular_exponent\n\n\nfloat\n\n\nspecular exponent\n\n\n\n\n\n\nambientTexture1_blendop\n\n\nint\n\n\nambient texture blend operation\n\n\n\n\n\n\ndiffuseTexture1_blendop\n\n\nint\n\n\ndiffuse texture blend operation\n\n\n\n\n\n\nspecularTexture1_blendop\n\n\nint\n\n\nspecular texture blend operation\n\n\n\n\n\n\nopacityTexture1_blendop\n\n\nint\n\n\nopacity texture blend operation\n\n\n\n\n\n\nemissiveTexture1_blendop\n\n\nint\n\n\nemissive texture blend operation\n\n\n\n\n\n\nlightmapTexture1_blendop\n\n\nint\n\n\nlightmap texture blend operation\n\n\n\n\n\n\n\n\nGVRMaterial.GVRShaderType.UnlitHorizontalStereo\n\n\nDisplays a single 2D texture across the framebuffer horizontally for both eyes. The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The u_right parameter controls whether it displays on the left half or the right half of the output display.\n\n\n\n\n\n\n\n\nuniform\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nu_texture\n\n\nsampler2D\n\n\ndiffuse texture\n\n\n\n\n\n\nu_color\n\n\nvec3\n\n\nRGB diffuse color\n\n\n\n\n\n\nu_opacity\n\n\nfloat\n\n\nalpha for transparency\n\n\n\n\n\n\nu_right\n\n\nint\n\n\n1 = right eye, 0 = left\n\n\n\n\n\n\n\n\nGVRMaterial.GVRShaderType.UnlitVertictalStereo\n\n\nDisplays a single 2D texture across the framebuffer vertically for both eyes.  The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The u_right parameter controls whether it displays on the top half or the bottom half of the output display.\n\n\n\n\n\n\n\n\nuniform\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nu_texture\n\n\nsampler2D\n\n\ndiffuse texture\n\n\n\n\n\n\nu_color\n\n\nvec3\n\n\nRGB diffuse color\n\n\n\n\n\n\nu_opacity\n\n\nfloat\n\n\nalpha for transparency\n\n\n\n\n\n\nu_right\n\n\nint\n\n\n1 = right eye, 0 = left\n\n\n\n\n\n\n\n\nGVRMaterial.GVRShaderType.OES\n\n\nMaps an external 2D texture onto the mesh. The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The texture supplied as u_ texture must be and OES external texture (type GL_TEXTURE_EXTERNAL_OES, not GL_TEXTURE_2D) as this shader uses samplerExternalOES as opposed to sampler2D.\n\n\n\n\n\n\n\n\nuniform\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nu_texture\n\n\nsampler2D\n\n\ndiffuse texture\n\n\n\n\n\n\nu_color\n\n\nvec3\n\n\nRGB diffuse color\n\n\n\n\n\n\nu_opacity\n\n\nfloat\n\n\nalpha for transparency\n\n\n\n\n\n\nu_right\n\n\nint\n\n\ndescription\n\n\n\n\n\n\n\n\nGVRMaterial.GVRShaderType.OESHorizontalStereo\n\n\nDisplays a single external 2D texture across the framebuffer vertically for both eyes. The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The texture supplied as u_ texture must be and OES external texture (type GL_TEXTURE_EXTERNAL_OES, not GL_TEXTURE_2D) as this shader uses samplerExternalOES as opposed to sampler2D.The u_right parameter controls whether it displays on the left half or the right half of the output display.\n\n\n\n\n\n\n\n\nuniform\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nu_texture\n\n\nsampler2D\n\n\ndiffuse texture\n\n\n\n\n\n\nu_color\n\n\nvec3\n\n\nRGB diffuse color\n\n\n\n\n\n\nu_opacity\n\n\nfloat\n\n\nalpha for transparency\n\n\n\n\n\n\nu_right\n\n\nint\n\n\n1 = right eye, 0 = left\n\n\n\n\n\n\n\n\nGVRMaterial.GVRShaderType.OESVerticalStereo\n\n\nDisplays a single external 2D texture across the framebuffer vertically for both eyes. The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The texture supplied as u_ texture must be and OES external texture (type GL_TEXTURE_EXTERNAL_OES, not GL_TEXTURE_2D) as this shader uses samplerExternalOES as opposed to sampler2D. The u_right parameter controls whether it displays on the top half or the bottom half of the output display.\n\n\n\n\n\n\n\n\nuniform\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nu_texture\n\n\nsampler2D\n\n\ndiffuse texture\n\n\n\n\n\n\nu_color\n\n\nvec3\n\n\nRGB diffuse color\n\n\n\n\n\n\nu_opacity\n\n\nfloat\n\n\nalpha for transparency\n\n\n\n\n\n\nu_right\n\n\nint\n\n\n1 = right eye, 0 = left\n\n\n\n\n\n\n\n\nGVRMaterial.GVRShaderType.Cubemap\n\n\nMaps a cubemap texture onto the mesh. The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The diffuse texture must be a cube map texture (six different textures for each face of the cube).\n\n\n\n\n\n\n\n\nuniform\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nu_texture\n\n\nsampler2D\n\n\ndiffuse texture\n\n\n\n\n\n\nu_color\n\n\nvec3\n\n\nRGB diffuse color\n\n\n\n\n\n\nu_opacity\n\n\nfloat\n\n\nalpha for transparency\n\n\n\n\n\n\nu_right\n\n\nint\n\n\ndescription\n\n\n\n\n\n\n\n\nGVRMaterial.GVRShaderType.CubemapReflection\n\n\nWraps a cubemap texture around a mesh as a reflection map which varies with the viewpoint. The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The diffuse texture must be a cube map texture (six different textures for each face of the cube).\n\n\n\n\n\n\n\n\nuniform\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nu_texture\n\n\nsampler2D\n\n\ndiffuse texture\n\n\n\n\n\n\nu_color\n\n\nvec3\n\n\nRGB diffuse color\n\n\n\n\n\n\nu_opacity\n\n\nfloat\n\n\nalpha for transparency\n\n\n\n\n\n\nu_view_i\n\n\nmat4atrix\n\n\nview matrix\n\n\n\n\n\n\nv_viewspace_position\n\n\nvec3\n\n\nview space position\n\n\n\n\n\n\nv_viewspace_normal\n\n\nvec3\n\n\nview space normal\n\n\n\n\n\n\n\n\nGVRMaterial.GVRShaderType.Texture\n\n\nMaps a single 2D texture onto a mesh with light sources. The computed fragment color is the product of the diffuse texture, diffuse color and opacity as illuminated the lights in the scene. It requires the vertex to have positions, normals and texture coordinates. If the scene is not lit, the material and light intensity properties are ignored. GVRPhongShader provides the same functionality and supports multiple layered textures.\n\n\n\n\n\n\n\n\nuniform\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nu_texture\n\n\nsampler2D\n\n\ndiffuse texture\n\n\n\n\n\n\nu_color\n\n\nvec3\n\n\nRGB diffuse color\n\n\n\n\n\n\nu_opacity\n\n\nfloat\n\n\nalpha for transparency\n\n\n\n\n\n\nambient_color\n\n\nvec4\n\n\ncolor reflected by ambient light\n\n\n\n\n\n\ndiffuse_color\n\n\nvec4\n\n\ncolor reflected by diffuse light\n\n\n\n\n\n\nspecular_color\n\n\nfloat\n\n\nexponent for specular reflection\n\n\n\n\n\n\nspecular_exponent\n\n\nvec4\n\n\ncolor reflected by specular light\n\n\n\n\n\n\nambient_intensity\n\n\nvec4\n\n\nintensity of ambient light\n\n\n\n\n\n\ndiffuse_intensity\n\n\nvec4\n\n\nintensity of diffuse light\n\n\n\n\n\n\nspecular_intensity\n\n\nvec4\n\n\nintensity of specular light\n\n\n\n\n\n\n\n\nGVRMaterial.GVRShaderType.Lightmap\n\n\nMaps a lightmap texture onto a mesh. The computed fragment color is the product of the diffuse texture, diffuse color and opacity as illuminated by a light map. It requires the vertex to have positions, normals and texture coordinates. GVRPhongShader supports light mapping integrated with other surface shading capabilities.\n\n\n\n\n\n\n\n\nuniform\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nu_main_texture\n\n\nsampler2D\n\n\ndiffuse texture\n\n\n\n\n\n\nu_lightmap_texture\n\n\nsampler2D\n\n\nlight map texture\n\n\n\n\n\n\nu_lightmap_offset\n\n\nvec2\n\n\nlight map offset\n\n\n\n\n\n\nu_lightmap_scale\n\n\nvec2\n\n\nlight map scal", 
            "title": "Built-In Legacy Shader"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/#gvrmaterialgvrshadertypephong", 
            "text": "The Phong shader implements the phong lighting model for multiple light sources. All of the models imported by the asset loader use the phong shader so they will automatically respond to the light sources in the scene. This shader maps multiple textures onto a mesh illuminated by any light sources in the scene. Although it supports many different uniforms, you will typically only use a few of them.  Depending on which uniforms you use and the format of your mesh, GearVRf will generate a custom native shader optimized for your specific usage. Adding or removing a light source from the scene can cause a new native shader to be recompiled.     uniform  type  description      ambientTexture  sampler2D  ambient texture    diffuseTexture  sampler2D  diffuse texture    specularTexture  sampler2D  specular texture    opacityTexture  sampler2D  opacity texture (alpha channel)    normalTexture  sampler2D  normal texture    emissiveTexture  sampler2D  emissive texture    lightmapTexture  sampler2D  lightmap texture    ambient_color  vec4  ambient color    diffuse_color  vec4  diffuse color    specular_color  vec4  specular color    emissive_color  vec4  emissive color    specular_exponent  float  specular exponent", 
            "title": "GVRMaterial.GVRShaderType.Phong"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/#gvrmaterialgvrshadertypephonglayered", 
            "text": "This shader performs the same computations as the Phong shader but allows layering of ambient, diffuse, specular, opacity and emissive texture maps. This shader supports up to two textures of each type and blends them at run time. It is primarily used for supporting FBX files with layered textures. Unless your application needs this functionality, the Phong shader is more efficient.  The following blend operations are supported:     value  operation      0  multiply    1  add    2  subtract    3  divice    4  smooth add    5  signed add     This table gives the additional uniforms for the Phong Layered shader. It supports all of the uniforms for the Phong shader as well.     uniform  type  description      ambientTexture1  sampler2D  second ambient texture    diffuseTexture1  sampler2D  second diffuse texture    specularTexture1  sampler2D  second specular texture    opacityTexture1  sampler2D  second opacity texture    emissiveTexture1  sampler2D  second emissive texture    lightmapTexture1  sampler2D  second lightmap texture    ambient_color  vec4  ambient color    diffuse_color  vec4  diffuse color    specular_color  vec4  specular color    emissive_color  vec4  emissive color    specular_exponent  float  specular exponent    ambientTexture1_blendop  int  ambient texture blend operation    diffuseTexture1_blendop  int  diffuse texture blend operation    specularTexture1_blendop  int  specular texture blend operation    opacityTexture1_blendop  int  opacity texture blend operation    emissiveTexture1_blendop  int  emissive texture blend operation    lightmapTexture1_blendop  int  lightmap texture blend operation", 
            "title": "GVRMaterial.GVRShaderType.PhongLayered"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/#gvrmaterialgvrshadertypeunlithorizontalstereo", 
            "text": "Displays a single 2D texture across the framebuffer horizontally for both eyes. The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The u_right parameter controls whether it displays on the left half or the right half of the output display.     uniform  type  description      u_texture  sampler2D  diffuse texture    u_color  vec3  RGB diffuse color    u_opacity  float  alpha for transparency    u_right  int  1 = right eye, 0 = left", 
            "title": "GVRMaterial.GVRShaderType.UnlitHorizontalStereo"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/#gvrmaterialgvrshadertypeunlitvertictalstereo", 
            "text": "Displays a single 2D texture across the framebuffer vertically for both eyes.  The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The u_right parameter controls whether it displays on the top half or the bottom half of the output display.     uniform  type  description      u_texture  sampler2D  diffuse texture    u_color  vec3  RGB diffuse color    u_opacity  float  alpha for transparency    u_right  int  1 = right eye, 0 = left", 
            "title": "GVRMaterial.GVRShaderType.UnlitVertictalStereo"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/#gvrmaterialgvrshadertypeoes", 
            "text": "Maps an external 2D texture onto the mesh. The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The texture supplied as u_ texture must be and OES external texture (type GL_TEXTURE_EXTERNAL_OES, not GL_TEXTURE_2D) as this shader uses samplerExternalOES as opposed to sampler2D.     uniform  type  description      u_texture  sampler2D  diffuse texture    u_color  vec3  RGB diffuse color    u_opacity  float  alpha for transparency    u_right  int  description", 
            "title": "GVRMaterial.GVRShaderType.OES"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/#gvrmaterialgvrshadertypeoeshorizontalstereo", 
            "text": "Displays a single external 2D texture across the framebuffer vertically for both eyes. The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The texture supplied as u_ texture must be and OES external texture (type GL_TEXTURE_EXTERNAL_OES, not GL_TEXTURE_2D) as this shader uses samplerExternalOES as opposed to sampler2D.The u_right parameter controls whether it displays on the left half or the right half of the output display.     uniform  type  description      u_texture  sampler2D  diffuse texture    u_color  vec3  RGB diffuse color    u_opacity  float  alpha for transparency    u_right  int  1 = right eye, 0 = left", 
            "title": "GVRMaterial.GVRShaderType.OESHorizontalStereo"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/#gvrmaterialgvrshadertypeoesverticalstereo", 
            "text": "Displays a single external 2D texture across the framebuffer vertically for both eyes. The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The texture supplied as u_ texture must be and OES external texture (type GL_TEXTURE_EXTERNAL_OES, not GL_TEXTURE_2D) as this shader uses samplerExternalOES as opposed to sampler2D. The u_right parameter controls whether it displays on the top half or the bottom half of the output display.     uniform  type  description      u_texture  sampler2D  diffuse texture    u_color  vec3  RGB diffuse color    u_opacity  float  alpha for transparency    u_right  int  1 = right eye, 0 = left", 
            "title": "GVRMaterial.GVRShaderType.OESVerticalStereo"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/#gvrmaterialgvrshadertypecubemap", 
            "text": "Maps a cubemap texture onto the mesh. The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The diffuse texture must be a cube map texture (six different textures for each face of the cube).     uniform  type  description      u_texture  sampler2D  diffuse texture    u_color  vec3  RGB diffuse color    u_opacity  float  alpha for transparency    u_right  int  description", 
            "title": "GVRMaterial.GVRShaderType.Cubemap"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/#gvrmaterialgvrshadertypecubemapreflection", 
            "text": "Wraps a cubemap texture around a mesh as a reflection map which varies with the viewpoint. The computed fragment color is the product of the diffuse texture, diffuse color and opacity. It requires the vertex to have positions and texture coordinates. Normals are not required because lights in the scene are not used by this shader. The diffuse texture must be a cube map texture (six different textures for each face of the cube).     uniform  type  description      u_texture  sampler2D  diffuse texture    u_color  vec3  RGB diffuse color    u_opacity  float  alpha for transparency    u_view_i  mat4atrix  view matrix    v_viewspace_position  vec3  view space position    v_viewspace_normal  vec3  view space normal", 
            "title": "GVRMaterial.GVRShaderType.CubemapReflection"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/#gvrmaterialgvrshadertypetexture", 
            "text": "Maps a single 2D texture onto a mesh with light sources. The computed fragment color is the product of the diffuse texture, diffuse color and opacity as illuminated the lights in the scene. It requires the vertex to have positions, normals and texture coordinates. If the scene is not lit, the material and light intensity properties are ignored. GVRPhongShader provides the same functionality and supports multiple layered textures.     uniform  type  description      u_texture  sampler2D  diffuse texture    u_color  vec3  RGB diffuse color    u_opacity  float  alpha for transparency    ambient_color  vec4  color reflected by ambient light    diffuse_color  vec4  color reflected by diffuse light    specular_color  float  exponent for specular reflection    specular_exponent  vec4  color reflected by specular light    ambient_intensity  vec4  intensity of ambient light    diffuse_intensity  vec4  intensity of diffuse light    specular_intensity  vec4  intensity of specular light", 
            "title": "GVRMaterial.GVRShaderType.Texture"
        }, 
        {
            "location": "/programming_guide/features/builtin_legacy_shader/#gvrmaterialgvrshadertypelightmap", 
            "text": "Maps a lightmap texture onto a mesh. The computed fragment color is the product of the diffuse texture, diffuse color and opacity as illuminated by a light map. It requires the vertex to have positions, normals and texture coordinates. GVRPhongShader supports light mapping integrated with other surface shading capabilities.     uniform  type  description      u_main_texture  sampler2D  diffuse texture    u_lightmap_texture  sampler2D  light map texture    u_lightmap_offset  vec2  light map offset    u_lightmap_scale  vec2  light map scal", 
            "title": "GVRMaterial.GVRShaderType.Lightmap"
        }, 
        {
            "location": "/programming_guide/features/picking/", 
            "text": "3D object picking, colliders, and examples\n\n\nFor a scene object to be pickable, it must have a collider component attached. The collider typically references collision geometry that is simpler than the scene object's mesh. For example, the collider might be a sphere or an axially aligned bounding box.\n\n\nTo pick a 3D object GearVRf casts a ray from the camera viewpoint in the direction the viewer is looking through the entire scene looking only at the geometry in the colliders. When the ray penetrates the collider geometry, the scene object that owns it is \"picked\". The list of picked objects is sorted based on distance from the camera so it is easy to choose the closest object to the viewer.\n\n\n\n\nTypes of Colliders\n\n\nGearVRf provides several types of colliders to use depending on how accurate you want picking to be.\n\n\n\n\nGVRSphereCollider\n is the fastest collision to compute but is the least accurate because it approximates the shape of the scene object as spherical. For meshes that are larger in one dimension than another, the picker might register false positives.\n\n\nGVRBoxCollider\n approximates the scene object shape as a rectangular box. It is slightly less efficient than the sphere collider but may bound the mesh more tightly.\n\n\nGVRMeshCollider\n can be used in several ways. You can direct it to use the mesh of the scene object that owns it or you can provide your own collision mesh. You can also request the mesh collider to use the bounding box of the scene object's mesh. This is usually a lot faster and sufficient for a lot of picking needs. It accommodates irregularly shaped objects better than the sphere collider.\n\n\n\n\nPicking\n\n\nThe picking operation is performed by the GVRPicker class. The picker can operate in two modes. You can call the \nGVRPicker.pickObjects\n function directly to get back the list of objects that were picked and information about the collision. You can also attach the picker to a scene object and it will automatically cast a ray from that scene object and generate events indicating what was picked.\n\n\nProcedural Picking\n\n\nTo use the picker procedurally you must provide the origin and direction of the pick ray in world coordinates and the GVRScene you want to pick against. The picker returns an array of GVRPickedObject instances that indicate what was picked and where it was hit. The hit position returned will be in the coordinate system of the collider geometry - not in world coordinates. To transform it to world coordinates you must multiply it by the model matrix of the scene object hit. This is not the most efficient method of picking and should only be called once per frame.\n\n\nFor mesh colliders, you can enable coordinate picking in the constructor and the picker will provide additional information with the barycentric coordinates, texture coordinates and normal at the hit location.\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nhitObject\n\n\nGVRSceneObject\n\n\nthe scene object that was hit\n\n\n\n\n\n\nhitCollider\n\n\nGVRCollider\n\n\nthe collider that was hit\n\n\n\n\n\n\nhitPosition\n\n\nfloat[3]\n\n\nX, Y, Z coordinates of where collider was hit\n\n\n\n\n\n\nhitDistance\n\n\nfloat\n\n\ndistance from camera in world coordinates\n\n\n\n\n\n\nfaceIndex*\n\n\nint\n\n\nindex of face hit\n\n\n\n\n\n\nbarycentricCoordinates*\n\n\nfloat[3]\n\n\nbarycentric coordinates of the hit location on the collided face\n\n\n\n\n\n\ntextureCoordinates*\n\n\nfloat[2]\n\n\nU,V coordinates of the hit location on the mesh\n\n\n\n\n\n\nnormalCoordinates*\n\n\nfloat[3]\n\n\nnormalized surface normal at the hit location\n\n\n\n\n\n\n\n\nPicking Events\n\n\nThe most convenient way to use the picker is to attach it to a scene object, typically the owner of the current camera, and respond to the pick events generated when objects are hit. Events are raised each time the picking ray enters or exits an object. You can also observe changes to the list of picked objects as a whole.\n\n\nTo handle pick events in your application you provide a class which implements the IPickEvents interface and attach it as a listener to the scene's event receiver. (The picker routes all pick events through the scene.)\n\n\nPicking Example\n\n\nThis example shows how to use picking events to do selection highlighting. When the pick ray enters an object, its material color is changed to red. When the pick ray exists, the color is changed back to white again.\n\n\npublic\n \nclass\n \nPickHandler\n \nimplements\n \nIPickEvents\n\n\n{\n\n    \npublic\n \nvoid\n \nonEnter\n(\nGVRSceneObject\n \nsceneObj\n,\n \nGVRPicker\n.\nGVRPickedObject\n \npickInfo\n)\n\n    \n{\n\n         \nsceneObj\n.\ngetRenderData\n().\ngetMaterial\n().\nsetDiffuseColor\n(\n1\n,\n \n0\n,\n \n0\n,\n \n1\n);\n\n    \n}\n\n    \npublic\n \nvoid\n \nonExit\n(\nGVRSceneObject\n \nsceneObj\n,\n \nGVRPicker\n.\nGVRPickedObject\n \npickInfo\n)\n\n    \n{\n\n        \nsceneObj\n.\ngetRenderData\n().\ngetMaterial\n().\nsetDiffuseColor\n(\n1\n,\n \n1\n,\n \n1\n,\n \n1\n);\n\n    \n}\n\n    \npublic\n \nvoid\n \nonInside\n(\nGVRSceneObject\n \nsceneObj\n,\n \nGVRPicker\n.\nGVRPickedObject\n \npickInfo\n)\n \n{\n \n}\n\n    \npublic\n \nvoid\n \nonPick\n(\nGVRPicker\n)\n \n{\n \n}\n\n    \npublic\n \nvoid\n \nonNoPick\n(\nGVRPicker\n)\n \n{\n \n}\n\n\n}\n\n\n\npublic\n \nvoid\n \nonInit\n(\nGVRContext\n \ncontext\n)\n\n\n{\n\n    \nGVRScene\n \nscene\n \n=\n \ncontext\n.\ngetNextMainScene\n();\n\n    \n{\n\n         \nGVRSceneObject\n \nsphere\n \n=\n \nnew\n \nGVRSphereSceneObject\n(\ncontext\n);\n\n         \nscene\n.\ngetEventReceiver\n().\naddListener\n(\nnew\n \nPickHandler\n());\n\n         \nscene\n.\ngetMainCameraRig\n().\ngetOwnerObject\n().\nattachComponent\n(\nnew\n \nGVRPicker\n(\ncontext\n,\n \nscene\n));\n\n         \nsphere\n.\ngetTransform\n().\nsetPositionZ\n(-\n2.0f\n);\n\n         \nsphere\n.\nattachComponent\n(\nnew\n \nGVRSphereCollider\n(\ncontext\n));\n\n         \nscene\n.\naddSceneObject\n(\nsphere\n);\n\n    \n}\n\n\n}", 
            "title": "Picking"
        }, 
        {
            "location": "/programming_guide/features/picking/#3d-object-picking-colliders-and-examples", 
            "text": "For a scene object to be pickable, it must have a collider component attached. The collider typically references collision geometry that is simpler than the scene object's mesh. For example, the collider might be a sphere or an axially aligned bounding box.  To pick a 3D object GearVRf casts a ray from the camera viewpoint in the direction the viewer is looking through the entire scene looking only at the geometry in the colliders. When the ray penetrates the collider geometry, the scene object that owns it is \"picked\". The list of picked objects is sorted based on distance from the camera so it is easy to choose the closest object to the viewer.", 
            "title": "3D object picking, colliders, and examples"
        }, 
        {
            "location": "/programming_guide/features/picking/#types-of-colliders", 
            "text": "GearVRf provides several types of colliders to use depending on how accurate you want picking to be.   GVRSphereCollider  is the fastest collision to compute but is the least accurate because it approximates the shape of the scene object as spherical. For meshes that are larger in one dimension than another, the picker might register false positives.  GVRBoxCollider  approximates the scene object shape as a rectangular box. It is slightly less efficient than the sphere collider but may bound the mesh more tightly.  GVRMeshCollider  can be used in several ways. You can direct it to use the mesh of the scene object that owns it or you can provide your own collision mesh. You can also request the mesh collider to use the bounding box of the scene object's mesh. This is usually a lot faster and sufficient for a lot of picking needs. It accommodates irregularly shaped objects better than the sphere collider.", 
            "title": "Types of Colliders"
        }, 
        {
            "location": "/programming_guide/features/picking/#picking", 
            "text": "The picking operation is performed by the GVRPicker class. The picker can operate in two modes. You can call the  GVRPicker.pickObjects  function directly to get back the list of objects that were picked and information about the collision. You can also attach the picker to a scene object and it will automatically cast a ray from that scene object and generate events indicating what was picked.", 
            "title": "Picking"
        }, 
        {
            "location": "/programming_guide/features/picking/#procedural-picking", 
            "text": "To use the picker procedurally you must provide the origin and direction of the pick ray in world coordinates and the GVRScene you want to pick against. The picker returns an array of GVRPickedObject instances that indicate what was picked and where it was hit. The hit position returned will be in the coordinate system of the collider geometry - not in world coordinates. To transform it to world coordinates you must multiply it by the model matrix of the scene object hit. This is not the most efficient method of picking and should only be called once per frame.  For mesh colliders, you can enable coordinate picking in the constructor and the picker will provide additional information with the barycentric coordinates, texture coordinates and normal at the hit location.     Field  Type  Description      hitObject  GVRSceneObject  the scene object that was hit    hitCollider  GVRCollider  the collider that was hit    hitPosition  float[3]  X, Y, Z coordinates of where collider was hit    hitDistance  float  distance from camera in world coordinates    faceIndex*  int  index of face hit    barycentricCoordinates*  float[3]  barycentric coordinates of the hit location on the collided face    textureCoordinates*  float[2]  U,V coordinates of the hit location on the mesh    normalCoordinates*  float[3]  normalized surface normal at the hit location", 
            "title": "Procedural Picking"
        }, 
        {
            "location": "/programming_guide/features/picking/#picking-events", 
            "text": "The most convenient way to use the picker is to attach it to a scene object, typically the owner of the current camera, and respond to the pick events generated when objects are hit. Events are raised each time the picking ray enters or exits an object. You can also observe changes to the list of picked objects as a whole.  To handle pick events in your application you provide a class which implements the IPickEvents interface and attach it as a listener to the scene's event receiver. (The picker routes all pick events through the scene.)", 
            "title": "Picking Events"
        }, 
        {
            "location": "/programming_guide/features/picking/#picking-example", 
            "text": "This example shows how to use picking events to do selection highlighting. When the pick ray enters an object, its material color is changed to red. When the pick ray exists, the color is changed back to white again.  public   class   PickHandler   implements   IPickEvents  { \n     public   void   onEnter ( GVRSceneObject   sceneObj ,   GVRPicker . GVRPickedObject   pickInfo ) \n     { \n          sceneObj . getRenderData (). getMaterial (). setDiffuseColor ( 1 ,   0 ,   0 ,   1 ); \n     } \n     public   void   onExit ( GVRSceneObject   sceneObj ,   GVRPicker . GVRPickedObject   pickInfo ) \n     { \n         sceneObj . getRenderData (). getMaterial (). setDiffuseColor ( 1 ,   1 ,   1 ,   1 ); \n     } \n     public   void   onInside ( GVRSceneObject   sceneObj ,   GVRPicker . GVRPickedObject   pickInfo )   {   } \n     public   void   onPick ( GVRPicker )   {   } \n     public   void   onNoPick ( GVRPicker )   {   }  }  public   void   onInit ( GVRContext   context )  { \n     GVRScene   scene   =   context . getNextMainScene (); \n     { \n          GVRSceneObject   sphere   =   new   GVRSphereSceneObject ( context ); \n          scene . getEventReceiver (). addListener ( new   PickHandler ()); \n          scene . getMainCameraRig (). getOwnerObject (). attachComponent ( new   GVRPicker ( context ,   scene )); \n          sphere . getTransform (). setPositionZ (- 2.0f ); \n          sphere . attachComponent ( new   GVRSphereCollider ( context )); \n          scene . addSceneObject ( sphere ); \n     }  }", 
            "title": "Picking Example"
        }, 
        {
            "location": "/programming_guide/build_instructions/", 
            "text": "GearVRf app requirements, build instructions, building and running, and sample apps\n\n\nIn order to build and and run GearVRf applications (your own or sample apps) in Android Studio, you have two options:\n\n\n\n\n(Preferred) Download the \nlatest release\n of previously built Gear VR Framework .aar files\n\n\nOptionally, locally build the Gear VR Framework from the latest \nsource code\n\n\n\n\nPrerequisites\n\n\nFor BOTH methods, locally building AND using a pre-built framework, the following prerequisites must be met.\n\n\n1. Install Required SDKs\n\n\nMake sure follow the \nGetting Started Guide\n and install all the required SDKs\n\n\n2. Building Gear VR Framework\n\n\nYou can optionally locally build GearVRf from source code using Android Studio.\n\n\nHere are the steps:\n\n\n\n\nAdd \nOVR_MOBILE_SDK\n to \ngradle.properties\n and set it to the path to the Oculus Mobile SDK; recommended to use the global gradle.properties \n$HOMEPATH/.gradle/gradle.properties\n or \n~/.gradle/gradle.properties)\n.\n\n\nAdd \nANDROID_NDK_HOME\n to gradle.properties and set it to the path to the Android NDK installation.\n\n\nNavigate to the GearVR Framework and select the folder, and click \nOK\n\n\nClick \nMake Project\n (from the \nBuild\n menu)\n\n\n\n\n3. Building and Running GearVRf Applications\n\n\nAfter you have the Gear VR Framework, by either locally building or using a pre-built framework, you can now import, build, and run GearVRf applications (your own or sample apps) in Android Studio. The specific procedure you use depends on whether you locally built the Gear VR Framework from source code files or used pre-built framework files.\n\n\n\n\nWhen building your own GearVRf apps from scratch, copy the appropriate device XML file from the GearVRf SDK to your application's assets folder. \nGearVRf provides an xml file for you to use: gvr.xml.\n\n\nImport the GearVRf application code.\n\n\nClick \nFile -\n Open ...\n\n\nNavigate to the project folder (for example, gvr-simplesample).\n\n\nClick \nOK\n\n\n\n\n\n\nClean and build the application.\n\n\nGo to the \nBuild\n menu and click \nClean\n...\n\n\nClick \nMake Project\n (from the \nBuild\n menu)\n\n\n\n\n\n\nRun the application.\n\n\nConnect an Android mobile device to your local machine.\n\n\nSelect your project in the project explorer\n\n\nClick \nRun\n on the toolbar\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nYou may need to apply to Oculus for a signature file for your device.\nFor application and use details, refer to the online signature generator (\nhttps://dashboard.oculus.com/tools/osig-generator/\n)\n\n\n\n\nGenerate Javadocs\n\n\nWhen locally building the Gear VR Framework, you can optionally generate Javadoc files with details about the GearVRf API.\n\n\nOptional: To get GearVRf API reference details by generating GearVRf Javadoc files in the specified directory:\n\n\n\n\nIn Android Studio, click \nProject \n Generate Javadoc...\n\n\nIn the Generate Javadoc window:\n\n\nJavadoc command\n: (Pathname to the Javadoc executable file) Typically, in the bin directory of the Java directory.\n\n\nClick on the plus-icon to \nexpand the Framework listing\n.\n\n\nCheckmark \nsrc\n\n\nSelect \nUse standard doclet\n\n\nDestination:\n (Convenient local directory to contain the Javadoc files)\n\n\nOptional Specify VM options:\n    NOTE: You may encounter errors if VM options is not specified.\n\n\nClick \nNext \n\n\nClick \nNext \n\n\nVM options: -bootclasspath \npath to your Android jar file\n\n\n\n\n\n\nClick \nFinish", 
            "title": "Build Instructions"
        }, 
        {
            "location": "/programming_guide/build_instructions/#prerequisites", 
            "text": "For BOTH methods, locally building AND using a pre-built framework, the following prerequisites must be met.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/programming_guide/build_instructions/#1-install-required-sdks", 
            "text": "Make sure follow the  Getting Started Guide  and install all the required SDKs", 
            "title": "1. Install Required SDKs"
        }, 
        {
            "location": "/programming_guide/build_instructions/#2-building-gear-vr-framework", 
            "text": "You can optionally locally build GearVRf from source code using Android Studio.  Here are the steps:   Add  OVR_MOBILE_SDK  to  gradle.properties  and set it to the path to the Oculus Mobile SDK; recommended to use the global gradle.properties  $HOMEPATH/.gradle/gradle.properties  or  ~/.gradle/gradle.properties) .  Add  ANDROID_NDK_HOME  to gradle.properties and set it to the path to the Android NDK installation.  Navigate to the GearVR Framework and select the folder, and click  OK  Click  Make Project  (from the  Build  menu)", 
            "title": "2. Building Gear VR Framework"
        }, 
        {
            "location": "/programming_guide/build_instructions/#3-building-and-running-gearvrf-applications", 
            "text": "After you have the Gear VR Framework, by either locally building or using a pre-built framework, you can now import, build, and run GearVRf applications (your own or sample apps) in Android Studio. The specific procedure you use depends on whether you locally built the Gear VR Framework from source code files or used pre-built framework files.   When building your own GearVRf apps from scratch, copy the appropriate device XML file from the GearVRf SDK to your application's assets folder. \nGearVRf provides an xml file for you to use: gvr.xml.  Import the GearVRf application code.  Click  File -  Open ...  Navigate to the project folder (for example, gvr-simplesample).  Click  OK    Clean and build the application.  Go to the  Build  menu and click  Clean ...  Click  Make Project  (from the  Build  menu)    Run the application.  Connect an Android mobile device to your local machine.  Select your project in the project explorer  Click  Run  on the toolbar      Note  You may need to apply to Oculus for a signature file for your device.\nFor application and use details, refer to the online signature generator ( https://dashboard.oculus.com/tools/osig-generator/ )", 
            "title": "3. Building and Running GearVRf Applications"
        }, 
        {
            "location": "/programming_guide/build_instructions/#generate-javadocs", 
            "text": "When locally building the Gear VR Framework, you can optionally generate Javadoc files with details about the GearVRf API.  Optional: To get GearVRf API reference details by generating GearVRf Javadoc files in the specified directory:   In Android Studio, click  Project   Generate Javadoc...  In the Generate Javadoc window:  Javadoc command : (Pathname to the Javadoc executable file) Typically, in the bin directory of the Java directory.  Click on the plus-icon to  expand the Framework listing .  Checkmark  src  Select  Use standard doclet  Destination:  (Convenient local directory to contain the Javadoc files)  Optional Specify VM options:\n    NOTE: You may encounter errors if VM options is not specified.  Click  Next   Click  Next   VM options: -bootclasspath  path to your Android jar file    Click  Finish", 
            "title": "Generate Javadocs"
        }, 
        {
            "location": "/programming_guide/gearvr_settings_files/", 
            "text": "Definitions of GearVRf XML settings file parameters\n\n\nBefore rendering can start, the framework needs to know about the characteristics of the display device. These are specified in the XML settings file passed to GVRActivity.setMain or GVRActivity.setScript when your application is being initialized.\n\n\n\n\n\n\n\n\nvr-app-settings\n\n\n\n\n\n\n\n\n\n\n\n\nframebufferPixelsHigh\n\n\nHeight of the framebuffer\n\n\n\n\n\n\nframebufferPixelsWide\n\n\nWidth of the framebuffer\n\n\n\n\n\n\nshowLoadingIcon\n\n\nEnable / disable loading icon\n\n\n\n\n\n\nuseProtectedFramebuffer\n\n\nEnable / disable use of protected framebuffer\n\n\n\n\n\n\nuseSrgbFramebuffer\n\n\nEnable / disable use of SRGB framebuffer\n\n\n\n\n\n\nuseMultiview\n\n\nEnable / disable OVR multiview extension\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmono-mode-parms\n\n\n\n\n\n\n\n\n\n\n\n\nallowPowerSave\n\n\nIf enabled, the application will run at 30 fps when power is low. Otherwise, it will show an error message when power is low.\n\n\n\n\n\n\nresetWindowFullScreen\n\n\nIf enabled, the fullscreen flag of the activity window will be on when a VR activity returns from background to foreground. It will help performance since it won't draw a DecorView as background.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nperformance-parms\n\n\n\n\n\n\n\n\n\n\n\n\ncpuLevel\n\n\nCPU clock level\n\n\n\n\n\n\ngpuLevel\n\n\nGPU clock level\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neye-buffer-parms\n\n\n\n\n\n\n\n\n\n\n\n\ncolorFormat\n\n\nFormat of the color buffer (default COLOR_8888) \n COLOR_5551 5 bits R,G,B, 1 bit alpha \n COLOR_565 5 bits red, 6 bits green, 5 bits blue\n COLOR_4444 4 bits RGBA\n COLOR_888 8 bits RGBA \n COLOR_888_sRGB SRGB color format \n COLOR_RGBA16F 16 bits float RGBA\n\n\n\n\n\n\ndepthFormat\n\n\nFormat of the depth buffer (default DEPTH_24) \n DEPTH_0 no depth buffer \n DEPTH_16 16 bit depth buffer \n DEPTH_24 24 bit depth buffer \n DEPTH_24_STENCIL_8 32 bit depth buffer\n\n\n\n\n\n\nfov-y\n\n\nY field of view in degrees (default 90)\n\n\n\n\n\n\nresolutionWidth\n\n\nEye buffer resolution width in pixels (default 1024)\n\n\n\n\n\n\nresolutionHeight\n\n\nEye buffer resolution height in pixels (default 1024)\n\n\n\n\n\n\nresolveDepth\n\n\nTrue to resolve framebuffer to a texture (default false)\n\n\n\n\n\n\nmultiSamples\n\n\nNumber of framebuffer multisamples for anti-aliasing \n 1 = no multisampling (not recommended) \n 2 = 2xMSAA recommended setting \n 4 = 4xMSAA Higher visual quality but lower performance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhead-model-parms\n\n\n\n\n\n\n\n\n\n\n\n\neyeHeight\n\n\nDistance from ground to eye\n\n\n\n\n\n\nheadModelDepth\n\n\nOffset of head center ahead of eyes based on eye height\n\n\n\n\n\n\nheadModelHeight\n\n\nDistance from neck joint to eyes based on eye height\n\n\n\n\n\n\ninterpupillaryDistance\n\n\nDistance between left and right eye", 
            "title": "GearVRf Settings File"
        }, 
        {
            "location": "/programming_guide/faq/", 
            "text": "1. Is there any example of object following the head tracking, just like a reticle?\n\n\nSee \ngvr-tutorial-lesson2 sample\n. Examine the BalloonMain.java and the headTracker tracker object it sets up. The key part is adding the object to the main camera rig.\n\n\n2. I want to implement a scrollable list of item like ListView in Android. How to go about that?\n\n\n\n\nBackground objects: rendering order N, depth test on\n\n\nClip object: rendering order N+1, depth test on, alpha blend on, alpha = 0 (completely transparent)\n\n\nList view object: rendering order N+2, depth test on\n\n\n\n\nThe clip object should be a plane with a hole in it where you want to see the list view. It should be completely transparent. It will be rendered after the background so it will update the depth buffer but the background will show thru completely. This clip object should have a Z value putting it IN FRONT of the list view object even though it will be rendered before that object (because you set the rendering order to a smaller value).\n\n\nGearVRF will render objects in ascending rendering order so the background will be rendered first. The clip object will update the depth buffer so that anything drawn BEHIND it will show thru the hole but will be obscured by the transparent clip area (the depth buffer will do the clipping for us).\n\n\n3. Can i use an emulator during development for testing?\n\n\nShort answer: No.\n\n\nLong answer: It would likely be somewhat painful to do. Oculus only provides 32bit arm libraries. Which means you would need to set up an arm emulator (rather than an x86 one). In that emulator, we would detect the oculus service is not on the system and fall back to daydream. However, in our experience, running an arm emulator is horrifically slow, especially for anything GL related. It's best to stick with a physical phone for development.\n\n\n4. I am using Windows, trying to build the framework and getting weird errors. Like this one:\n\n\n...\\GearVRf\\GVRf\\Framework\\framework\\..\\backend_oculus/src/main/jni/util/configuration_helper.cpp:235:1: fatal error: opening dependency file ./obj/local/armeabi-v7a/objs/...\\GearVRf\\GVRf\\Framework\\framework\\..\\backend_oculus/src/main/jni/util/configuration_helper.o.d: No such file or directory\n\n\n\n\n\nYour paths might be too long. Try moving the framework to C:\\ and build again.\n\n\n5. I want to inflate and show an Android view. Can I do that?\n\n\nYes. The gvr-renderableview sample shows how to do that.\n\n\n6. I want to use ExoPlayer instead of MediaPlayer for video playback. Can I do this?\n\n\nYes. See the gvr-360video sample, which allows you to use either. Set the USE_EXO_PLAYER flag in Minimal360VideoActivity.java.\n\n\n7. How can I create a mixed VR android app and launching VR Mode later, by clicking a button for example? I need to create an activity visualized in normal mode for settings and later launch a VR mode, showing the \"you need gear vr\" screen if you have not attached it.\n\n\nUnfortunately, this is not supported. Apps get marked as \"vr\" not individual activities. Which means the prompt will show when you try to launch your \"normal\" activity. This is not a gvrf limitation.\n\n\n8. Trying to build a sample but I get the following error:\n\n\nWhat\n \nwent\n \nwrong\n:\n\n\nExecution\n \nfailed\n \nfor\n \ntask\n \n:app:transformClassesWithDexForDebug\n.\n\n\n \ncom\n.\nandroid\n.\nbuild\n.\napi\n.\ntransform\n.\nTransformException\n:\n \ncom\n.\nandroid\n.\nide\n.\ncommon\n.\nprocess\n.\nProcessException\n:\n \njava\n.\nutil\n.\nconcurrent\n.\nExecutionException\n:\n \ncom\n.\nandroid\n.\ndex\n.\nDexException\n:\n \nMultiple\n \ndex\n \nfiles\n \ndefine\n \nLcom\n/\noculus\n/\nsystemutils\n/\nBuildConfig\n;\n\n\n\n\n\n\nMost likely you still have VrApi.jar and SystemUtils.jar under the framework module (GearVRf/GVRf/Framework/framework/src/main/libs/). Please remove them from there, clean and build.\n\n\n9. I am using Linux and getting a strange aapt error during the build. Something like \njava.io.IOException: Cannot run program \"/aapt\": error=2, No such file or directory\n\n\nYou might be missing support for executing 32bit binaries and/or libraries aapt depends on. Please run the following:\n\n\nsudo dpkg --add-architecture i386\nsudo apt-get update\nsudo apt-get install libc6:i386 libncurses5:i386 libstdc++6:i386\nsudo apt-get install zlib1g:i386\n\n\n\n\n\n10. Is there support for the Oculus Platform SDK?\n\n\nYes, the entitlement check is supported. Go to GVRf/Extensions/. There is a platformsdk_support module. To build it run \n./gradlew -Pplatformsdk_support=true platformsdk_support:assembleDebug\n. Checkout the javadoc in PlatformEntitlementCheck.java. Have been verified to work with Platform SDK versions 1.6, 1.7 and 1.8. For further information see \nhttps://github.com/Samsung/GearVRf/wiki/Entitlement-Check-using-GVRF\n\n\n11. Is there any way to play youtube video from url?\n\n\nYes. See \nhttps://github.com/Samsung/GearVRf/issues/1033#issuecomment-278244683\n\n\n12. I am trying to use GVRF on a Google Pixel phone and I get this exception:\n\n\n02-15 19:53:15.697 23156-23156/? E/AndroidRuntime: FATAL EXCEPTION: main\nProcess: pl.lynx.daydream.test, PID: 23156\njava.lang.UnsatisfiedLinkError: dalvik.system.PathClassLoader[DexPathList[[zip file \n/data/app/pl.lynx.daydream.test-2/base.apk\n],nativeLibraryDirectories=[/data/app/pl.lynx.daydream.test-2/lib/arm64, /system/fake-libs64, /data/app/pl.lynx.daydream.test-2/base.apk!/lib/arm64-v8a, /system/lib64, /vendor/lib64]]] couldn\nt find \nlibgvrf.so\n\n\n\n\n\n\nDaydream has 64bit binaries but GVRf only supports 32bit binaries. In your app's gradle file you need to add this:\n\n\nandroid {\n\n    // ignore the x86 and arm-v8 files from the google vr libraries\n    packagingOptions {\n        exclude \nlib/x86/libgvr.so\n\n        exclude \nlib/arm64-v8a/libgvr.so\n\n    }\n}\n\n\n\n\n\n13. I used to build the demos from the GearVRf-Demos repo just fine. Suddenly I am getting errors. What happened?\n\n\nThe master branch is subject to frequent improvements. The GVRf team pushes updated framework snapshots to the maven repo, but due to the gradle's caching you are most likely using outdated snapshot. Please pass the --refresh-dependencies argument to gradlew if you are building from the command line. Or you can just delete the gradle cache via \n\n\nrm -rf ~/.gradle/caches/.\n\n\n\n\n\nAlternatively you could use the 3.1 branch which is stable. After cloning the demos repo, switch to the release_v3.1 branch.\n\n\n14. I am building an app from scratch. How do I add support for GVRF to my app? What are the minimum dependencies?\n\n\nPlease see this bare-bones project that can serve as a reference: \nhttps://github.com/gearvrf/GearVRf-Demos/tree/master/template/GVRFApplication\n.\n\n\n15. My app in the Oculus Store fails to install on Android N devices. I get an UNTRUSTED_APK_ERROR error.\n\n\nOculus doesn't support APK signature scheme v2 yet. It should be disabled if you plan to submit apps to the Oculus store. Android Studio seems to apply the scheme unconditionally. Build from the command line and include the following section in your gradle file:\n\n\nandroid {\n    signingConfigs {\n        release {\n            v2SigningEnabled false\n            storeFile file(\nfull-path-to-your-store-file\n)\n            storePassword \nyour_store_pwd\n\n            keyAlias \nalias\n\n            keyPassword \nkey_pwd\n\n        }\n    }\n    defaultConfig {\n        signingConfig signingConfigs.release\n    }\n}\n\n\n\n\n\n16. Can you run GearVR app on your phone without GearVR?\n\n\nYes, in your phone's Settings-\nApplications-\nApplication Manager-\nGear VR Service-\nManage Storage Tab on VR Service Version multiple times until the 'Developer Options' menu appears. Then flick on the 'Developer mode' switch. You may need to do this every time when the phone restarts\n\n\n17. How to reduce nausea?\n\n\nMaintain a high frame-rate, at least 60 fps.\nAvoid rapidly turning the camera.\nMove slowly are carefully when teleporting.\n\n\n18. How many triangles can I display max for a good VR experience with high frame rate?\n\n\nOn a mobile phone such as Galaxy S6/S7, please keep triangle count in the thousands to tens of thousands range if possible, depending on shader complexities. A mobile GPU can display around 100,000 vertices with a simple shader, about half that if using real-time shadow mapping from a single light source.\n\n\n19. How many separate scene objects can I display for a good VR experience?\n\n\nOn OpenGL, each scene object can potentially generate a draw call. On most phones, exceeding 100 draw calls will compromise VR performance. GearVRf attempts to batch together objects which use the same shader when possible.\n\n\n20. What are some graphics performance tips?\n\n\nKeep draw calls minimal and relatively cheap pixel shader. A mobile GPU can execute about 100 draw calls per frame. It can display about 100,000 vertices before it becomes difficult to maintain 60fps.\n\n\nKeep in mind shadows from shadow map more or less doubles the number of draw calls and vertices rendered. Use profiler to see if you are really GPU bound.\n\n\n21. Which phones are compatible with GearVR?\n\n\nCurrently, Samsung Galaxy S6, S6 Edge, S6 Edge+, S7, S7 Edge, S7 Edge+, S8, S8+, Note 5 and Note 8.", 
            "title": "FAQ"
        }, 
        {
            "location": "/programming_guide/faq/#1-is-there-any-example-of-object-following-the-head-tracking-just-like-a-reticle", 
            "text": "See  gvr-tutorial-lesson2 sample . Examine the BalloonMain.java and the headTracker tracker object it sets up. The key part is adding the object to the main camera rig.", 
            "title": "1. Is there any example of object following the head tracking, just like a reticle?"
        }, 
        {
            "location": "/programming_guide/faq/#2-i-want-to-implement-a-scrollable-list-of-item-like-listview-in-android-how-to-go-about-that", 
            "text": "Background objects: rendering order N, depth test on  Clip object: rendering order N+1, depth test on, alpha blend on, alpha = 0 (completely transparent)  List view object: rendering order N+2, depth test on   The clip object should be a plane with a hole in it where you want to see the list view. It should be completely transparent. It will be rendered after the background so it will update the depth buffer but the background will show thru completely. This clip object should have a Z value putting it IN FRONT of the list view object even though it will be rendered before that object (because you set the rendering order to a smaller value).  GearVRF will render objects in ascending rendering order so the background will be rendered first. The clip object will update the depth buffer so that anything drawn BEHIND it will show thru the hole but will be obscured by the transparent clip area (the depth buffer will do the clipping for us).", 
            "title": "2. I want to implement a scrollable list of item like ListView in Android. How to go about that?"
        }, 
        {
            "location": "/programming_guide/faq/#3-can-i-use-an-emulator-during-development-for-testing", 
            "text": "Short answer: No.  Long answer: It would likely be somewhat painful to do. Oculus only provides 32bit arm libraries. Which means you would need to set up an arm emulator (rather than an x86 one). In that emulator, we would detect the oculus service is not on the system and fall back to daydream. However, in our experience, running an arm emulator is horrifically slow, especially for anything GL related. It's best to stick with a physical phone for development.", 
            "title": "3. Can i use an emulator during development for testing?"
        }, 
        {
            "location": "/programming_guide/faq/#4-i-am-using-windows-trying-to-build-the-framework-and-getting-weird-errors-like-this-one", 
            "text": "...\\GearVRf\\GVRf\\Framework\\framework\\..\\backend_oculus/src/main/jni/util/configuration_helper.cpp:235:1: fatal error: opening dependency file ./obj/local/armeabi-v7a/objs/...\\GearVRf\\GVRf\\Framework\\framework\\..\\backend_oculus/src/main/jni/util/configuration_helper.o.d: No such file or directory  Your paths might be too long. Try moving the framework to C:\\ and build again.", 
            "title": "4. I am using Windows, trying to build the framework and getting weird errors. Like this one:"
        }, 
        {
            "location": "/programming_guide/faq/#5-i-want-to-inflate-and-show-an-android-view-can-i-do-that", 
            "text": "Yes. The gvr-renderableview sample shows how to do that.", 
            "title": "5. I want to inflate and show an Android view. Can I do that?"
        }, 
        {
            "location": "/programming_guide/faq/#6-i-want-to-use-exoplayer-instead-of-mediaplayer-for-video-playback-can-i-do-this", 
            "text": "Yes. See the gvr-360video sample, which allows you to use either. Set the USE_EXO_PLAYER flag in Minimal360VideoActivity.java.", 
            "title": "6. I want to use ExoPlayer instead of MediaPlayer for video playback. Can I do this?"
        }, 
        {
            "location": "/programming_guide/faq/#7-how-can-i-create-a-mixed-vr-android-app-and-launching-vr-mode-later-by-clicking-a-button-for-example-i-need-to-create-an-activity-visualized-in-normal-mode-for-settings-and-later-launch-a-vr-mode-showing-the-you-need-gear-vr-screen-if-you-have-not-attached-it", 
            "text": "Unfortunately, this is not supported. Apps get marked as \"vr\" not individual activities. Which means the prompt will show when you try to launch your \"normal\" activity. This is not a gvrf limitation.", 
            "title": "7. How can I create a mixed VR android app and launching VR Mode later, by clicking a button for example? I need to create an activity visualized in normal mode for settings and later launch a VR mode, showing the \"you need gear vr\" screen if you have not attached it."
        }, 
        {
            "location": "/programming_guide/faq/#8-trying-to-build-a-sample-but-i-get-the-following-error", 
            "text": "What   went   wrong :  Execution   failed   for   task   :app:transformClassesWithDexForDebug .    com . android . build . api . transform . TransformException :   com . android . ide . common . process . ProcessException :   java . util . concurrent . ExecutionException :   com . android . dex . DexException :   Multiple   dex   files   define   Lcom / oculus / systemutils / BuildConfig ;   Most likely you still have VrApi.jar and SystemUtils.jar under the framework module (GearVRf/GVRf/Framework/framework/src/main/libs/). Please remove them from there, clean and build.", 
            "title": "8. Trying to build a sample but I get the following error:"
        }, 
        {
            "location": "/programming_guide/faq/#9-i-am-using-linux-and-getting-a-strange-aapt-error-during-the-build-something-like-javaioioexception-cannot-run-program-aapt-error2-no-such-file-or-directory", 
            "text": "You might be missing support for executing 32bit binaries and/or libraries aapt depends on. Please run the following:  sudo dpkg --add-architecture i386\nsudo apt-get update\nsudo apt-get install libc6:i386 libncurses5:i386 libstdc++6:i386\nsudo apt-get install zlib1g:i386", 
            "title": "9. I am using Linux and getting a strange aapt error during the build. Something like java.io.IOException: Cannot run program \"/aapt\": error=2, No such file or directory"
        }, 
        {
            "location": "/programming_guide/faq/#10-is-there-support-for-the-oculus-platform-sdk", 
            "text": "Yes, the entitlement check is supported. Go to GVRf/Extensions/. There is a platformsdk_support module. To build it run  ./gradlew -Pplatformsdk_support=true platformsdk_support:assembleDebug . Checkout the javadoc in PlatformEntitlementCheck.java. Have been verified to work with Platform SDK versions 1.6, 1.7 and 1.8. For further information see  https://github.com/Samsung/GearVRf/wiki/Entitlement-Check-using-GVRF", 
            "title": "10. Is there support for the Oculus Platform SDK?"
        }, 
        {
            "location": "/programming_guide/faq/#11-is-there-any-way-to-play-youtube-video-from-url", 
            "text": "Yes. See  https://github.com/Samsung/GearVRf/issues/1033#issuecomment-278244683", 
            "title": "11. Is there any way to play youtube video from url?"
        }, 
        {
            "location": "/programming_guide/faq/#12-i-am-trying-to-use-gvrf-on-a-google-pixel-phone-and-i-get-this-exception", 
            "text": "02-15 19:53:15.697 23156-23156/? E/AndroidRuntime: FATAL EXCEPTION: main\nProcess: pl.lynx.daydream.test, PID: 23156\njava.lang.UnsatisfiedLinkError: dalvik.system.PathClassLoader[DexPathList[[zip file  /data/app/pl.lynx.daydream.test-2/base.apk ],nativeLibraryDirectories=[/data/app/pl.lynx.daydream.test-2/lib/arm64, /system/fake-libs64, /data/app/pl.lynx.daydream.test-2/base.apk!/lib/arm64-v8a, /system/lib64, /vendor/lib64]]] couldn t find  libgvrf.so   Daydream has 64bit binaries but GVRf only supports 32bit binaries. In your app's gradle file you need to add this:  android {\n\n    // ignore the x86 and arm-v8 files from the google vr libraries\n    packagingOptions {\n        exclude  lib/x86/libgvr.so \n        exclude  lib/arm64-v8a/libgvr.so \n    }\n}", 
            "title": "12. I am trying to use GVRF on a Google Pixel phone and I get this exception:"
        }, 
        {
            "location": "/programming_guide/faq/#13-i-used-to-build-the-demos-from-the-gearvrf-demos-repo-just-fine-suddenly-i-am-getting-errors-what-happened", 
            "text": "The master branch is subject to frequent improvements. The GVRf team pushes updated framework snapshots to the maven repo, but due to the gradle's caching you are most likely using outdated snapshot. Please pass the --refresh-dependencies argument to gradlew if you are building from the command line. Or you can just delete the gradle cache via   rm -rf ~/.gradle/caches/.  Alternatively you could use the 3.1 branch which is stable. After cloning the demos repo, switch to the release_v3.1 branch.", 
            "title": "13. I used to build the demos from the GearVRf-Demos repo just fine. Suddenly I am getting errors. What happened?"
        }, 
        {
            "location": "/programming_guide/faq/#14-i-am-building-an-app-from-scratch-how-do-i-add-support-for-gvrf-to-my-app-what-are-the-minimum-dependencies", 
            "text": "Please see this bare-bones project that can serve as a reference:  https://github.com/gearvrf/GearVRf-Demos/tree/master/template/GVRFApplication .", 
            "title": "14. I am building an app from scratch. How do I add support for GVRF to my app? What are the minimum dependencies?"
        }, 
        {
            "location": "/programming_guide/faq/#15-my-app-in-the-oculus-store-fails-to-install-on-android-n-devices-i-get-an-untrusted_apk_error-error", 
            "text": "Oculus doesn't support APK signature scheme v2 yet. It should be disabled if you plan to submit apps to the Oculus store. Android Studio seems to apply the scheme unconditionally. Build from the command line and include the following section in your gradle file:  android {\n    signingConfigs {\n        release {\n            v2SigningEnabled false\n            storeFile file( full-path-to-your-store-file )\n            storePassword  your_store_pwd \n            keyAlias  alias \n            keyPassword  key_pwd \n        }\n    }\n    defaultConfig {\n        signingConfig signingConfigs.release\n    }\n}", 
            "title": "15. My app in the Oculus Store fails to install on Android N devices. I get an UNTRUSTED_APK_ERROR error."
        }, 
        {
            "location": "/programming_guide/faq/#16-can-you-run-gearvr-app-on-your-phone-without-gearvr", 
            "text": "Yes, in your phone's Settings- Applications- Application Manager- Gear VR Service- Manage Storage Tab on VR Service Version multiple times until the 'Developer Options' menu appears. Then flick on the 'Developer mode' switch. You may need to do this every time when the phone restarts", 
            "title": "16. Can you run GearVR app on your phone without GearVR?"
        }, 
        {
            "location": "/programming_guide/faq/#17-how-to-reduce-nausea", 
            "text": "Maintain a high frame-rate, at least 60 fps.\nAvoid rapidly turning the camera.\nMove slowly are carefully when teleporting.", 
            "title": "17. How to reduce nausea?"
        }, 
        {
            "location": "/programming_guide/faq/#18-how-many-triangles-can-i-display-max-for-a-good-vr-experience-with-high-frame-rate", 
            "text": "On a mobile phone such as Galaxy S6/S7, please keep triangle count in the thousands to tens of thousands range if possible, depending on shader complexities. A mobile GPU can display around 100,000 vertices with a simple shader, about half that if using real-time shadow mapping from a single light source.", 
            "title": "18. How many triangles can I display max for a good VR experience with high frame rate?"
        }, 
        {
            "location": "/programming_guide/faq/#19-how-many-separate-scene-objects-can-i-display-for-a-good-vr-experience", 
            "text": "On OpenGL, each scene object can potentially generate a draw call. On most phones, exceeding 100 draw calls will compromise VR performance. GearVRf attempts to batch together objects which use the same shader when possible.", 
            "title": "19. How many separate scene objects can I display for a good VR experience?"
        }, 
        {
            "location": "/programming_guide/faq/#20-what-are-some-graphics-performance-tips", 
            "text": "Keep draw calls minimal and relatively cheap pixel shader. A mobile GPU can execute about 100 draw calls per frame. It can display about 100,000 vertices before it becomes difficult to maintain 60fps.  Keep in mind shadows from shadow map more or less doubles the number of draw calls and vertices rendered. Use profiler to see if you are really GPU bound.", 
            "title": "20. What are some graphics performance tips?"
        }, 
        {
            "location": "/programming_guide/faq/#21-which-phones-are-compatible-with-gearvr", 
            "text": "Currently, Samsung Galaxy S6, S6 Edge, S6 Edge+, S7, S7 Edge, S7 Edge+, S8, S8+, Note 5 and Note 8.", 
            "title": "21. Which phones are compatible with GearVR?"
        }, 
        {
            "location": "/programming_guide/video_tutorials/", 
            "text": "Below is a set of six lessons that show how to build a simple VR game using GearVRf. The sessions are 10 - 20 minutes long and show live demonstrations of GearVRf programming.\n\n\nThe sample code used in all the tutorials can be found here: \nhttps://github.com/gearvrf/GearVRf-Demos\n  \n\n\nLesson 1: Overview of Tutorials\n\n\n\n\n\nLesson 2: Application Structure and Scene Graph\n\n\n\n\n\nLesson 3: Events and Picking\n\n\n\n\nLesson 4: Components\n\n\n\n\nLesson 5: Sound and Text\n\n\n\n\nLesson 6: Working with Assets", 
            "title": "Video Tutorials"
        }, 
        {
            "location": "/programming_guide/video_tutorials/#lesson-1-overview-of-tutorials", 
            "text": "", 
            "title": "Lesson 1: Overview of Tutorials"
        }, 
        {
            "location": "/programming_guide/video_tutorials/#lesson-2-application-structure-and-scene-graph", 
            "text": "", 
            "title": "Lesson 2: Application Structure and Scene Graph"
        }, 
        {
            "location": "/programming_guide/video_tutorials/#lesson-3-events-and-picking", 
            "text": "", 
            "title": "Lesson 3: Events and Picking"
        }, 
        {
            "location": "/programming_guide/video_tutorials/#lesson-4-components", 
            "text": "", 
            "title": "Lesson 4: Components"
        }, 
        {
            "location": "/programming_guide/video_tutorials/#lesson-5-sound-and-text", 
            "text": "", 
            "title": "Lesson 5: Sound and Text"
        }, 
        {
            "location": "/programming_guide/video_tutorials/#lesson-6-working-with-assets", 
            "text": "", 
            "title": "Lesson 6: Working with Assets"
        }, 
        {
            "location": "/programming_guide/sample_code/", 
            "text": "GearVRf Samples and Demos\n\n\nTo get the GearVR Framework Samples and Demos, clone the following repository in the same directory as where you did the clone for the framework source code:\n\n\n$ git clone https://github.com/gearvrf/GearVRf-Demos.git -b release_v3.2\n\n\n\n\n\n!!!Note You should put both GearVRf/ and GearVRf-Demos/ in the same directory.\n\n\nSample GearVRf Applications\n\n\nSample GearVRf applications, available in the GearVRf SDK, can provide you with valuable insight into writing your own VR applications.\n\n\n!!!Note: The flat images below represent GearVRf applications with actual stereographic displays.\n\n\n\n\n\n\n\n\nApplication\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nSolar System\n \n (gvrsolarsystem) \n \n\n\nSolar system with four inner rotating planets revolving around the rotating sun as viewed from the point on the moon closest to Earth\n \n Asynchronous loading of meshes and textures is used. \nLoading order is by priority; no mesh or texture is loaded twice. \nAnimation is used to create the illusion of rotation and revolution.\n\n\n\n\n\n\nDynamic Graphics Board\n(gvropacityanigalery)\n\n\nImages and video displayed on a board inside a 3D scene\nA video or image is displayed in a rectangular scene object (up the stairs).\nAnimation is used to switch images and video smoothly.\nOnStep decides and defines the animation to apply, based on HMD orientation.\nPost-effect converts view to sepia colors.\n\n\n\n\n\n\nEye Picking\n \n(gvreyepickingsample)\n\n\nColor of objects (bunnies and rectangles) changes to red when an object is at the center of the view\nOnStep updates scene object colors.\nImplements a color custom shader.\n\n\n\n\n\n\nPick and Move\n \n(gvr-pickandmove)\n\n\n3D mirror ball can be selected and repositioned in a 3D scene\nUses the HMD trackpad.\nUses Cubemap support.\nUses scene picking.\n\n\n\n\n\n\nUseful Scene Objects\n(scene-objects)\n\n\nContains fundamental scene objects (such as a cube, sphere, cone, and text) that you can use in your own apps", 
            "title": "Sample Code"
        }, 
        {
            "location": "/programming_guide/sample_code/#gearvrf-samples-and-demos", 
            "text": "To get the GearVR Framework Samples and Demos, clone the following repository in the same directory as where you did the clone for the framework source code:  $ git clone https://github.com/gearvrf/GearVRf-Demos.git -b release_v3.2  !!!Note You should put both GearVRf/ and GearVRf-Demos/ in the same directory.", 
            "title": "GearVRf Samples and Demos"
        }, 
        {
            "location": "/programming_guide/sample_code/#sample-gearvrf-applications", 
            "text": "Sample GearVRf applications, available in the GearVRf SDK, can provide you with valuable insight into writing your own VR applications.  !!!Note: The flat images below represent GearVRf applications with actual stereographic displays.     Application  Description      Solar System    (gvrsolarsystem)     Solar system with four inner rotating planets revolving around the rotating sun as viewed from the point on the moon closest to Earth    Asynchronous loading of meshes and textures is used.  Loading order is by priority; no mesh or texture is loaded twice.  Animation is used to create the illusion of rotation and revolution.    Dynamic Graphics Board (gvropacityanigalery)  Images and video displayed on a board inside a 3D scene A video or image is displayed in a rectangular scene object (up the stairs). Animation is used to switch images and video smoothly. OnStep decides and defines the animation to apply, based on HMD orientation. Post-effect converts view to sepia colors.    Eye Picking   (gvreyepickingsample)  Color of objects (bunnies and rectangles) changes to red when an object is at the center of the view OnStep updates scene object colors. Implements a color custom shader.    Pick and Move   (gvr-pickandmove)  3D mirror ball can be selected and repositioned in a 3D scene Uses the HMD trackpad. Uses Cubemap support. Uses scene picking.    Useful Scene Objects (scene-objects)  Contains fundamental scene objects (such as a cube, sphere, cone, and text) that you can use in your own apps", 
            "title": "Sample GearVRf Applications"
        }, 
        {
            "location": "/api_reference/", 
            "text": "Online GearVRf API Reference (\nhttp://docs.gearvrf.org\n)", 
            "title": "API Reference"
        }, 
        {
            "location": "/blog/2017_4_17_x3d/", 
            "text": "GVRF and the X3D file format\n\n\nby Mitch Williams April 17, 2017\n\n\nYou may think creating VR has a steep learning curve; that you need to be an experienced game programmer or Java/C++ expert. However, in addition to a java interface, the GearVR Framework (GVRf) supports an XML-based 3D language that along with JavaScript for interactivity (skills in-common with HTML web page designers) makes VR creation simple.\n\n\nThe X3D file format is an international standard specifying 3D scenes on the web. It is exported by 3D modeling tools such as 3D Studio Max, supported by many 3D printers, and is now integrated into GVRf.\n\n\nX3D can be thought of as the 3D version of HTML.  It uses similar tags with left and right brackets \u2018\n\u2019 and \u2018\n\u2019.  Here is an example X3D file:\n\n\nX3D\n\n    \nScene\n\n        \nTransform\n \ntranslation=\n\u201d0\n \n0\n \n-10\u201d\n\n           \nShape\n\n            \nAppearance\n\n               \nMaterial\n \ndiffuseColor=\n1 .5 0\n/\n\n            \n/Appearance\n\n            \nSphere/\n\n\n           \n/Shape\n\n        \n/Transform\n\n    \n/Scene\n\n\n/X3D\n\n\n\n\n\n\nThis produces an orange sphere, 10 units away in front of the camera.\n\n\n\n\nYou will likely see X3D embedded inside an HTML file.  Saving the code below which contains the same X3D code above, and saving it as an .htm file will show the same orange sphere on a web page:\n\n\nhtml\n\n\n\nhead\n\n\n    \ntitle\nMy first X3DOM page\n/\ntitle\n\n\n    \nscript\n \ntype\n=\ntext/javascript\n \nsrc\n=\nhttp://www.x3dom.org/download/x3dom.js\n \n/\nscript\n\n\n    \nlink\n \nrel\n=\nstylesheet\n \ntype\n=\ntext/css\n \nhref\n=\nhttp://www.x3dom.org/download/x3dom.css\n/\n\n\n\n/\nhead\n\n\n\nbody\n\n\n\nX3D\n\n\n    \nScene\n\n\n        \nTransform\n \ntranslation\n=\n\u201d0\n \n0\n \n-10\n\u201d\n\n\n           \nShape\n\n\n            \nAppearance\n\n\n               \nMaterial\n \ndiffuseColor\n=\n1 .5 0\n/\n\n\n            \n/\nAppearance\n\n\n            \nSphere\n/\n\n\n           \n/\nShape\n\n\n        \n/\nTransform\n\n\n    \n/\nScene\n\n\n\n/\nX3D\n\n\n\n/\nbody\n\n\n\n/\nhtml\n\n\n\n\n\n\nThe HTML file relies on a JavaScript file, x3dom.js, that parses X3D and displays the 3D scene using WebGL.  GVRf parses the same X3D file, but uses GearVR\u2019s technology to display the orange sphere in VR.\n\n\nIf you have the GVRf development environment already set up with Android Studio, and have downloaded \nGVRf\u2019s Demos repository\n, look for the folder \ngvr-x3d-demo\n for other examples plus showing how to insert your .x3d files.  Remember that .x3d files will be added to Android Studio\u2019s \u2018assets\u2019 folder.  Just change the line of code in the file X3DparserScript.java \u201cString filename = . . . \u201c to your .x3d file.\n\n\nReviewing the X3D file, \u2018diffuseColor\u2019 is the color of the object (more on that later).  Color in 3D is specified as red, green, blue with values between 0 and 1.  The Sphere has red = 1, green = .5 and blue = 0, producing orange.  The Sphere can be replaced with other primitives: Box, Cone or Cylinder.\n\n\nRotation is specified as \u201crotation=\u2019x, y, z, angle\u2019\u201d where \u2018angle\u2019 is in radians.  Thus a rotation of 90 degrees (1.57 radians) around the z-axis will be: rotation=\u20190 0 1 1.57\u2019.\n\n\nNow is a good time to experiment.  Change the diffuseColor, translation or the object itself.  This scene contains a red Box on the left rotated around the x-axis 1.2 radians, a green Cone in the center, and a blue Cylinder on the right rotated -.4 radians around the z-axis.\n\n\nX3D\n\n\n    \nScene\n\n\n        \nTransform\n \nDEF=\nobj1\n \ntranslation=\n-3 0 -10\n \nrotation=\n1 0 0 1.2\n\n\n           \nShape\n\n\n            \nAppearance\n\n\n               \nMaterial\n \ndiffuseColor=\n1 0 0\n/\n\n\n            \n/Appearance\n\n\n            \nBox/\n\n\n           \n/Shape\n\n\n        \n/Transform\n\n\n        \nTransform\n \nDEF=\nobj2\n \ntranslation=\n0 0 -10\n\n\n           \nShape\n\n\n            \nAppearance\n\n\n               \nMaterial\n \ndiffuseColor=\n0 1 0\n/\n\n\n            \n/Appearance\n\n\n            \nCone/\n\n\n           \n/Shape\n\n\n        \n/Transform\n\n\n        \nTransform\n \nDEF=\nobj3\n \ntranslation=\n3 0 -10\n \nrotation=\n0 0 1 -.4\n\n\n           \nShape\n\n\n            \nAppearance\n\n\n               \nMaterial\n \ndiffuseColor=\n0 0 1\n/\n\n\n            \n/Appearance\n\n\n            \nCylinder/\n\n\n           \n/Shape\n\n\n        \n/Transform\n\n\n        \nViewpoint\n \nposition=\n0 0 0\n/\n\n\n        \nPointLight/\n\n\n    \n/Scene\n\n\n\n/X3D\n\n\n\n\n\n\n\n\nToward the bottom there is a PointLight and Viewpoint, the light and camera in the scene.  VR, just like a Hollywood movie, has \u201cLights, Camera, Action!\u201d  Each Transform has a DEFine which assigns names to our Scene Objects as if they were actors in a movie. \n\n\nFeel free to experiment with the color, location and rotation of the primitives, then view them in GearVR.  If you have access to a 3D modeling program such as 3D Studio (3DS) Max, Blender or BS Content Studio, you can create your own 3D scenes and display them in GearVR.  \nBlender\n and \nBS Content Studio\n are free and export directly to X3D.  3DS Max exports to VRML, which can be converted to X3D using online tools such as \nInstant Reality converter\n.\n\n\nAdditional tutorials can be found at \nhttps://doc.x3dom.org/tutorials/index.html\n and \nhttp://www.web3d.org/getting-started-x3d\n.", 
            "title": "GVRF and the X3D file format"
        }, 
        {
            "location": "/blog/2017_4_17_x3d/#gvrf-and-the-x3d-file-format", 
            "text": "by Mitch Williams April 17, 2017  You may think creating VR has a steep learning curve; that you need to be an experienced game programmer or Java/C++ expert. However, in addition to a java interface, the GearVR Framework (GVRf) supports an XML-based 3D language that along with JavaScript for interactivity (skills in-common with HTML web page designers) makes VR creation simple.  The X3D file format is an international standard specifying 3D scenes on the web. It is exported by 3D modeling tools such as 3D Studio Max, supported by many 3D printers, and is now integrated into GVRf.  X3D can be thought of as the 3D version of HTML.  It uses similar tags with left and right brackets \u2018 \u2019 and \u2018 \u2019.  Here is an example X3D file:  X3D \n     Scene \n         Transform   translation= \u201d0   0   -10\u201d \n            Shape \n             Appearance \n                Material   diffuseColor= 1 .5 0 / \n             /Appearance \n             Sphere/ \n\n            /Shape \n         /Transform \n     /Scene  /X3D   This produces an orange sphere, 10 units away in front of the camera.   You will likely see X3D embedded inside an HTML file.  Saving the code below which contains the same X3D code above, and saving it as an .htm file will show the same orange sphere on a web page:  html  head \n\n     title My first X3DOM page / title \n\n     script   type = text/javascript   src = http://www.x3dom.org/download/x3dom.js   / script \n\n     link   rel = stylesheet   type = text/css   href = http://www.x3dom.org/download/x3dom.css /  / head  body  X3D \n\n     Scene \n\n         Transform   translation = \u201d0   0   -10 \u201d \n\n            Shape \n\n             Appearance \n\n                Material   diffuseColor = 1 .5 0 / \n\n             / Appearance \n\n             Sphere / \n\n            / Shape \n\n         / Transform \n\n     / Scene  / X3D  / body  / html   The HTML file relies on a JavaScript file, x3dom.js, that parses X3D and displays the 3D scene using WebGL.  GVRf parses the same X3D file, but uses GearVR\u2019s technology to display the orange sphere in VR.  If you have the GVRf development environment already set up with Android Studio, and have downloaded  GVRf\u2019s Demos repository , look for the folder  gvr-x3d-demo  for other examples plus showing how to insert your .x3d files.  Remember that .x3d files will be added to Android Studio\u2019s \u2018assets\u2019 folder.  Just change the line of code in the file X3DparserScript.java \u201cString filename = . . . \u201c to your .x3d file.  Reviewing the X3D file, \u2018diffuseColor\u2019 is the color of the object (more on that later).  Color in 3D is specified as red, green, blue with values between 0 and 1.  The Sphere has red = 1, green = .5 and blue = 0, producing orange.  The Sphere can be replaced with other primitives: Box, Cone or Cylinder.  Rotation is specified as \u201crotation=\u2019x, y, z, angle\u2019\u201d where \u2018angle\u2019 is in radians.  Thus a rotation of 90 degrees (1.57 radians) around the z-axis will be: rotation=\u20190 0 1 1.57\u2019.  Now is a good time to experiment.  Change the diffuseColor, translation or the object itself.  This scene contains a red Box on the left rotated around the x-axis 1.2 radians, a green Cone in the center, and a blue Cylinder on the right rotated -.4 radians around the z-axis.  X3D \n\n     Scene \n\n         Transform   DEF= obj1   translation= -3 0 -10   rotation= 1 0 0 1.2 \n\n            Shape \n\n             Appearance \n\n                Material   diffuseColor= 1 0 0 / \n\n             /Appearance \n\n             Box/ \n\n            /Shape \n\n         /Transform \n\n         Transform   DEF= obj2   translation= 0 0 -10 \n\n            Shape \n\n             Appearance \n\n                Material   diffuseColor= 0 1 0 / \n\n             /Appearance \n\n             Cone/ \n\n            /Shape \n\n         /Transform \n\n         Transform   DEF= obj3   translation= 3 0 -10   rotation= 0 0 1 -.4 \n\n            Shape \n\n             Appearance \n\n                Material   diffuseColor= 0 0 1 / \n\n             /Appearance \n\n             Cylinder/ \n\n            /Shape \n\n         /Transform \n\n         Viewpoint   position= 0 0 0 / \n\n         PointLight/ \n\n     /Scene  /X3D    Toward the bottom there is a PointLight and Viewpoint, the light and camera in the scene.  VR, just like a Hollywood movie, has \u201cLights, Camera, Action!\u201d  Each Transform has a DEFine which assigns names to our Scene Objects as if they were actors in a movie.   Feel free to experiment with the color, location and rotation of the primitives, then view them in GearVR.  If you have access to a 3D modeling program such as 3D Studio (3DS) Max, Blender or BS Content Studio, you can create your own 3D scenes and display them in GearVR.   Blender  and  BS Content Studio  are free and export directly to X3D.  3DS Max exports to VRML, which can be converted to X3D using online tools such as  Instant Reality converter .  Additional tutorials can be found at  https://doc.x3dom.org/tutorials/index.html  and  http://www.web3d.org/getting-started-x3d .", 
            "title": "GVRF and the X3D file format"
        }, 
        {
            "location": "/about/contribution/", 
            "text": "Gear VR development process, guidelines, and tips; and getting answers, reporting a bug, and submitting a patch\n\n\nGetting involved with the GearVRf Project is easy.\n\n\nTo contribute to the GearVRf Project (such as reporting bugs and submitting patches):\n\n\n\n\nFollow the \nGitHub contributor guidelines\n\n\nAdd the \nGearVRf DCO\n signoff to each commit message during development.\n\n\n\n\nDevelopment Process\n\n\nIt is the responsibility of GearVRf Maintainers and Reviewers to decide whether submitted code should be integrated into the mainline code, returned for revision, or rejected.\n\n\nIndividual developers maintain a local copy of the GearVRf codebase using the git revision control system. Git ensures that all participants are working with a common and up-to-date code base at all times. Each developer works to develop, debug, build, and validate their own code against the current codebase, so that when the time comes to integrate into the mainline Project, their changes apply cleanly and with a minimum amount of merging effort.\n\n\nThe GearVRf Project development process is marked by the following highlights:\n\n\n\n\nThe feature development process starts with an author discussing a proposed feature with the Maintainers and/or or Reviewers.\n\n\nThe Maintainers and Reviewers evaluate the idea, give feedback, and finally approve or reject the proposal.\n\n\nThe author shares the proposal with the community via the mailing list.\n\n\nThe community provides feedback which can be used by the author to modify their proposal and share it with the community again.\n\n\nThe above steps are repeated until the community reaches a consensus according to the Community Guidelines.\n\n\nAfter a consensus is reached, the author proceeds with the implementation and testing of the feature.\n\n\nAfter the author is confident their code is ready for integration:\n\n\nThe author generates a patch and signs off on their code.\n\n\nThe author submits a patch by opening a \npull request\n.\n\n\n\n\n\n\nThe Maintainers and/or Reviewers watch the pull request for the patch, test the code, and accept or reject the patch accordingly.\n\n\nAfter the code passes code review, the Maintainers and/or Reviewers accept the code (integrated into the main branch), which completes the development process.\n\n\n\n\nAfter a patch has been accepted, it remains the authoring developer's responsibility to maintain the code throughout its lifecycle, and to provide security and feature updates as needed.\n\n\nFor more information about GitHub issues, refer to the \nGitHub issues guidelines\n.\n\n\nCoding Guidelines\n\n\nWhen generating you own GearVRf project code, please follow these guidelines.\n\n\n\n\n\n\nGeneral:\n\n\n\n\nDo not abbreviate variable names.\n\n\nAbbreviations may not be familiar to new and other project members. Code with abbreviations will not be merged.\n\n\nPublic classes must start with GVR (for example, when adding a Foo class, the class name should be GVRFoo).\n\n\nImplementation classes should not start with GVR.\n\n\n\n\n\n\n\n\nIn Java:\n\n\n\n\nUse camel case for names (for example, setBackgroundColor).\n\n\nSet up and use the auto-formatter for Java code in Eclipse (see below).\n\n\n\n\n\n\n\n\nIn C++:\n\n\n\n\nUse underscore case for names (for example, gvr_note4).\n\n\nPut all JNI interface calls in a separate file with the postfix _jni. \n\nFor example, put the JNI interfaces for GVRSceneObject in a separate file scene_object_jni.cpp\n\n\nFollow the actual logic in plain C++ .h and .cpp files.\n\n\nFor each new C++ file that has a correlative Java GVR class, do not add GVR as a prefix to the file name. \n\nFor example, for GVRSceneObject.java, the C++ file name would be scene_object.cpp/scene_object.h\n\n\nSet up and use the auto-formatter for C++ code in Eclipse (see below).\n\n\n\n\n\n\n\n\nTo set up and use the Java auto-formatter in Eclipse:\n\n\n\n\n\n\nIn Eclipse, set up auto-formatting by following methods:\n\n\n\n\nDownload the \nCode formatter profile: Java conventions (all spaces) XML\n file.\n\n\nClick \nWindow \n Preferences\n\n\nIn the Preferences dialog box:\n\n\nClick \nJava \n Code Style \n Formatter\n OR \nJava \n Code Style\n\n\nImport \nJava-conventions-all-spaces.xml\n file\n\n\n\n\n\n\n\n\n\n\n\n\nIn Eclipse, auto-format your code\n\n\n\n\n\n\nTo set up and use the C++ auto-formatter in Eclipse:\n\n\n\n\nIn Eclipse, set up auto-formatting by following methods:\n\n\nDownload the \nCode formatter profile: K\nR, spaces only\n\n\nClick \nWindow \n Preferences\n\n\nIn the Preferences dialog box:\n\n\nClick \nC/C++ \n Code Style \n Formatter\n OR \nC/C++ \n Code Style\n\n\nImport \nK-and-R-C++-spaces-only.xml\n file\n\n\n\n\n\n\n\n\n\n\nIn Eclipse, auto-format your code\n\n\n\n\nSubmit a Patch\n\n\nThe following guidelines on the submission process are provided to help you be more effective when submitting code to the GearVRf Project.\n\n\nWhen development is complete, a patch set should be submitted via Github pull requests. A review of the patch set will take place. When accepted, the patch set will be integrated into the next build, verified, and tested. It is then the responsibility of the authoring developer to maintain the code throughout its lifecycle.\n\n\nPlease submit all patches in public by opening a pull request. Patches sent privately to Maintainers or Reviewers will not be considered. Because the GearVRf Project is an open source Project, be prepared for feedback and criticism--it happens to everyone. If asked to rework your code, be persistent and resubmit after making changes.\n\n\n\n\n\n\nScope the patch\n\n\nSmaller patches are generally easier to understand and test, so please submit changes in the smallest increments possible, within reason. Smaller patches are less likely to have unintended consequences, and if they do, getting to root cause is much easier for you and the Maintainers and Reviewers. Additionally, smaller patches are much more likely to be accepted.\n\n\n\n\n\n\nSign your work with the\n  \nGearVRf DCO\n.\n\n\nThe sign-off is a simple line at the end of the explanation for the patch, which certifies that you wrote it or otherwise have the right to pass it on as an open-source patch. The rules are pretty simple, and the sign-off is required for a patch to be accepted.\n\n\n\n\n\n\nOpen a Github \npull request\n\n\n\n\n\n\nWhat if my patch is rejected? \n\n\nIt happens all the time, for many reasons, and not necessarily because the code is bad. Take the feedback, adapt your code, and try again. Remember, the ultimate goal is to preserve the quality of the code and maintain the focus of the Project through intensive review.\nMaintainers typically have to process a lot of submissions, and the time for any individual response is generally limited. If the reason for rejection is unclear, please ask for more information on the mailing list or on the IRC channel.\nIf you have a solid technical reason to disagree with feedback and you feel that reason has been overlooked, take the time to thoroughly explain it in your response.\n\n\n\n\n\n\nEscalation\n\n\nIf you submitted a patch and did not receive a response within 5 business days:\n\n\n\n\nPlease send an email to the GearVRf Project Developers \nMailing List\n.\n\n\nIn the first line of the email, include this phrase \"Patch escalation: no response for x days\". \nThis is one of those rare cases where you should top post, to make sure that Maintainers and Reviewers see the escalation text, which cues them to make sure someone responds.\n\n\n\n\n\n\n\n\nCode review\n\n\nCode review can be performed by all the members of the Project (not just Maintainers and Reviewers). Members can review code changes and share their opinion by comments with the following principles:\n    * Discuss code; never discuss the code's author.\n    * Respect and acknowledge contributions, suggestions, and comments.\n    * Listen and be open to all different opinions.\n    * Help each other.\n\n\nChanges are submitted via pull requests and only the Maintainer or Reviewers of the module affected by the code change should approve or reject the pull request.\nChanges should be reviewed in reasonable amount of time. Maintainers and Reviewers should leave changes open for some time (at least 1 full business day) so others can offer feedback. Review times increase with the complexity of the review.\n\n\nGitHub Development Tips\n\n\nTips for working on GitHub\n\n\n\n\n\n\nFork the \nGitHub repository\n and clone it locally.\n\n\nConnect your local repository to the original upstream repository by adding it as a remote.\n\n\nPull in upstream changes often to stay up-to-date so that when you submit your pull request, merge conflicts will be less likely.\n\n\nFor more details, see \nGitHub fork synching guidelines\n.\n\n\n\n\n\n\nCreate a \nbranch\n for your edits.\n\n\n\n\n\n\n\n\n\n\nOur usual github workflow:\n\n\n\n\nGoto: \nhttps://github.com/Samsung/GearVRf/\n\n\nFind the \u2018fork\u2019 button in the upper right. Fork GearVRf into your own repository\n\n\nIn your own fork of GearVRf, click on the \u2018branch\u2019 button and create a new branch\n\n\nClone your repo onto your local machine. (you\u2019ll notice a convenience \u2018HTTPS clone URL\u2019 thing to the right on the webpage, that\u2019ll give the full URL you need to clone. The URL will look something like: \nhttps://github.com/thomasflynn/GearVRf.git\n, but with your own github id in the middle there.\n\n\nYou\u2019ll need to get the Samsung GearVRf repo as the upstream remote repo for your fork. git remote add parent \nhttps://github.com/Samsung/GearVRf/\n\n\nSwitch to the branch you created (git checkout branchname) on your local machine.\n\n\nMake your changes.\n\n\nGit add, git commit.\n\n\nGit push origin branchname:branchname ; \n- this will push it up to your forked repo on github.\n\n\nOn the webpage for your repo, you\u2019ll see a \u2018pull request button\u2019. Click that. You\u2019ll see your commit message and you\u2019ll need to add your DCO (see submitting a patch on gearvrf.org. also: wiki.gearvrf.org/bin/view/GearVRF/GearVRfDCO\n\n\nClick the green \u2018create pull request\u2019 button at the bottom.\n\n\n\n\nIf you need to upload a second patchset due to comments on your pull-request\n\n\n\n\nMake changes in your branch\n\n\nGit add, git commit, git push origin branchname:branchname\n\n\nYour new changes are now a part of the commit.\n\n\n\n\nTo rebase:\n\n\n\n\nSwitch to your master branch: git checkout master\n\n\nPull the remote master: git pull parent master:master\n\n\nForce-push the update to your master branch: git push \u2013f origin master:master\n\n\nSwitch to your branch: git checkout branchname\n\n\nRebase: git rebase master\n\n\nGit add, git commit, git push -f origin branchname:branchname\n\n\n\n\nGet Answers and Report a Bug\n\n\nIf you have a question about GearVRf code, have trouble following documentation, or find a bug, review the current GearVRf issues in GitHub, and if necessary, create a new issue.\n\n\nTips on GitHub Issues\n\n\n\n\n\n\nCheck existing GearVRf issues for the answer to your \nissue\n.\n\n\nDuplicating an issue slows you and others. Search through open and closed issues to see if the problem you are running into has already been addressed.\n\n\n\n\n\n\nIf necessary, open a \nnew issue\n.\n\n\n\n\nClearly describe the issue. \n\n\nWhat did you expect to happen?\n\n\nWhat actually happened instead?\n\n\nHow can someone else recreate the problem?\n\n\n\n\n\n\nLink to demos that recreate the problem on things such as \nJSFiddle\n or \nCodePen\n.\n\n\nInclude system details (such as the hardware, library, and operating system you are using and their versions).\n\n\n\n\nPaste error output and logs in the issue or in a \nGist\n. \n\n\nWhen pasting in the issue, wrap code in three backticks: ``` so that it renders nicely.", 
            "title": "Contribution"
        }, 
        {
            "location": "/about/contribution/#development-process", 
            "text": "It is the responsibility of GearVRf Maintainers and Reviewers to decide whether submitted code should be integrated into the mainline code, returned for revision, or rejected.  Individual developers maintain a local copy of the GearVRf codebase using the git revision control system. Git ensures that all participants are working with a common and up-to-date code base at all times. Each developer works to develop, debug, build, and validate their own code against the current codebase, so that when the time comes to integrate into the mainline Project, their changes apply cleanly and with a minimum amount of merging effort.  The GearVRf Project development process is marked by the following highlights:   The feature development process starts with an author discussing a proposed feature with the Maintainers and/or or Reviewers.  The Maintainers and Reviewers evaluate the idea, give feedback, and finally approve or reject the proposal.  The author shares the proposal with the community via the mailing list.  The community provides feedback which can be used by the author to modify their proposal and share it with the community again.  The above steps are repeated until the community reaches a consensus according to the Community Guidelines.  After a consensus is reached, the author proceeds with the implementation and testing of the feature.  After the author is confident their code is ready for integration:  The author generates a patch and signs off on their code.  The author submits a patch by opening a  pull request .    The Maintainers and/or Reviewers watch the pull request for the patch, test the code, and accept or reject the patch accordingly.  After the code passes code review, the Maintainers and/or Reviewers accept the code (integrated into the main branch), which completes the development process.   After a patch has been accepted, it remains the authoring developer's responsibility to maintain the code throughout its lifecycle, and to provide security and feature updates as needed.  For more information about GitHub issues, refer to the  GitHub issues guidelines .", 
            "title": "Development Process"
        }, 
        {
            "location": "/about/contribution/#coding-guidelines", 
            "text": "When generating you own GearVRf project code, please follow these guidelines.    General:   Do not abbreviate variable names.  Abbreviations may not be familiar to new and other project members. Code with abbreviations will not be merged.  Public classes must start with GVR (for example, when adding a Foo class, the class name should be GVRFoo).  Implementation classes should not start with GVR.     In Java:   Use camel case for names (for example, setBackgroundColor).  Set up and use the auto-formatter for Java code in Eclipse (see below).     In C++:   Use underscore case for names (for example, gvr_note4).  Put all JNI interface calls in a separate file with the postfix _jni.  For example, put the JNI interfaces for GVRSceneObject in a separate file scene_object_jni.cpp  Follow the actual logic in plain C++ .h and .cpp files.  For each new C++ file that has a correlative Java GVR class, do not add GVR as a prefix to the file name.  For example, for GVRSceneObject.java, the C++ file name would be scene_object.cpp/scene_object.h  Set up and use the auto-formatter for C++ code in Eclipse (see below).     To set up and use the Java auto-formatter in Eclipse:    In Eclipse, set up auto-formatting by following methods:   Download the  Code formatter profile: Java conventions (all spaces) XML  file.  Click  Window   Preferences  In the Preferences dialog box:  Click  Java   Code Style   Formatter  OR  Java   Code Style  Import  Java-conventions-all-spaces.xml  file       In Eclipse, auto-format your code    To set up and use the C++ auto-formatter in Eclipse:   In Eclipse, set up auto-formatting by following methods:  Download the  Code formatter profile: K R, spaces only  Click  Window   Preferences  In the Preferences dialog box:  Click  C/C++   Code Style   Formatter  OR  C/C++   Code Style  Import  K-and-R-C++-spaces-only.xml  file      In Eclipse, auto-format your code", 
            "title": "Coding Guidelines"
        }, 
        {
            "location": "/about/contribution/#submit-a-patch", 
            "text": "The following guidelines on the submission process are provided to help you be more effective when submitting code to the GearVRf Project.  When development is complete, a patch set should be submitted via Github pull requests. A review of the patch set will take place. When accepted, the patch set will be integrated into the next build, verified, and tested. It is then the responsibility of the authoring developer to maintain the code throughout its lifecycle.  Please submit all patches in public by opening a pull request. Patches sent privately to Maintainers or Reviewers will not be considered. Because the GearVRf Project is an open source Project, be prepared for feedback and criticism--it happens to everyone. If asked to rework your code, be persistent and resubmit after making changes.    Scope the patch  Smaller patches are generally easier to understand and test, so please submit changes in the smallest increments possible, within reason. Smaller patches are less likely to have unintended consequences, and if they do, getting to root cause is much easier for you and the Maintainers and Reviewers. Additionally, smaller patches are much more likely to be accepted.    Sign your work with the    GearVRf DCO .  The sign-off is a simple line at the end of the explanation for the patch, which certifies that you wrote it or otherwise have the right to pass it on as an open-source patch. The rules are pretty simple, and the sign-off is required for a patch to be accepted.    Open a Github  pull request    What if my patch is rejected?   It happens all the time, for many reasons, and not necessarily because the code is bad. Take the feedback, adapt your code, and try again. Remember, the ultimate goal is to preserve the quality of the code and maintain the focus of the Project through intensive review.\nMaintainers typically have to process a lot of submissions, and the time for any individual response is generally limited. If the reason for rejection is unclear, please ask for more information on the mailing list or on the IRC channel.\nIf you have a solid technical reason to disagree with feedback and you feel that reason has been overlooked, take the time to thoroughly explain it in your response.    Escalation  If you submitted a patch and did not receive a response within 5 business days:   Please send an email to the GearVRf Project Developers  Mailing List .  In the first line of the email, include this phrase \"Patch escalation: no response for x days\". \nThis is one of those rare cases where you should top post, to make sure that Maintainers and Reviewers see the escalation text, which cues them to make sure someone responds.     Code review  Code review can be performed by all the members of the Project (not just Maintainers and Reviewers). Members can review code changes and share their opinion by comments with the following principles:\n    * Discuss code; never discuss the code's author.\n    * Respect and acknowledge contributions, suggestions, and comments.\n    * Listen and be open to all different opinions.\n    * Help each other.  Changes are submitted via pull requests and only the Maintainer or Reviewers of the module affected by the code change should approve or reject the pull request.\nChanges should be reviewed in reasonable amount of time. Maintainers and Reviewers should leave changes open for some time (at least 1 full business day) so others can offer feedback. Review times increase with the complexity of the review.", 
            "title": "Submit a Patch"
        }, 
        {
            "location": "/about/contribution/#github-development-tips", 
            "text": "Tips for working on GitHub    Fork the  GitHub repository  and clone it locally.  Connect your local repository to the original upstream repository by adding it as a remote.  Pull in upstream changes often to stay up-to-date so that when you submit your pull request, merge conflicts will be less likely.  For more details, see  GitHub fork synching guidelines .    Create a  branch  for your edits.      Our usual github workflow:   Goto:  https://github.com/Samsung/GearVRf/  Find the \u2018fork\u2019 button in the upper right. Fork GearVRf into your own repository  In your own fork of GearVRf, click on the \u2018branch\u2019 button and create a new branch  Clone your repo onto your local machine. (you\u2019ll notice a convenience \u2018HTTPS clone URL\u2019 thing to the right on the webpage, that\u2019ll give the full URL you need to clone. The URL will look something like:  https://github.com/thomasflynn/GearVRf.git , but with your own github id in the middle there.  You\u2019ll need to get the Samsung GearVRf repo as the upstream remote repo for your fork. git remote add parent  https://github.com/Samsung/GearVRf/  Switch to the branch you created (git checkout branchname) on your local machine.  Make your changes.  Git add, git commit.  Git push origin branchname:branchname ;  - this will push it up to your forked repo on github.  On the webpage for your repo, you\u2019ll see a \u2018pull request button\u2019. Click that. You\u2019ll see your commit message and you\u2019ll need to add your DCO (see submitting a patch on gearvrf.org. also: wiki.gearvrf.org/bin/view/GearVRF/GearVRfDCO  Click the green \u2018create pull request\u2019 button at the bottom.   If you need to upload a second patchset due to comments on your pull-request   Make changes in your branch  Git add, git commit, git push origin branchname:branchname  Your new changes are now a part of the commit.   To rebase:   Switch to your master branch: git checkout master  Pull the remote master: git pull parent master:master  Force-push the update to your master branch: git push \u2013f origin master:master  Switch to your branch: git checkout branchname  Rebase: git rebase master  Git add, git commit, git push -f origin branchname:branchname", 
            "title": "GitHub Development Tips"
        }, 
        {
            "location": "/about/contribution/#get-answers-and-report-a-bug", 
            "text": "If you have a question about GearVRf code, have trouble following documentation, or find a bug, review the current GearVRf issues in GitHub, and if necessary, create a new issue.  Tips on GitHub Issues    Check existing GearVRf issues for the answer to your  issue .  Duplicating an issue slows you and others. Search through open and closed issues to see if the problem you are running into has already been addressed.    If necessary, open a  new issue .   Clearly describe the issue.   What did you expect to happen?  What actually happened instead?  How can someone else recreate the problem?    Link to demos that recreate the problem on things such as  JSFiddle  or  CodePen .  Include system details (such as the hardware, library, and operating system you are using and their versions).   Paste error output and logs in the issue or in a  Gist .   When pasting in the issue, wrap code in three backticks: ``` so that it renders nicely.", 
            "title": "Get Answers and Report a Bug"
        }, 
        {
            "location": "/about/certificate/", 
            "text": "Gear DCO and signed-off-by process\n\n\nThe GearVRf project uses the signed-off-by language and process used by the Linux kernel, to give us a clear chain of trust for every patch received.\n\n\nGearVRf Developer\ns Certificate of Origin 1.0\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or\n\n(c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it.\n\n(d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project, under the same open source license.\n\n\n\n\n\nUsing the Signed-Off-By Process\n\n\nWe have the same requirements for using the signed-off-by process as the Linux kernel. In short, you need to include a signed-off-by tag in every patch:\n\n\n\"Signed-off-by:\" this is a developer's certification that he or she has the right to submit the patch for inclusion into the project. It is an agreement to the Developer's Certificate of Origin (above). Code without a proper signoff cannot be merged into the mainline.\n\n\nYou should use your real name and email address in the format below:\n\n\nGearVRf-DCO-1.0-Signed-off-by: Random J Developer\nrandom@developer.example.org", 
            "title": "Developer Certificate of Origin"
        }, 
        {
            "location": "/about/certificate/#using-the-signed-off-by-process", 
            "text": "We have the same requirements for using the signed-off-by process as the Linux kernel. In short, you need to include a signed-off-by tag in every patch:  \"Signed-off-by:\" this is a developer's certification that he or she has the right to submit the patch for inclusion into the project. It is an agreement to the Developer's Certificate of Origin (above). Code without a proper signoff cannot be merged into the mainline.  You should use your real name and email address in the format below:  GearVRf-DCO-1.0-Signed-off-by: Random J Developer\nrandom@developer.example.org", 
            "title": "Using the Signed-Off-By Process"
        }, 
        {
            "location": "/about/release_notes/", 
            "text": "release_notes.md", 
            "title": "Release Notes"
        }, 
        {
            "location": "/about/roadmap/", 
            "text": "The following features are planned for the GearVRf Project:\n\n\nVersion 3.1\n The following items were completed:\n\n\n\n\nHierarchical bounding volume culling \n\n\nSupport for Assimp's animations\n\n\nSupport for all the different shaders Assimp supports \n\n\nSupport for multiple lights\n\n\nSupport for shadows\n\n\nState sorting\n\n\nDraw call batching (initial batching completed; additional batching planned for 4.0)\n\n\nDayDream Support \n\n\nAndroidStudio Support\n\n\n\n\nVersion 3.2\n The following updates are planned for release in late Apr/early May 2017:\n\n\n\n\nNew physics support\n\n\nBatching for all shaders\n\n\nLOD rework\n\n\nDayDream controller support\n\n\nNew GearVR controller support\n\n\nVarious optimizations\n\n\n\n\nVersion 4.0\n The following updates are planned:\n\n\n\n\nFull support for Vulkan\n\n\nAdditional draw call batching support", 
            "title": "Roadmap"
        }, 
        {
            "location": "/about/license/", 
            "text": "GearVRf License\n\n\nThe \nGearVRf software license\n is under the Apache 2.0 license.\n\n\nOpen Source Software\n\n\nThe following open source software supports GearVRf:\n\n\n\n\nApache Commons Mathematics Library v3.2 Apache 2.0\n\n\nAssimp v3.1 Open Asset Import Library\n\n\nLibpng v1.2.45 PNG Reference Library\n\n\nGLM v0.9.6.3 OpenGL Mathematics library\n Licensed under \nThe Happy Bunny License and MIT License", 
            "title": "License"
        }, 
        {
            "location": "/about/license/#gearvrf-license", 
            "text": "The  GearVRf software license  is under the Apache 2.0 license.", 
            "title": "GearVRf License"
        }, 
        {
            "location": "/about/license/#open-source-software", 
            "text": "The following open source software supports GearVRf:   Apache Commons Mathematics Library v3.2 Apache 2.0  Assimp v3.1 Open Asset Import Library  Libpng v1.2.45 PNG Reference Library  GLM v0.9.6.3 OpenGL Mathematics library  Licensed under  The Happy Bunny License and MIT License", 
            "title": "Open Source Software"
        }, 
        {
            "location": "/about/privacy_policy/", 
            "text": "We respect your privacy and are committed to protecting it.\n\n\nWe require that you provide some personal information, to provide GearVRf Project services to its Members, and to improve the GearVRf Project. The type of information collected, how it is used, and the information privacy choices you have, are detailed in this policy.\n\n\nInformation Collection and Use\n\n\nWe collect the information you provide when you register for an account or complete an information request form.\n\n\nWe use this information to satisfy your requests for further information, to customize our responses and our future communication with you, and to contact you, regarding development and events in the GearVRf Project and/or Project areas that you have expressed general interest in.\n\n\nWe make every effort to allow you to opt-in and opt-out of receiving GearVRf Project messages. However, if you are receiving messages from us and cannot find a way to unsubscribe, please contact us at \n\n\nInformation Sharing\n\n\nWe will not release your personal information to anyone by any method, including selling, renting, or sharing, unless:\n\n\n\n\nYou grant us permission.\n\n\nWe are required to do so by law.\n\n\n\n\nWe will not share personal identification information data, either for single individuals or groups, with any parties, including those affiliated with the GearVRf Project, such as members or sponsors.\n\n\nHow We Use Cookies\n\n\nThe GearVRf Project website uses cookies. A cookie is a small amount of text data, sent from our webserver to your browser, and stored on your device. The cookie is sent back to the webserver each time the browser connects to this website. We use cookies to personalize the website and to streamline your interaction with the website.\n\n\nIt may be possible to configure your browser to refuse cookies, or to ask you to accept each time a cookie is offered. If you choose not to accept cookies, areas of this website may have reduced functionality or performance.\n\n\nSoftware on our servers or third-party web statics services may store your IP address and other information passed on by your browser (such as browser version, operating system, screen size, language, etc). gearvrf.org and/or third-party services will aggregate this information to provide usage statistics for this website. We use this information to optimize the design, structure, and performance of the Project website. In particular, Google Analytics is used to provide usage statistics. For more information, read the Google Analytics Privacy Policy.\n\n\nData Security\n\n\nThe GearVRf Project is also committed to the security of your personal information. We train those who work on the GearVRf Project on this privacy policy. The Project website uses SSL (Secure Sockets Layer) to protect your personal information, by encrypting your information when you send it to the GearVRf Project.\n\n\nPublic Forum Content\n\n\nThe GearVRf Project makes available to its users several communication forums (such as mail lists, blogs, and others). Be aware that any information or messages you share in these forums immediately becomes public information. Exercise caution in determining whether to disclose any of your personal information. The GearVRf Project is an open source website, intended to encourage creative thinking and free expression; however, the GearVRf Project reserves the right to act as necessary to preserve the integrity of the website and its forums, including removing any and all posts deemed vulgar or inappropriate.\n\n\nChildren's Online Privacy\n\n\nRegarding children under the age of 13, the GearVRf Project does not knowingly:\n\n\n\n\nAccept personal information from them\n\n\nAllow them to become registered members of the GearVRf Project website\n\n\n\n\nUpdates to this Privacy Policy\n\n\nWe may update this policy. We will contact you if we make any substantial changes in how we use your personal information. This privacy policy was last updated on March 25, 2015.\n\n\nContact Information\n\n\nIf you have any questions about this privacy policy itself, or on how we use personal information in the GearVRf Project, please contact us at", 
            "title": "Privacy Policy"
        }, 
        {
            "location": "/about/privacy_policy/#we-respect-your-privacy-and-are-committed-to-protecting-it", 
            "text": "We require that you provide some personal information, to provide GearVRf Project services to its Members, and to improve the GearVRf Project. The type of information collected, how it is used, and the information privacy choices you have, are detailed in this policy.", 
            "title": "We respect your privacy and are committed to protecting it."
        }, 
        {
            "location": "/about/privacy_policy/#information-collection-and-use", 
            "text": "We collect the information you provide when you register for an account or complete an information request form.  We use this information to satisfy your requests for further information, to customize our responses and our future communication with you, and to contact you, regarding development and events in the GearVRf Project and/or Project areas that you have expressed general interest in.  We make every effort to allow you to opt-in and opt-out of receiving GearVRf Project messages. However, if you are receiving messages from us and cannot find a way to unsubscribe, please contact us at", 
            "title": "Information Collection and Use"
        }, 
        {
            "location": "/about/privacy_policy/#information-sharing", 
            "text": "We will not release your personal information to anyone by any method, including selling, renting, or sharing, unless:   You grant us permission.  We are required to do so by law.   We will not share personal identification information data, either for single individuals or groups, with any parties, including those affiliated with the GearVRf Project, such as members or sponsors.", 
            "title": "Information Sharing"
        }, 
        {
            "location": "/about/privacy_policy/#how-we-use-cookies", 
            "text": "The GearVRf Project website uses cookies. A cookie is a small amount of text data, sent from our webserver to your browser, and stored on your device. The cookie is sent back to the webserver each time the browser connects to this website. We use cookies to personalize the website and to streamline your interaction with the website.  It may be possible to configure your browser to refuse cookies, or to ask you to accept each time a cookie is offered. If you choose not to accept cookies, areas of this website may have reduced functionality or performance.  Software on our servers or third-party web statics services may store your IP address and other information passed on by your browser (such as browser version, operating system, screen size, language, etc). gearvrf.org and/or third-party services will aggregate this information to provide usage statistics for this website. We use this information to optimize the design, structure, and performance of the Project website. In particular, Google Analytics is used to provide usage statistics. For more information, read the Google Analytics Privacy Policy.", 
            "title": "How We Use Cookies"
        }, 
        {
            "location": "/about/privacy_policy/#data-security", 
            "text": "The GearVRf Project is also committed to the security of your personal information. We train those who work on the GearVRf Project on this privacy policy. The Project website uses SSL (Secure Sockets Layer) to protect your personal information, by encrypting your information when you send it to the GearVRf Project.", 
            "title": "Data Security"
        }, 
        {
            "location": "/about/privacy_policy/#public-forum-content", 
            "text": "The GearVRf Project makes available to its users several communication forums (such as mail lists, blogs, and others). Be aware that any information or messages you share in these forums immediately becomes public information. Exercise caution in determining whether to disclose any of your personal information. The GearVRf Project is an open source website, intended to encourage creative thinking and free expression; however, the GearVRf Project reserves the right to act as necessary to preserve the integrity of the website and its forums, including removing any and all posts deemed vulgar or inappropriate.", 
            "title": "Public Forum Content"
        }, 
        {
            "location": "/about/privacy_policy/#childrens-online-privacy", 
            "text": "Regarding children under the age of 13, the GearVRf Project does not knowingly:   Accept personal information from them  Allow them to become registered members of the GearVRf Project website", 
            "title": "Children's Online Privacy"
        }, 
        {
            "location": "/about/privacy_policy/#updates-to-this-privacy-policy", 
            "text": "We may update this policy. We will contact you if we make any substantial changes in how we use your personal information. This privacy policy was last updated on March 25, 2015.", 
            "title": "Updates to this Privacy Policy"
        }, 
        {
            "location": "/about/privacy_policy/#contact-information", 
            "text": "If you have any questions about this privacy policy itself, or on how we use personal information in the GearVRf Project, please contact us at", 
            "title": "Contact Information"
        }, 
        {
            "location": "/about/terms_of_service/", 
            "text": "By using the GearVRf Project website, you are agreeing to be bound by the following terms and conditions here.\n\n\nHere is the basic idea:\n\n\n\n\nDo not violate anyone's intellectual property or post anyone else's copyrighted or confidential material you do not have permission to use.\n\n\nDo not post anything vulgar, inflammatory, pornographic, illegal, etc.\n\n\nDo not post spam.\n\n\nDo not post too many meta-links and tags; too many will not be accepted. If you try to boost your SEO, that is an abuse of the system.\n\n\nDo not post just to sell something. Sure, talk up your project, but do not mention how to buy it every single time.\n\n\nDo not launch into personal attacks. You are not going to agree with everyone, but name-calling will just cause trouble and will be regarded as flaming behavior.\n\n\nTone down foul language. You can say what you need to say without relying on cursing. In fact, your writing will be regarded as that much more impressive.\n\n\n\n\nThe GearVRf Project is providing a framework for discussion and user generated information to expand the knowledge base of GearVRf-related information. Please note that articles, as well as any other user content on the GearVRf Project (such as blogs, directory content, forums, comments, etc.), do not reflect the views or endorsements of the GearVRf Project, or its community members. We recognize there may be inaccurate information reflected in this website; users should understand that something that appears on the GearVRf Project website does not mean the GearVRf Project has vetted or endorsed that content.\n\n\nThe GearVRf Project reserves the right to take down anything posted on this website for the above or other reasons. If you do not agree to these terms, do not use this website. Violation of the terms in this document may result in the removal of the content and a warning from website administrators. A second violation will result in the removal of your user account.\n\n\nGearVRf Project Terms of Use\n\n\nMarch 20, 2015\n\n\nTerms and Conditions of Use for the GearVRf Project website\n\n\nBY ACCESSING, BROWSING OR USING THIS WEBSITE, YOU ACKNOWLEDGE THAT YOU HAVE READ, UNDERSTAND AND AGREE TO BE BOUND BY THESE TERMS AND CONDITIONS.\n\n\nThis website is a service made available by the GearVRf Project. All software, documentation, information, and/or other materials provided on and through this website (\"Content\") may be used solely under the following terms and conditions (\"Terms of Use\").\n\n\nThis website may contain other proprietary notices and copyright information, the terms of which must be observed and followed. The Content on this website may contain technical inaccuracies or typographical errors and may be changed or updated without notice. The GearVRf Project may also make improvements and/or changes to the Content at any time without notice.\n\n\nThe GearVRf Project and its community members (\"Members\") assume no responsibility regarding the accuracy of the Content. Use of the Content is at the recipient's own risk. The GearVRf Project and its Members provide no assurances that any reported problems with any Content will be resolved.\n\n\nIntellectual Property Rights\n\n\nExcept as otherwise provided, Content on this website, including all materials posted by the GearVRf Project, is licensed under a \nCreative Commons Attribution 3.0 License\n.\n\n\nAll logos and trademarks contained on this website are and remain the property of their respective owners. No licenses or other rights in or to such logos and/or trademarks are granted.\n\n\nExcept as otherwise expressly stated, by providing the Content, neither the GearVRf Project nor its Members grant any licenses to any copyrights, patents, or any other intellectual property rights.\n\n\nUsers Submissions\n\n\nUsers are solely responsible for all materials, whether publicly posted or privately transmitted, that users upload, post, e-mail, transmit, or otherwise make available on our websites (\"User Content\"). Neither the GearVRf Project nor any of its Members shall be liable for any claims arising out of User Content. You warrant that you have all rights needed to provide User Content in accordance with these terms and all applicable laws or regulations.\n\n\nThe GearVRf Project or other projects that may be hosted by the GearVRf Project may have license terms or Contributor Agreements that are specific to the project and may require Users to sign an agreement (such as a Contributor Agreement) assigning and/or licensing rights in submissions made to such projects. In all such cases, and to the extent there is a conflict, those license terms or agreements take precedence over these Terms of Use. With respect to any User Content not governed by other project-specific terms or agreements, you agree that the following non-exclusive, irrevocable, royalty-free, worldwide licenses shall apply:\n\n\nCode Submissions\n User Content in the form of source or object code will be governed by the Apache 2.0 License.\n\n\nAll Other Submissions\n User Content that is not in the form of source or object code, including but not limited to white papers, dissertations, articles or other literary works, Power Point presentations, encyclopedias, anthologies, wikis, blogs, diagrams, drawings, sketches, photos or other images, audio content, video content and audiovisual materials will be governed by the Creative Commons Attribution 3.0.\n\n\nThe GearVRf Project and its Members do not want to receive confidential information from you through this website. Please note that any information or material sent to the GearVRf Project or the Members will be deemed NOT to be confidential.\n\n\nYou are prohibited from posting or transmitting to or from this website any unlawful, threatening, libelous, defamatory, obscene, scandalous, inflammatory, pornographic, or profane material; or any other material that could give rise to any civil or criminal liability under the law.\n\n\nDisclaimers and Limitations of Liability\n\n\nThe GearVRf Project and the Members make no representations whatsoever about any other website that you may access through this website. When you access a non-GearVRf project website, even one that may contain the GearVRf Project's name or mark, please understand that it is independent from the GearVRf Project, and that the GearVRf Project and its Members have no control over the content on such websites. In addition, a link to a non-GearVRf project website does not mean that the GearVRf Project or its Members endorse or accept any responsibility for the content of or the use of such websites.\n\n\nIt is up to you to take precautions to ensure that whatever you select for your use is free of such items as viruses, worms, Trojan horses, and other items of a destructive nature.\n\n\nIN NO EVENT WILL THE GEARVRf PROJECT AND/OR ITS MEMBERS BE LIABLE TO YOU (AN INDIVIDUAL OR ENTITY) OR ANY OTHER INDIVIDUAL OR ENTITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, PUNITIVE, SPECIAL, OR CONSEQUENTIAL DAMAGES RELATED TO ANY USE OF THIS WEBSITE, THE CONTENT, OR ON ANY OTHER HYPERLINKED WEBSITE, INCLUDING, WITHOUT LIMITATION, ANY LOST PROFITS, LOST SALES, LOST REVENUE, LOSS OF GOODWILL, BUSINESS INTERRUPTION, LOSS OF PROGRAMS OR OTHER DATA ON YOUR INFORMATION HANDLING SYSTEM OR OTHERWISE, EVEN IF THE GEARVRf PROJECT OR ITS MEMBERS ARE EXPRESSLY ADVISED OR AWARE OF THE POSSIBILITY OF SUCH DAMAGES OR LOSSES.\n\n\nALL CONTENT IS PROVIDED BY THE GEARVRf PROJECT AND/OR ITS MEMBERS ON AN \"AS IS\" BASIS ONLY. THE GEARVRf PROJECT AND ITS MEMBERS PROVIDE NO REPRESENTATIONS, CONDITIONS AND/OR WARRANTIES, EXPRESS OR IMPLIED, INCLUDING, WITHOUT LIMITATION, THE IMPLIED WARRANTIES OF FITNESS FOR A PARTICULAR PURPOSE, MERCHANTABILITY AND NONINFRINGEMENT.\n\n\nThe GearVRf Project and its Members reserve the right to investigate complaints or reported violations of these Terms of Use and to take any action they deem appropriate including, without limitation, reporting any suspected unlawful activity to law enforcement officials, regulators, or other third-parties and disclosing any information necessary or appropriate to such persons or entities relating to user profiles, e-mail addresses, usage history, posted materials, IP addresses, and traffic information.\n\n\nThe GearVRf Project and its Members reserve the right to seek all remedies available at law and in equity for violations of these Terms of Use, including but not limited to the right to block access from a particular Internet address or account holder to this website.\n\n\nPrivacy\n\n\nCOPPA prohibits unfair or deceptive acts or practices in connection with the collection, use, or disclosure of personally identifiable information from and about children under 13 on the Internet. The law requires operators to notify parents and obtain their consent before collecting, using, or disclosing children's personal information.\n\n\nYou can learn more about the \nGearVRf Project Privacy Policy\n.\n\n\nDigital Millennium Copyright Act\n\n\nThe GearVRf Project respects the intellectual property of others, and we ask users of our websites to do the same. In accordance with the Digital Millennium Copyright Act (DMCA) and other applicable law, we have adopted a policy of terminating, in appropriate circumstances and at our sole discretion, subscribers or account holders who are deemed to be repeat infringers. We may also at our sole discretion limit access to our website and/or terminate the accounts of any users who infringe any intellectual property rights of others, whether or not there is any repeat infringement.\n\n\nNotice and Procedure for Notifying Designated Agent of Claims of Copyright Infringement\n\n\nIf you believe that any material on this website infringes upon any copyright which you own or control, or that any link on this website directs users to another website that contains material that infringes upon any copyright which you own or control, you may file a notification of such infringement with our Designated Agent as set forth below. Notifications of claimed copyright infringement must be sent to the GearVRf Project Designated Agent for notice of claims of copyright infringement. Our Designated Agent may be reached as follows:\n\n\nDesignated Agent: Waddah Kudaimi\n\n\nAddress of Designated Agent: 645 Clyde Avenue, Mountain View CA 94043\n\n\nEmail Address of Designated Agent:", 
            "title": "Terms of Service"
        }, 
        {
            "location": "/about/terms_of_service/#gearvrf-project-terms-of-use", 
            "text": "March 20, 2015  Terms and Conditions of Use for the GearVRf Project website  BY ACCESSING, BROWSING OR USING THIS WEBSITE, YOU ACKNOWLEDGE THAT YOU HAVE READ, UNDERSTAND AND AGREE TO BE BOUND BY THESE TERMS AND CONDITIONS.  This website is a service made available by the GearVRf Project. All software, documentation, information, and/or other materials provided on and through this website (\"Content\") may be used solely under the following terms and conditions (\"Terms of Use\").  This website may contain other proprietary notices and copyright information, the terms of which must be observed and followed. The Content on this website may contain technical inaccuracies or typographical errors and may be changed or updated without notice. The GearVRf Project may also make improvements and/or changes to the Content at any time without notice.  The GearVRf Project and its community members (\"Members\") assume no responsibility regarding the accuracy of the Content. Use of the Content is at the recipient's own risk. The GearVRf Project and its Members provide no assurances that any reported problems with any Content will be resolved.", 
            "title": "GearVRf Project Terms of Use"
        }, 
        {
            "location": "/about/terms_of_service/#intellectual-property-rights", 
            "text": "Except as otherwise provided, Content on this website, including all materials posted by the GearVRf Project, is licensed under a  Creative Commons Attribution 3.0 License .  All logos and trademarks contained on this website are and remain the property of their respective owners. No licenses or other rights in or to such logos and/or trademarks are granted.  Except as otherwise expressly stated, by providing the Content, neither the GearVRf Project nor its Members grant any licenses to any copyrights, patents, or any other intellectual property rights.", 
            "title": "Intellectual Property Rights"
        }, 
        {
            "location": "/about/terms_of_service/#users-submissions", 
            "text": "Users are solely responsible for all materials, whether publicly posted or privately transmitted, that users upload, post, e-mail, transmit, or otherwise make available on our websites (\"User Content\"). Neither the GearVRf Project nor any of its Members shall be liable for any claims arising out of User Content. You warrant that you have all rights needed to provide User Content in accordance with these terms and all applicable laws or regulations.  The GearVRf Project or other projects that may be hosted by the GearVRf Project may have license terms or Contributor Agreements that are specific to the project and may require Users to sign an agreement (such as a Contributor Agreement) assigning and/or licensing rights in submissions made to such projects. In all such cases, and to the extent there is a conflict, those license terms or agreements take precedence over these Terms of Use. With respect to any User Content not governed by other project-specific terms or agreements, you agree that the following non-exclusive, irrevocable, royalty-free, worldwide licenses shall apply:  Code Submissions  User Content in the form of source or object code will be governed by the Apache 2.0 License.  All Other Submissions  User Content that is not in the form of source or object code, including but not limited to white papers, dissertations, articles or other literary works, Power Point presentations, encyclopedias, anthologies, wikis, blogs, diagrams, drawings, sketches, photos or other images, audio content, video content and audiovisual materials will be governed by the Creative Commons Attribution 3.0.  The GearVRf Project and its Members do not want to receive confidential information from you through this website. Please note that any information or material sent to the GearVRf Project or the Members will be deemed NOT to be confidential.  You are prohibited from posting or transmitting to or from this website any unlawful, threatening, libelous, defamatory, obscene, scandalous, inflammatory, pornographic, or profane material; or any other material that could give rise to any civil or criminal liability under the law.", 
            "title": "Users Submissions"
        }, 
        {
            "location": "/about/terms_of_service/#disclaimers-and-limitations-of-liability", 
            "text": "The GearVRf Project and the Members make no representations whatsoever about any other website that you may access through this website. When you access a non-GearVRf project website, even one that may contain the GearVRf Project's name or mark, please understand that it is independent from the GearVRf Project, and that the GearVRf Project and its Members have no control over the content on such websites. In addition, a link to a non-GearVRf project website does not mean that the GearVRf Project or its Members endorse or accept any responsibility for the content of or the use of such websites.  It is up to you to take precautions to ensure that whatever you select for your use is free of such items as viruses, worms, Trojan horses, and other items of a destructive nature.  IN NO EVENT WILL THE GEARVRf PROJECT AND/OR ITS MEMBERS BE LIABLE TO YOU (AN INDIVIDUAL OR ENTITY) OR ANY OTHER INDIVIDUAL OR ENTITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, PUNITIVE, SPECIAL, OR CONSEQUENTIAL DAMAGES RELATED TO ANY USE OF THIS WEBSITE, THE CONTENT, OR ON ANY OTHER HYPERLINKED WEBSITE, INCLUDING, WITHOUT LIMITATION, ANY LOST PROFITS, LOST SALES, LOST REVENUE, LOSS OF GOODWILL, BUSINESS INTERRUPTION, LOSS OF PROGRAMS OR OTHER DATA ON YOUR INFORMATION HANDLING SYSTEM OR OTHERWISE, EVEN IF THE GEARVRf PROJECT OR ITS MEMBERS ARE EXPRESSLY ADVISED OR AWARE OF THE POSSIBILITY OF SUCH DAMAGES OR LOSSES.  ALL CONTENT IS PROVIDED BY THE GEARVRf PROJECT AND/OR ITS MEMBERS ON AN \"AS IS\" BASIS ONLY. THE GEARVRf PROJECT AND ITS MEMBERS PROVIDE NO REPRESENTATIONS, CONDITIONS AND/OR WARRANTIES, EXPRESS OR IMPLIED, INCLUDING, WITHOUT LIMITATION, THE IMPLIED WARRANTIES OF FITNESS FOR A PARTICULAR PURPOSE, MERCHANTABILITY AND NONINFRINGEMENT.  The GearVRf Project and its Members reserve the right to investigate complaints or reported violations of these Terms of Use and to take any action they deem appropriate including, without limitation, reporting any suspected unlawful activity to law enforcement officials, regulators, or other third-parties and disclosing any information necessary or appropriate to such persons or entities relating to user profiles, e-mail addresses, usage history, posted materials, IP addresses, and traffic information.  The GearVRf Project and its Members reserve the right to seek all remedies available at law and in equity for violations of these Terms of Use, including but not limited to the right to block access from a particular Internet address or account holder to this website.", 
            "title": "Disclaimers and Limitations of Liability"
        }, 
        {
            "location": "/about/terms_of_service/#privacy", 
            "text": "COPPA prohibits unfair or deceptive acts or practices in connection with the collection, use, or disclosure of personally identifiable information from and about children under 13 on the Internet. The law requires operators to notify parents and obtain their consent before collecting, using, or disclosing children's personal information.  You can learn more about the  GearVRf Project Privacy Policy .", 
            "title": "Privacy"
        }, 
        {
            "location": "/about/terms_of_service/#digital-millennium-copyright-act", 
            "text": "The GearVRf Project respects the intellectual property of others, and we ask users of our websites to do the same. In accordance with the Digital Millennium Copyright Act (DMCA) and other applicable law, we have adopted a policy of terminating, in appropriate circumstances and at our sole discretion, subscribers or account holders who are deemed to be repeat infringers. We may also at our sole discretion limit access to our website and/or terminate the accounts of any users who infringe any intellectual property rights of others, whether or not there is any repeat infringement.", 
            "title": "Digital Millennium Copyright Act"
        }, 
        {
            "location": "/about/terms_of_service/#notice-and-procedure-for-notifying-designated-agent-of-claims-of-copyright-infringement", 
            "text": "If you believe that any material on this website infringes upon any copyright which you own or control, or that any link on this website directs users to another website that contains material that infringes upon any copyright which you own or control, you may file a notification of such infringement with our Designated Agent as set forth below. Notifications of claimed copyright infringement must be sent to the GearVRf Project Designated Agent for notice of claims of copyright infringement. Our Designated Agent may be reached as follows:  Designated Agent: Waddah Kudaimi  Address of Designated Agent: 645 Clyde Avenue, Mountain View CA 94043  Email Address of Designated Agent:", 
            "title": "Notice and Procedure for Notifying Designated Agent of Claims of Copyright Infringement"
        }, 
        {
            "location": "/about/marketing_resource/", 
            "text": "GearVRf is open source by nature and the GearVRf logos below have been created to capture the freedom that open source allows.\n\n\n\n\nNote\n\n\nThe GearVRf logos below are for labeling purposes and should not be used in creating artwork such as banners or promotional material.\n\n\n\n\nTo download a GearVRf logo PNG file, \nright-click file image \n Save image as...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColor GearVRf logo with color text below\n\n\n\n\n\n\n\n\nColor GearVRf logo with white text below\n\n\n\n\n\n\n\n\nColor GearVRf logo with color text to the right", 
            "title": "Marketing Resources"
        }, 
        {
            "location": "/about/credits/", 
            "text": "We gratefully acknowledge the developers of the following images:\n\n\n\n\nGearVRf Features image    Cube And Flowers Pictures image courtesy of njaj at FreeDigitalPhotos.net\n\n\nGearVRf Features image    The Wooden Doll With Light Bulb image courtesy of tigger11th at FreeDigitalPhotos.net\n\n\nGearVRf Features image    Book image courtesy of Boykung at FreeDigitalPhotos.net", 
            "title": "Credits"
        }
    ]
}